{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvcnj-gNfWxy"
      },
      "source": [
        "# Assignment 3: Machine Translation\n",
        "\n",
        "TA contact for this assignment: Raymond Xiong (raymond.xiong@duke.edu), Xinchang Xiong (xinchang.xiong@duke.edu)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this assignment you will implement a LSTM based sequence-to-sequence model for machine translation.\n",
        "* We have provided an implementation of the encoder. You will need to implement an LSTM based decoder and then use it to train a basic sequence-to-sequence model.\n",
        "* Next, you will extend the decoder with attention and implement beam search decoding (we have provided a greedy decoder as reference).\n",
        "* Lastly you can try to improve the model using extensions such as a back translation or data augmentation.\n",
        "\n",
        "**Warning**: Attention and beam search can be tricky to implement. We expect this assignment to take longer than the CRF one. Please don't start the day before it is due!\n",
        "\n",
        "We will use the Multi30k for this assignment which consists of 30k German and English sentences.\n",
        "\n",
        "**Note**: When implementing beam search, to keep things simple we will not use batching (beam search on one sentence at a time). However, for implementing the decoders, please use batching.\n",
        "\n",
        "**Grading Rubric**\n",
        "- 70% results\n",
        "  - 20% seq2seq_predictions_baseline.json (meets target)\n",
        "  - 20% seq2seq_predictions_attention.json (meets target)\n",
        "  - 20% beam_seqs.json (correctness)\n",
        "  - 10% seq2seq_predictions_attention.json (improvement)\n",
        "  \n",
        "- 30% writeup\n",
        " - 12.5% clarity\n",
        " - 12.5% correctness\n",
        " - 5% interestingness of ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNTSYtkLhCw"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Feel free to add other libraries here (that don't already implement what you are supposed to!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "pMgTktaSLgVy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade sacrebleu sentencepiece gdown\n",
        "# Standard library imports\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Third party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "import sentencepiece\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm.notebook\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ETWUf7d978"
      },
      "source": [
        "Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU. Manage this by go to the Runtime tab in your colab.\n",
        "We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.\n",
        "\n",
        "Note that if you use \"CPU\" training time would be much slower depending on the CPU (likely 20 times slower). So use of GPU is recommended, be sure to manage your GPU so that it doesn't run out of quota."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FvPeUJmd2nM",
        "outputId": "2123d9ee-7d2c-4f08-ab26-aaea10ab3f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    assert torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\")\n",
        "except:\n",
        "    device = torch.device(\"mps\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxHeAMdOMZbq"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data for this assignment comes from the [Multi30K dataset](https://arxiv.org/abs/1605.00459), which contains English and German captions for images from Flickr. We can download it using `gdown`. We use the Multi30K dataset because it is simpler than standard translation benchmark datasets and allows for models to be trained and evaluated in a matter of minutes rather than days using a GPU.\n",
        "\n",
        "We will be translating from German to English in this assignment, but the same techniques apply equally well to any language pair.\n",
        "\n",
        "\n",
        "\n",
        "**You do not need to modify anything in this section.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd70s6-3n2vJ"
      },
      "source": [
        "First let's download the data and visualize some of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwiYczSz6B7n",
        "outputId": "b2f96e6a-70ed-4d90-90b5-464684300aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ll4fDiPLQ0u9osdtSlsUcehK_p_2dykV\n",
            "To: /Users/yd211/Documents/pace/Yash/training_data.json\n",
            "100%|██████████████████████████████████████| 4.28M/4.28M [00:00<00:00, 12.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OEBVpX9F2FX0Mqj17jOWJKI2efUN_HBR\n",
            "To: /Users/yd211/Documents/pace/Yash/validation_data.json\n",
            "100%|████████████████████████████████████████| 152k/152k [00:00<00:00, 1.67MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zZF8EXtzcd3oXSGEfyKywkSMosX_T6Jo\n",
            "To: /Users/yd211/Documents/pace/Yash/test_data.json\n",
            "100%|██████████████████████████████████████| 80.2k/80.2k [00:00<00:00, 1.27MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1ll4fDiPLQ0u9osdtSlsUcehK_p_2dykV\n",
        "!gdown 1OEBVpX9F2FX0Mqj17jOWJKI2efUN_HBR\n",
        "!gdown 1zZF8EXtzcd3oXSGEfyKywkSMosX_T6Jo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFWARAfahBaT",
        "outputId": "19fc1dc3-51b4-4105-e6ae-ebd0b9d45660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 29001\n",
            "Number of validation examples: 1015\n",
            "Number of test examples: 1000\n",
            "\n",
            "Mean sequence length of training data: 70.5822557842833\n",
            "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "Two young, White males are outside near many bushes.\n",
            "\n",
            "Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
            "Several men in hard hats are operating a giant pulley system.\n",
            "\n",
            "Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "A little girl climbing into a wooden playhouse.\n",
            "\n",
            "Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "\n",
            "Zwei Männer stehen am Herd und bereiten Essen zu.\n",
            "Two men are at the stove preparing food.\n",
            "\n",
            "Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
            "A man in green holds a guitar while the other man observes his shirt.\n",
            "\n",
            "Ein Mann lächelt einen ausgestopften Löwen an.\n",
            "A man is smiling at a stuffed lion\n",
            "\n",
            "Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
            "A trendy girl talking on her cellphone while gliding slowly down the street.\n",
            "\n",
            "Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\n",
            "A woman with a large purse is walking by a gate.\n",
            "\n",
            "Jungen tanzen mitten in der Nacht auf Pfosten.\n",
            "Boys dancing on poles in the middle of the night.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(\"training_data.json\",\"r\") as f:\n",
        "    training_data = json.load(f)\n",
        "with open(\"validation_data.json\",\"r\") as f:\n",
        "    validation_data = json.load(f)\n",
        "with open(\"test_data.json\",\"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(\"Number of training examples:\", len(list(training_data)))\n",
        "print(\"Number of validation examples:\", len(list(validation_data)))\n",
        "print(\"Number of test examples:\", len(list(test_data)))\n",
        "print()\n",
        "\n",
        "# Calculate and print mean sequence length for training data\n",
        "mean_length = sum(len(example[0]) for example in training_data) / len(training_data)\n",
        "print(\"Mean sequence length of training data:\", mean_length)\n",
        "\n",
        "for example in training_data[:10]:\n",
        "  print(example[0])\n",
        "  print(example[1])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZILrv5m9oyDQ"
      },
      "source": [
        "Vocabulary:\n",
        "Now We can use `sentencepiece` to create a joint German-English subword vocabulary from the training corpus. Subwords are words being divided into smaller pieces. They usually provide better performance since it takes advantages over common section among different words (see more at https://huggingface.co/docs/transformers/en/tokenizer_summary) and it handles Out of Vocabulary (OOV) words a lot better (https://blog.octanove.org/guide-to-subword-tokenization/). Because the number of training examples is small, we choose a smaller vocabulary size than would be used for large-scale NMT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS5TwOStbUlL"
      },
      "source": [
        "Let's download the English and German training corpous to construct our vocabulary. The two files downloaded here contains English and German sentences are from the training data we downloaded above but are decoupled to be used for the `sentencepiece` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF_S8ObkHHQA",
        "outputId": "42b9cdbe-d9f7-437a-f95c-889c0454eea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bO7SVCjvVzp__ibwED8wbMRSiJQNNP52\n",
            "To: /storage/ice1/3/4/pponnusamy7/Yash/train.en\n",
            "100%|██████████████████████████████████████| 1.80M/1.80M [00:00<00:00, 65.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1A2w-F6kmUXNtuFtG2qdpfw0dx2Mk7phR\n",
            "To: /storage/ice1/3/4/pponnusamy7/Yash/train.de\n",
            "100%|██████████████████████████████████████| 2.11M/2.11M [00:00<00:00, 39.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1bO7SVCjvVzp__ibwED8wbMRSiJQNNP52\n",
        "!gdown 1A2w-F6kmUXNtuFtG2qdpfw0dx2Mk7phR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eseil4-uj4e7"
      },
      "source": [
        "We will use a unigram language model for subword segmentation (https://aclanthology.org/P18-1007.pdf). There are other techniques such as using BPE and or using characters, but we won't explore into them here (you can consider trying different subword strategies to improve your model later in the improvement section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZAohTmoLMbMh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --input=train.de,train.en --vocab_size=8000 --model_prefix=Multi30k\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: train.de\n",
            "  input: train.en\n",
            "  input_format: \n",
            "  model_prefix: Multi30k\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 3\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: 0\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: train.de\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: train.en\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 58000 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=3877107\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9537% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=60\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999537\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 58000 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=2139779\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 81782 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 58000\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 39388\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 39388 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=24756 obj=10.2757 num_tokens=79134 num_tokens/piece=3.19656\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=20873 obj=8.08679 num_tokens=79533 num_tokens/piece=3.81033\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=15641 obj=8.07036 num_tokens=85721 num_tokens/piece=5.48053\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=15630 obj=8.04388 num_tokens=85743 num_tokens/piece=5.4858\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11722 obj=8.16114 num_tokens=95990 num_tokens/piece=8.18888\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11722 obj=8.13206 num_tokens=95994 num_tokens/piece=8.18922\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=8.30125 num_tokens=107803 num_tokens/piece=12.2503\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=8.27055 num_tokens=107797 num_tokens/piece=12.2497\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: Multi30k.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: Multi30k.vocab\n"
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    \"pad_id\": 0,\n",
        "    \"bos_id\": 1,\n",
        "    \"eos_id\": 2,\n",
        "    \"unk_id\": 3,\n",
        "    \"input\": \"train.de,train.en\",\n",
        "    \"vocab_size\": 8000,\n",
        "    \"model_prefix\": \"Multi30k\",\n",
        "    # \"model_type\": \"word\",\n",
        "}\n",
        "combined_args = \" \".join(\n",
        "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
        "sentencepiece.SentencePieceTrainer.Train(combined_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px9gmkJfvPkd"
      },
      "source": [
        "This creates two files: `Multi30k.model` and `Multi30k.vocab`. The first is a binary file containing the relevant data for the vocabulary. The second is a human-readable listing of each subword and its associated score. The score is the logged probability of the subword in the corpus. A higher score means that subword appears more frequently in the corpus.\n",
        "\n",
        "`sentencepiece` trainer basically finds a set of those subwords such that their joint probabily maximaizes over the corpus. How do you find the correct segmentation of words to subword such that you can maximize this joint probability becomes the question. This can be done by using a Viterbi algorithm (you implemented last assignment!). You don't need to know exactly how this is done but if you are intrested you can look into this [paper](https://aclanthology.org/P18-1007.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5ZC5rQpvV-7"
      },
      "source": [
        "You can preveiw some of the word scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z56YdM52vbQq",
        "outputId": "fcff34e0-023b-42fa-f029-faf117f5683a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad>\t0\n",
            "<s>\t0\n",
            "</s>\t0\n",
            "<unk>\t0\n",
            ".\t-2.72718\n",
            "▁a\t-3.21357\n",
            "▁in\t-3.43973\n",
            "m\t-3.78503\n",
            "▁eine\t-3.82141\n",
            "▁A\t-3.86856\n",
            "s\t-4.06457\n",
            "▁Ein\t-4.11399\n",
            ",\t-4.20405\n",
            "▁the\t-4.35217\n",
            "▁und\t-4.5704\n",
            "▁mit\t-4.57911\n",
            "▁auf\t-4.58144\n",
            "▁on\t-4.65674\n",
            "n\t-4.67038\n",
            "▁Mann\t-4.70521\n",
            "▁is\t-4.73988\n",
            "▁man\t-4.75331\n",
            "▁and\t-4.76404\n",
            "▁\t-4.76512\n",
            "ing\t-4.8072\n",
            "▁of\t-4.83344\n",
            "▁einer\t-4.86421\n",
            "▁with\t-4.93426\n",
            "▁Eine\t-4.98902\n",
            "▁ein\t-5.126\n"
          ]
        }
      ],
      "source": [
        "!head -n 30 Multi30k.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK9W1jt0vmBk"
      },
      "source": [
        "As we can see, the vocabulary consists of four special tokens (`<pad>` for padding, `<s>` for beginning of sentence (BOS), `</s>` for end of sentence (EOS), `<unk>` for unknown) and a mixture of German and English words and subwords. In order to ensure reversability, word boundaries are encoded with a special unicode character \"▁\" (U+2581)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suOYWS_AwBzz"
      },
      "source": [
        "To use the vocabulary, we first need to load it from the binary file produced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuqd9EhmwEDQ",
        "outputId": "ba40589d-96e2-45aa-d14a-276cbde854ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = sentencepiece.SentencePieceProcessor()\n",
        "vocab.Load(\"Multi30k.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRZkyH-xwIx4"
      },
      "source": [
        "The vocabulary object includes a number of methods for working with full sequences or individual pieces. We explore the most relevant ones below. A complete interface can be found on [GitHub](https://github.com/google/sentencepiece/tree/master/python#usage) for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL507EdMtSdD",
        "outputId": "fe50a986-1dc8-406a-ec6c-1f67cfc31b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 8000\n",
            "\n",
            "Two young, White males are outside near many bushes.\n",
            "['▁Two', '▁young', ',', '▁White', '▁males', '▁are', '▁outside', '▁near', '▁many', '▁bushes', '.']\n",
            "Two young, White males are outside near many bushes.\n",
            "[42, 54, 12, 2889, 2225, 36, 127, 173, 815, 3513, 4]\n",
            "Two young, White males are outside near many bushes.\n",
            "\n",
            "Several men in hard hats are operating a giant pulley system.\n",
            "['▁Se', 'veral', '▁men', '▁in', '▁hard', '▁hats', '▁are', '▁operating', '▁a', '▁g', 'iant', '▁pull', 'e', 'y', '▁s', 'y', 'ste', 'm', '.']\n",
            "Several men in hard hats are operating a giant pulley system.\n",
            "[298, 240, 73, 6, 712, 730, 36, 3106, 5, 631, 1679, 583, 32, 96, 552, 96, 1076, 7, 4]\n",
            "Several men in hard hats are operating a giant pulley system.\n",
            "\n",
            "A little girl climbing into a wooden playhouse.\n",
            "['▁A', '▁little', '▁girl', '▁climbing', '▁in', 'to', '▁a', '▁wooden', '▁play', 'house', '.']\n",
            "A little girl climbing into a wooden playhouse.\n",
            "[9, 132, 66, 500, 6, 112, 5, 542, 245, 4599, 4]\n",
            "A little girl climbing into a wooden playhouse.\n",
            "\n",
            "▁the\n",
            "13\n",
            "▁the\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocabulary size:\", vocab.GetPieceSize())\n",
        "print()\n",
        "\n",
        "for example in training_data[:3]:\n",
        "  sentence = example[1]\n",
        "  pieces = vocab.EncodeAsPieces(sentence)\n",
        "  indices = vocab.EncodeAsIds(sentence)\n",
        "  print(sentence)\n",
        "  print(pieces)\n",
        "  print(vocab.DecodePieces(pieces))\n",
        "  print(indices)\n",
        "  print(vocab.DecodeIds(indices))\n",
        "  print()\n",
        "\n",
        "piece = vocab.EncodeAsPieces(\"the\")[0]\n",
        "index = vocab.PieceToId(piece)\n",
        "print(piece)\n",
        "print(index)\n",
        "print(vocab.IdToPiece(index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRzFneNwNOi"
      },
      "source": [
        "We define some constants here for the first three special tokens that you may find useful in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzmImUrYwN0J",
        "outputId": "31fd911f-4f68-4476-b68c-46a76b25d611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad>: 0, <s>: 1, </s>: 2\n"
          ]
        }
      ],
      "source": [
        "pad_id = vocab.PieceToId(\"<pad>\")\n",
        "bos_id = vocab.PieceToId(\"<s>\")\n",
        "eos_id = vocab.PieceToId(\"</s>\")\n",
        "print(f\"<pad>: {pad_id}, <s>: {bos_id}, </s>: {eos_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRLxTA3wUjw"
      },
      "source": [
        "Note that these tokens will be stripped from the output when converting from word pieces to text. This may be helpful when implementing greedy search and beam search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DOK5hzbwRqC",
        "outputId": "766ca63c-cf90-4923-d483-86cbaff495a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Two young, White males are outside near many bushes.\n",
            "Two young, White males are outside near many bushes.\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "sentence = training_data[0][1]\n",
        "indices = vocab.EncodeAsIds(sentence)\n",
        "indices_augmented = [bos_id] + indices + [eos_id, pad_id, pad_id, pad_id]\n",
        "print(vocab.DecodeIds(indices))\n",
        "print(vocab.DecodeIds(indices_augmented))\n",
        "print(vocab.DecodeIds(indices) == vocab.DecodeIds(indices_augmented))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zdx72ZEY6El"
      },
      "source": [
        "Code for saving your results for submission. You don't have to read this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A0HQOLF3Y4-B"
      },
      "outputs": [],
      "source": [
        "# Please do not change the code below\n",
        "def generate_predictions_file_for_submission(filepath, model, dataset, method, batch_size=64):\n",
        "    assert method in {\"greedy\", \"beam\"}\n",
        "    source_sentences = [example[0] for example in dataset]\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "      for start_index in range(0, len(source_sentences), batch_size):\n",
        "        if method == \"greedy\":\n",
        "          prediction_batch = predict_greedy(\n",
        "              model, source_sentences[start_index:start_index + batch_size])\n",
        "          prediction_batch = [[x] for x in prediction_batch]\n",
        "        else:\n",
        "          prediction_batch = predict_beam(\n",
        "              model, source_sentences[start_index:start_index + batch_size])\n",
        "        predictions.extend(prediction_batch)\n",
        "    with open(filepath, \"w\") as outfile:\n",
        "        json.dump(predictions, outfile, indent=2)\n",
        "    print(\"Finished writing predictions to {}.\".format(filepath))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgmN23YvdX1M"
      },
      "source": [
        "## Seq2Seq Machine Translation Model\n",
        "\n",
        "Now let's implement a sequence-to-sequence machine translation model. We will first implement an `Encode` and an `Decode` method and then put these together in a `Seq2seqBaseline` class.\n",
        "\n",
        "We have implemented the `Encode` method below which encodes input sequences using a bi-directional LSTM. A bi-LSTM consists of a stack of two LSTM networks, one which processes the sequence in forward direction and another which processes the sequence in reverse direction. The output hidden states from both are concatenated to get the representations at each position. Further, we average the final states in either direction before returning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOL73L9wDpo6"
      },
      "source": [
        "### **Implementation Task \\# 1**\n",
        "\n",
        "Let's begin by defining a batch iterator for the training data. Given a dataset and a batch size, it will iterate over the dataset and yield pairs of tensors containing the subword indices for the source and target sentences in the batch, respectively.  We filled in `make_batch` below. It is advised to read the code below to get a sense of how sentences are tokenized and batched.\n",
        "\n",
        "**Note**: Maybe a little different from previous assignments, we are keeping batch_size to be the **2nd** dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfLjDrg6ynL8",
        "outputId": "06e2ce6b-08a1-421b-b321-4248479e36e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example batch tensor:\n",
            "tensor([[   1,    1],\n",
            "        [   5,    5],\n",
            "        [3966,  354],\n",
            "        [   6,   60],\n",
            "        [ 236,    6],\n",
            "        [ 698,  236],\n",
            "        [   2,  698],\n",
            "        [   0, 5285],\n",
            "        [   0,   13],\n",
            "        [   0, 3759],\n",
            "        [   0,    2]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def make_batch(sentences):\n",
        "  \"\"\"Convert a list of sentences into a batch of subword indices.\n",
        "\n",
        "  Args:\n",
        "    sentences: A list of sentences, each of which is a string.\n",
        "\n",
        "  Returns:\n",
        "    A LongTensor of size (max_sequence_length, batch_size) containing the\n",
        "    subword indices for the sentences, where max_sequence_length is the length\n",
        "    of the longest sentence as encoded by the subword vocabulary and batch_size\n",
        "    is the number of sentences in the batch. A beginning-of-sentence token\n",
        "    should be included before each sequence, and an end-of-sentence token should\n",
        "    be included after each sequence. Empty slots at the end of shorter sequences\n",
        "    should be filled with padding tokens. The tensor should be located on the\n",
        "    device defined at the beginning of the notebook.\n",
        "  \"\"\"\n",
        "\n",
        "  batch_indices = []\n",
        "  for sentence in sentences:\n",
        "    indices = vocab.EncodeAsIds(sentence)\n",
        "    indices_augmented = [bos_id] + indices + [eos_id]\n",
        "    indices_augmented = torch.LongTensor(indices_augmented)\n",
        "    batch_indices.append(indices_augmented)\n",
        "\n",
        "  batched_seq = torch.nn.utils.rnn.pad_sequence(batch_indices, padding_value=pad_id).to(device)\n",
        "  return batched_seq\n",
        "\n",
        "def make_batch_iterator(dataset, batch_size, shuffle=False):\n",
        "  \"\"\"Make a batch iterator that yields source-target pairs.\n",
        "\n",
        "  Args:\n",
        "    dataset: A torchtext dataset object.\n",
        "    batch_size: An integer batch size.\n",
        "    shuffle: A boolean indicating whether to shuffle the examples.\n",
        "\n",
        "  Yields:\n",
        "    Pairs of tensors constructed by calling the make_batch function on the\n",
        "    source and target sentences in the current group of examples. The max\n",
        "    sequence length can differ between the source and target tensor, but the\n",
        "    batch size will be the same. The final batch may be smaller than the given\n",
        "    batch size.\n",
        "  \"\"\"\n",
        "\n",
        "  examples = list(dataset)\n",
        "  if shuffle:\n",
        "    random.shuffle(examples)\n",
        "\n",
        "  for start_index in range(0, len(examples), batch_size):\n",
        "    example_batch = examples[start_index:start_index + batch_size]\n",
        "    source_sentences = [example[0] for example in example_batch]\n",
        "    target_sentences = [example[1] for example in example_batch]\n",
        "    yield make_batch(source_sentences), make_batch(target_sentences)\n",
        "\n",
        "test_batch = make_batch([\"a test input\", \"a longer input than the first\"])\n",
        "print(\"Example batch tensor:\")\n",
        "print(test_batch)\n",
        "assert test_batch.shape[1] == 2\n",
        "assert test_batch[0, 0] == bos_id\n",
        "assert test_batch[0, 1] == bos_id\n",
        "assert test_batch[-1, 0] == pad_id\n",
        "assert test_batch[-1, 1] == eos_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6VhQOnXyprP"
      },
      "source": [
        "Implement an LSTM based decoder below. The decoder should be similar to the encoder which is already implemented, except it will accept a `state` tuple with the initial values of the `h_n` and `c_n` states (which will be the final states from the encoder above). Also the inputs to the decoder will be embed using the same embedder from the encoder. We will also return the final state from the decoder since we will need it for inference later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wbe4yj_l4QQ0"
      },
      "outputs": [],
      "source": [
        "class Seq2seqBaseline(nn.Module):\n",
        "  def __init__(self, hidden_dim, word_vector_dim, dropout,num_layers):\n",
        "    super().__init__()\n",
        "    \"\"\"\n",
        "    args:\n",
        "      hidden_dim: hidden state size of LSTM\n",
        "      word_vector_dim: size of the word embedding table\n",
        "      dropout: this is applied to the output of the LSTM\n",
        "    \"\"\"\n",
        "    ### Encoder Params. Please do not change these functions at all.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Embedding table over input vocabulary\n",
        "    self.embedder = nn.Embedding(vocab.GetPieceSize(), word_vector_dim)\n",
        "    self.lstm = nn.LSTM(word_vector_dim, hidden_dim, bidirectional=True, num_layers=num_layers)\n",
        "    self.layer = nn.Linear(hidden_dim*num_layers*2, hidden_dim*num_layers)\n",
        "    self.layer2 = nn.Linear(hidden_dim*num_layers*2, hidden_dim*num_layers)\n",
        "    self.num_layers = num_layers\n",
        "    ### Decoder Params.\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.lstm2 = nn.LSTM(word_vector_dim, hidden_dim, num_layers=2)\n",
        "    self.output_layer2 = nn.Linear(hidden_dim, vocab.GetPieceSize())\n",
        "    self.log_softmax_layer = nn.LogSoftmax(dim=2)\n",
        "\n",
        "  def encode(self, source):\n",
        "    \"\"\"Encode the source batch using a bidirectional LSTM encoder.\n",
        "\n",
        "    Args:\n",
        "      source: An integer tensor with shape (max_source_sequence_length,\n",
        "        batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        encoder_output: The output of the bidirectional LSTM with shape\n",
        "          (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "        encoder_mask: A boolean tensor with shape (max_source_sequence_length,\n",
        "          batch_size) indicating which encoder outputs correspond to padding\n",
        "          tokens. Its elements should be True at positions corresponding to\n",
        "          padding tokens and False elsewhere.\n",
        "        encoder_hidden: The final hidden states of the bidirectional LSTM (after\n",
        "          a suitable projection) that will be used to initialize the decoder.\n",
        "          This should be a pair of tensors (h_n, c_n), each with shape\n",
        "          (num_layers, batch_size, hidden_size). Note that the hidden state\n",
        "          returned by the LSTM cannot be used directly. Its initial dimension is\n",
        "          twice the required size because it contains state from two directions.\n",
        "\n",
        "    The first two return values are not required for the baseline model and will\n",
        "    only be used later in the attention model. If desired, they can be replaced\n",
        "    with None for the initial implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Using packed sequences to more easily work\n",
        "    # with the variable-length sequences represented by the source tensor.\n",
        "    # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "\n",
        "    # Compute a tensor containing the length of each source sequence.\n",
        "    lengths = torch.sum(source != pad_id, axis=0).cpu()\n",
        "\n",
        "    seq_len, batch_size = source.size()\n",
        "\n",
        "    # embedded_sentence: seq_len x batch_size x word_vector_dim\n",
        "    embedded_sentence = self.embedder(source)\n",
        "\n",
        "    # pack it for rnn input\n",
        "    embedded_sentence = torch.nn.utils.rnn.pack_padded_sequence(embedded_sentence,lengths,enforce_sorted=False)\n",
        "\n",
        "    # lstm_out: seq_len x batch_size x 2 * hidden_dim\n",
        "    # h_n, c_n: num_lay*2 x batch_size x hidden_dim\n",
        "    lstm_out, (h_n, c_n) = self.lstm(embedded_sentence)\n",
        "\n",
        "    # Take sum of states across forward and reverse directions.\n",
        "    h_n = h_n.view(2, -1, batch_size, h_n.shape[-1]).sum(1)\n",
        "    c_n = c_n.view(2, -1, batch_size, c_n.shape[-1]).sum(1)\n",
        "\n",
        "    encoder_mask = source == pad_id\n",
        "\n",
        "    lstm_out, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "    return lstm_out, encoder_mask,(h_n, c_n)\n",
        "\n",
        "\n",
        "\n",
        "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "    \"\"\"Run the decoder LSTM starting from an initial hidden state.\n",
        "\n",
        "    The third and fourth arguments are not used in the baseline model, but are\n",
        "    included for compatibility with the attention model in the next section.\n",
        "\n",
        "    Args:\n",
        "      decoder_input: An integer tensor with shape (max_decoder_sequence_length,\n",
        "        batch_size) containing the subword indices for the decoder input. During\n",
        "        evaluation, where decoding proceeds one step at a time, the initial\n",
        "        dimension should be 1.\n",
        "      initial_hidden: A pair of tensors (h_0, c_0) representing the initial\n",
        "        state of the decoder, each with shape (2, batch_size,\n",
        "        hidden_size).\n",
        "      encoder_output: The output of the encoder with shape\n",
        "        (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "      encoder_mask: The output mask from the encoder with shape\n",
        "        (max_source_sequence_length, batch_size). Encoder outputs at positions\n",
        "        with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        log_probs: A tensor with shape (max_decoder_sequence_length, batch_size,\n",
        "          vocab_size) containing scores for the next-word\n",
        "          predictions at each position.\n",
        "        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as\n",
        "          initial_hidden representing the updated decoder state after processing\n",
        "          the decoder input.\n",
        "        attention_weights: This will be implemented later in the attention\n",
        "          model, but in order to maintain compatible type signatures, we also\n",
        "          include it here. This can be None or any other placeholder value.\n",
        "    \"\"\"\n",
        "\n",
        "    # These arguments are not used in the baseline model.\n",
        "    del encoder_output\n",
        "    del encoder_mask\n",
        "\n",
        "    # For the baseline model, we ignore encoder_output and encoder_mask.\n",
        "    # Embed the decoder input\n",
        "    embedded = self.embedder(decoder_input)  #  embed decoder input\n",
        "    # Run through the decoder LSTM using the provided initial hidden state.\n",
        "    outputs, hidden = self.lstm2(embedded, initial_hidden)  #  call to lstm2; outputs: (seq_len, batch, hidden_dim).\n",
        "    outputs = self.dropout2(outputs)  \n",
        "    logits = self.output_layer2(outputs)  #  project to vocabulary size.\n",
        "    log_probs = self.log_softmax_layer(logits)  #  compute log probabilities.\n",
        "    return log_probs, hidden, None  # * Returns log_probs, updated hidden state, and None for attention.\n",
        "\n",
        "\n",
        "  def compute_loss(self, source, target):\n",
        "    \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "    Args:\n",
        "      source: An integer tensor with shape (max_source_sequence_length,\n",
        "        batch_size) containing subword indices for the source sentences.\n",
        "      target: An integer tensor with shape (max_target_sequence_length,\n",
        "        batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "    Returns:\n",
        "      A scalar float tensor representing cross-entropy loss on the current batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Note that for a target sequence like <s> A B C </s>, you would\n",
        "    # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "    # suffix A B C </s>.\n",
        "\n",
        "    _, batch_size = source.size()\n",
        "    enc_output, encoder_mask, curr_state = self.encode(source)\n",
        "\n",
        "    lengths = torch.sum(target != pad_id, axis=0).cpu()-1\n",
        "    target_prefix = torch.clone(target).cpu()\n",
        "    target_prefix[lengths,torch.arange(target_prefix.size(1))] = pad_id\n",
        "    decoder_input = target_prefix[:-1,:].to(device)\n",
        "\n",
        "    log_probs, _, _ = self.decode(decoder_input, curr_state, enc_output, encoder_mask)\n",
        "\n",
        "    criterion = nn.NLLLoss(ignore_index=pad_id)\n",
        "    target = target[1:,:]\n",
        "    loss = criterion(log_probs.reshape(-1, vocab.GetPieceSize()), target.reshape(-1))\n",
        "\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_UVQzoeHaGn"
      },
      "source": [
        "We define the following functions for training.  This code will run as provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UqhYwvEPErel"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, batch_size, model_file, dataset=None):\n",
        "  \"\"\"Train the model and save its best checkpoint.\n",
        "\n",
        "  Model performance across epochs is evaluated using token-level accuracy on the\n",
        "  validation set. The best checkpoint obtained during training will be stored on\n",
        "  disk and loaded back into the model at the end of training.\n",
        "  \n",
        "  Args:\n",
        "      model: The model to be trained.\n",
        "      num_epochs: Number of epochs for training.\n",
        "      batch_size: Size of each training batch.\n",
        "      model_file: File path to save the best model checkpoint.\n",
        "      dataset: Optional dataset to use for training; if None, uses the global training_data.\n",
        "  \"\"\"\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  best_accuracy = 0.0\n",
        "  training_data_to_use = dataset if dataset is not None else training_data\n",
        "  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "    with tqdm.notebook.tqdm(\n",
        "        make_batch_iterator(training_data_to_use, batch_size, shuffle=True),\n",
        "        desc=\"epoch {}\".format(epoch + 1),\n",
        "        unit=\"batch\",\n",
        "        total=math.ceil(len(training_data_to_use) / batch_size)) as batch_iterator:\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      for i, (source, target) in enumerate(batch_iterator, start=1):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.compute_loss(source, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
        "      validation_perplexity, validation_accuracy = evaluate_next_token(\n",
        "          model, validation_data)\n",
        "      batch_iterator.set_postfix(\n",
        "          mean_loss=total_loss / i,\n",
        "          validation_perplexity=validation_perplexity,\n",
        "          validation_token_accuracy=validation_accuracy)\n",
        "      if validation_accuracy > best_accuracy:\n",
        "        print(\n",
        "            \"Obtained a new best validation accuracy of {:.2f}, saving model \"\n",
        "            \"checkpoint to {}...\".format(validation_accuracy, model_file))\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        best_accuracy = validation_accuracy\n",
        "  print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "  model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "def evaluate_next_token(model, dataset, batch_size=64):\n",
        "  \"\"\"Compute token-level perplexity and accuracy metrics.\n",
        "\n",
        "  Note that the perplexity here is over subwords, not words.\n",
        "\n",
        "  This function is used for validation set evaluation at the end of each epoch\n",
        "  and should not be modified.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  total_cross_entropy = 0.0\n",
        "  total_predictions = 0\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for source, target in make_batch_iterator(dataset, batch_size):\n",
        "      encoder_output, encoder_mask, encoder_hidden = model.encode(source)\n",
        "      decoder_input, decoder_target = target[:-1], target[1:]\n",
        "      logits, decoder_hidden, attention_weights = model.decode(\n",
        "          decoder_input, encoder_hidden, encoder_output, encoder_mask)\n",
        "      total_cross_entropy += F.cross_entropy(\n",
        "          logits.permute(1, 2, 0), decoder_target.permute(1, 0),\n",
        "          ignore_index=pad_id, reduction=\"sum\").item()\n",
        "      total_predictions += (decoder_target != pad_id).sum().item()\n",
        "      correct_predictions += (\n",
        "          (decoder_target != pad_id) &\n",
        "          (decoder_target == logits.argmax(2))).sum().item()\n",
        "  perplexity = math.exp(total_cross_entropy / total_predictions)\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  return perplexity, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwREgE23qsUz"
      },
      "source": [
        "We can now train the baseline model.\n",
        "\n",
        "Since we haven't yet defined a decoding method to output an entire string, we will measure performance for now by computing perplexity and the accuracy of predicting the next token given a gold prefix of the output. A correct implementation should get a validation token accuracy above 55%. The training code will automatically save the model with the highest validation accuracy and reload that checkpoint's parameters at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "3c9f7733d79144858625e0ecd021a69f",
            "2764139c35a54e959edd1de1e7314d66",
            "dc687e032e554315a45fec33c34c54b4",
            "c535f8c5d4374e38bd90852e304ecda7",
            "7436c93f90f04e53a5c6bda85ce4aab9",
            "f485197cc34446459899c7e55e445e39",
            "57beb3432a55408dbec4bb9ff52c043e",
            "6d4db5079eaf428fbc4f927a80c2656e",
            "1fb0ca0f9b7f4908820e73e965a4e905",
            "8999ee1ea4ec42f4b37aaf7238167a48",
            "54149af30d3549e3806aac19109aedf0",
            "1826ebd2711a4dd585ea23bd33f85255",
            "1bcfbd7357174878a73a9e07ee0f5abb",
            "c6c99c69be5c4c6c8f106198556f9187",
            "112db3e457d04733943b7f950963915d",
            "45034a27da974ab884f3c8cecf147aa9",
            "db7d7f2dfa5e4d1883f77aae26b2194a",
            "1355e071eafd452f9f3fe820471eb93e",
            "1b61edddb3be45f988f2a886295b0e0a",
            "ed34af228b464218805d6068e6b42fc4",
            "bb57be9341cf484b8337a29c30ac6559",
            "2e52169afdbe4e1783c54b457234ca16"
          ]
        },
        "id": "9HamQt-oHgC2",
        "outputId": "ec92f64a-a9f2-4d82-b824-be57e5db1e0f",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18c3581fcf4d4690a1355b22ff2971ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aeff19ff3da42fbbfadf7f8767dbbec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 1:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 43.43, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "503aeb7aa8ba41569713c18ac4b56bfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 2:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 47.64, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b5ac97e0479459f9cac5cbc1bbd3f95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 3:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 50.64, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "babc740ed8f741039feb7bb4e2e0f4a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 4:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 52.76, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69929eb789bd40089af6150c6b538689",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 5:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 53.76, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35cbb0a7789842dea8458c145b11698c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 6:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 54.47, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6209e5cb753c49c19eb0d95bb9329f4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 7:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 55.11, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a38c94c7bbb4f2d8ed6ff05b4d8d648",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 8:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 55.41, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8233e380913469a935f6c723d969d5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 9:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 55.54, saving model checkpoint to baseline_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7cab23b1607485ca3d19c733c795ca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 10:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading best model checkpoint from baseline_model.pt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1110052/364093175.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_file))\n"
          ]
        }
      ],
      "source": [
        "# We tune it to be fairly optimal, so idealy you don't have to change the parameters below.\n",
        "# But you are welcome to adjust these parameters.\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "hidden_dim = 256\n",
        "word_vector_dim = 256\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "baseline_model = Seq2seqBaseline(hidden_dim,word_vector_dim,dropout,num_layers).to(device)\n",
        "train(baseline_model, num_epochs, batch_size, \"baseline_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lvQiTeih4T3"
      },
      "source": [
        "For evaluation, we also need to be able to generate entire strings from the model. We gave a greedy decoding algorithm here. Later on, we'll implement beam search.\n",
        "\n",
        "A correct implementation of baseline model with greedy decoding should get above 19 BLEU on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RBbbZodXKq9J",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline model validation BLEU using greedy search: 19.66505186048235\n",
            "Finished writing predictions to seq2seq_predictions_baseline.json.\n"
          ]
        }
      ],
      "source": [
        "def predict_greedy(model, sentences, max_length=100):\n",
        "  \"\"\"Make predictions for the given inputs using greedy inference.\n",
        "\n",
        "  Args:\n",
        "    model: A sequence-to-sequence model.\n",
        "    sentences: A list of input sentences, represented as strings.\n",
        "    max_length: The maximum length at which to truncate outputs in order to\n",
        "      avoid non-terminating inference.\n",
        "\n",
        "  Returns:\n",
        "    A list of predicted translations, represented as strings.\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  batch_size = len(sentences)\n",
        "  indices = make_batch(sentences)\n",
        "  pred_translations = torch.zeros(max_length,batch_size, dtype=torch.long) # max_seq_length x batch_size\n",
        "\n",
        "  enc_output, encoder_mask, curr_state = model.encode(indices)\n",
        "  input = torch.LongTensor([bos_id] * batch_size).view(1, -1).to(device)\n",
        "  finished_mask = torch.zeros(batch_size, dtype=torch.bool).to(device) # mask ones that have finished because some may finish ealier than the other in the same batch\n",
        "  for i in range(0, max_length):\n",
        "      log_probs, hidden, _ = model.decode(input, curr_state, enc_output, encoder_mask)\n",
        "\n",
        "      # Prevent finished sequences from producing non-padding tokens.\n",
        "      log_probs[:, finished_mask, pad_id] = 1e9\n",
        "\n",
        "      # Get the most likely next token and its index.\n",
        "      _, next_tokens = log_probs.squeeze(0).max(dim=1)\n",
        "      pred_translations[i] = next_tokens\n",
        "\n",
        "      # Update the input for the next decoding step.\n",
        "      input = next_tokens.unsqueeze(0)\n",
        "\n",
        "      # Update the state and finished masks.\n",
        "      curr_state = hidden\n",
        "\n",
        "      finished_mask = finished_mask | next_tokens.eq(eos_id)\n",
        "      if finished_mask.all():\n",
        "          break\n",
        "\n",
        "  pred_translation_str = []\n",
        "  for i in range(batch_size):\n",
        "      string = vocab.DecodeIds(pred_translations[:,i].detach().cpu().numpy().astype(int).tolist())\n",
        "      pred_translation_str.append(string)\n",
        "  return pred_translation_str\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, batch_size=64, method=\"greedy\"):\n",
        "  assert method in {\"greedy\", \"beam\"}\n",
        "  source_sentences = [example[0] for example in dataset]\n",
        "  target_sentences = [example[1] for example in dataset]\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    for start_index in range(0, len(source_sentences), batch_size):\n",
        "      if method == \"greedy\":\n",
        "        prediction_batch = predict_greedy(\n",
        "            model, source_sentences[start_index:start_index + batch_size])\n",
        "      else:\n",
        "        prediction_batch = predict_beam(\n",
        "            model, source_sentences[start_index:start_index + batch_size])\n",
        "        prediction_batch = [candidates[0] for candidates in prediction_batch]\n",
        "      predictions.extend(prediction_batch)\n",
        "  return sacrebleu.corpus_bleu(predictions, [target_sentences]).score\n",
        "\n",
        "print(\"Baseline model validation BLEU using greedy search:\",\n",
        "      evaluate(baseline_model, validation_data))\n",
        "\n",
        "### Generate the predictions for the baseline model using greedy decoding on the test_data.\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_baseline.json\", baseline_model, test_data, \"greedy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qVQO5ouAKuFQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline model sample predictions:\n",
            "\n",
            "Input:\n",
            "  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
            "Target:\n",
            "  A group of men are loading cotton onto a truck\n",
            "Greedy prediction:\n",
            "  A group of men are looking out of a building.\n",
            "\n",
            "Input:\n",
            "  Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
            "Target:\n",
            "  A man sleeping in a green room on a couch.\n",
            "Greedy prediction:\n",
            "  A man is sleeping in a green blanket on a couch.\n",
            "\n",
            "Input:\n",
            "  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
            "Target:\n",
            "  A boy wearing headphones sits on a woman's shoulders.\n",
            "Greedy prediction:\n",
            "  A boy in a hat sits on the side of a woman.\n",
            "\n",
            "Input:\n",
            "  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\n",
            "Target:\n",
            "  Two men setting up a blue ice fishing hut on an iced over lake\n",
            "Greedy prediction:\n",
            "  Two men are carrying a blue nets on a track in a martial arts match.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def show_predictions(model, num_examples=4, num_beam=5,include_beam=False):\n",
        "  for example in validation_data[:num_examples]:\n",
        "    print(\"Input:\")\n",
        "    print(\" \", example[0])\n",
        "    print(\"Target:\")\n",
        "    print(\" \", example[1])\n",
        "    print(\"Greedy prediction:\")\n",
        "    print(\" \", predict_greedy(model, [example[0]])[0])\n",
        "    if include_beam:\n",
        "      print(f\"Beam predictions (showing top {num_beam}):\")\n",
        "      for candidate in predict_beam(model, [example[0]])[0][:num_beam]:\n",
        "        print(\" \", candidate)\n",
        "    print()\n",
        "\n",
        "print(\"Baseline model sample predictions:\")\n",
        "print()\n",
        "show_predictions(baseline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qre2zMYwi9se"
      },
      "source": [
        "## Attention\n",
        "We'll now improve our seq2seq parsing model by adding [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) style attention. In particular, (largely following the notation in the paper) let $\\bar{\\mathbf{h}}_1, \\ldots, \\bar{\\mathbf{h}}_S$ be the sequence of *encoder* RNN states which are obtained from running the encoder RNN over $S$ source tokens (i.e., the English words in the question in our case). Also let $\\mathbf{h}_t$ be the *decoder* RNN state after it consumes the $t$th target token (i.e., the $t$th token in the logical form). Then $\\boldsymbol{\\alpha}_t$, the attention vector at time $t$, has $S$ elements defined as follows:\n",
        "$$\n",
        "\\alpha_{t,s} = \\mathrm{softmax} ([\\bar{\\mathbf{h}}_{1}^{\\top}; \\ldots; \\bar{\\mathbf{h}}_{S}^{\\top}] \\mathbf{W}_a^{\\top} \\mathbf{h}_t)_s = \\frac{\\exp(\\mathbf{h}_t^{\\top} \\mathbf{W}_a \\bar{\\mathbf{h}}_s)}{\\sum_{s'=1}^S\\exp(\\mathbf{h}_t^{\\top} \\mathbf{W}_a \\bar{\\mathbf{h}}_{s'})},\n",
        "$$\n",
        "where vectors are assumed to be column-vectors and $;$ represents vertical stacking. Note $\\mathbf{W}_a \\in \\mathbb{R}^{d_{dec} \\times d_{enc}}$ is a learnable parameter matrix.\n",
        "\n",
        "Given $\\boldsymbol{\\alpha}_t$, we can then compute a \"context vector\" $\\mathbf{c}_t$ that is a weighted average of the encoder states:\n",
        "$$\n",
        "\\mathbf{c}_t = [\\bar{\\mathbf{h}}_{1}, \\ldots, \\bar{\\mathbf{h}}_{S}] \\boldsymbol{\\alpha}_t\n",
        "$$\n",
        "where $,$ represents horizontal stacking.\n",
        "\n",
        "Finally, we concatenate $\\mathbf{c}_t$ and $\\mathbf{h}_t$ to arrive at a modified decoder state at time $t$ defined as follows:\n",
        "$$\n",
        "\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_c [\\mathbf{c}_t; \\mathbf{h}_t]),\n",
        "$$\n",
        "where $\\mathbf{W}_c \\in \\mathbb{R}^{d_{dec} + d_{enc} \\times d_{dec} + d_{enc}} $ is some learned projection. We can then obtain our logits for each word type as usual with $\\mathbf{V} \\tilde{\\mathbf{h}}_t + \\mathbf{b}$.\n",
        "\n",
        "\n",
        "### **Implementation Task \\# 2**\n",
        "Complete the `decode()` function of the `Seq2seqAttention` module below so that it implements the attention scheme described above. The function should return log probabilities and the final decoder state and cell just as the `decode()` function of the `Seq2seqBaseline` module above does.\n",
        "\n",
        "**Hint:** The most efficient implementations will make use of [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) in computing attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O552dQ8coMmp"
      },
      "outputs": [],
      "source": [
        "class Seq2seqAttention(Seq2seqBaseline):\n",
        "  # Note that this class inherents from Seq2seqBaseline, so all the parameters in Seq2seqBaseline are initialized when this class is\n",
        "  # initialized.\n",
        "  def __init__(self, hidden_dim, enc_output_size, word_vector_dim, dropout,num_layers):\n",
        "    super().__init__(hidden_dim, word_vector_dim, dropout,num_layers)\n",
        "\n",
        "\n",
        "    # Initialize any additional parameters needed for this model that are not\n",
        "    # already included in the baseline model.\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    # Initialize additional parameters for Luong-style attention.\n",
        "    self.W_a = nn.Linear(enc_output_size, hidden_dim, bias=False)  #  projects encoder outputs to hidden_dim.\n",
        "    self.W_c = nn.Linear(hidden_dim + enc_output_size, hidden_dim)   #  projects concatenated [h_t; c_t] to hidden_dim.\n",
        "    self.out = nn.Linear(hidden_dim, vocab.GetPieceSize())           #  final output layer for vocabulary logits.\n",
        "    \n",
        "    ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "    \"\"\"Run the decoder LSTM starting from an initial hidden state.\n",
        "\n",
        "    Args:\n",
        "      decoder_input: An integer tensor with shape (max_decoder_sequence_length,\n",
        "        batch_size) containing the subword indices for the decoder input. During\n",
        "        evaluation, where decoding proceeds one step at a time, the initial\n",
        "        dimension should be 1.\n",
        "      initial_hidden: A pair of tensors (h_0, c_0) representing the initial\n",
        "        state of the decoder, each with shape (num_layers, batch_size,\n",
        "        hidden_size).\n",
        "      encoder_output: The output of the encoder with shape\n",
        "        (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "      encoder_mask: The output mask from the encoder with shape\n",
        "        (max_source_sequence_length, batch_size). Encoder outputs at positions\n",
        "        with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        logits: A tensor with shape (max_decoder_sequence_length, batch_size,\n",
        "          vocab_size) containing scores for the next-word\n",
        "          predictions at each position.\n",
        "        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as\n",
        "          initial_hidden representing the updated decoder state after processing\n",
        "          the decoder input.\n",
        "        attention_weights: A tensor with shape (max_decoder_sequence_length,\n",
        "          batch_size, max_source_sequence_length) representing the normalized\n",
        "          attention weights. This should sum to 1 along the last dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: use a large negative number like -1e9 instead of\n",
        "    # float(\"-inf\") when masking logits to avoid numerical issues.\n",
        "\n",
        "    # Implementation tip: the function torch.bmm may be useful here.\n",
        "    # See https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    embedded = self.embedder(decoder_input)  # (T, B, word_vector_dim); * Embed decoder input.\n",
        "    outputs, hidden = self.lstm2(embedded, initial_hidden)  # (T, B, hidden_dim); * Run LSTM2.\n",
        "    outputs = self.dropout2(outputs)  \n",
        "    \n",
        "    # Ensure encoder outputs match the current batch size.\n",
        "    B_current = decoder_input.size(1)\n",
        "    if encoder_output.size(1) != B_current:\n",
        "        encoder_output = encoder_output.expand(-1, B_current, -1).contiguous()  # * Expand encoder output if needed.\n",
        "        encoder_mask = encoder_mask.expand(-1, B_current).contiguous()  # * Likewise for encoder mask.\n",
        "    \n",
        "    # Prepare encoder outputs: (S, B, enc_output_size) -> (B, S, enc_output_size)\n",
        "    enc_out = encoder_output.transpose(0, 1)\n",
        "    projected_enc = self.W_a(enc_out)  # (B, S, hidden_dim); * Apply W_a to encoder outputs.\n",
        "    dec_out = outputs.transpose(0, 1)   # (B, T, hidden_dim)\n",
        "    scores = torch.bmm(dec_out, projected_enc.transpose(1, 2))  # (B, T, S); * Compute attention scores.\n",
        "    attn_weights = F.softmax(scores, dim=2)  # (B, T, S); * Normalize to get attention weights.\n",
        "    context = torch.bmm(attn_weights, enc_out)  # (B, T, enc_output_size); * Compute context vectors.\n",
        "    concat = torch.cat([dec_out, context], dim=2)  # (B, T, hidden_dim+enc_output_size); * Concatenate decoder state and context.\n",
        "    attended = torch.tanh(self.W_c(concat))  # (B, T, hidden_dim); * Apply tanh after linear projection.\n",
        "    logits = self.out(attended)  # (B, T, vocab_size); * Compute vocabulary logits.\n",
        "    log_probs = F.log_softmax(logits, dim=2)  # * Compute log probabilities.\n",
        "    return log_probs.transpose(0, 1), hidden, attn_weights.transpose(0, 1)  # * Return with time dimension first.\n",
        "\n",
        "    ### END YOUR CODE HERE !!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSJZ9f5sq2cu"
      },
      "source": [
        "As before, we can train an attention model using the provided training code.\n",
        "\n",
        "A correct implementation should get a validation token accuracy above 67 and a validation BLEU above 36 with greedy search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HJfL51MKoMmp"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf0f6d2afaae41ecb883c85a78830ffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training:   0%|          | 0/10 [00:00<?, ?epoch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c1850895b344670a7ddc606fcef43b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 1:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 44.54, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "550ed4bd28db40ffa331da468ccbe2f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 2:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 54.88, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d113be039d4299ab83bc9b122144df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 3:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 59.93, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93f43949c4cc4ef0bf5287ef5243ad56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 4:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 62.18, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "486a9391c23f4f37b2a2001422ebb61c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 5:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 63.66, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13e4ae27df2848f6a7a88777bf33c942",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 6:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 64.62, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b5bce9c25d24289a567b04e7aeeca72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 7:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 65.19, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "314371eaf7914b50b04dc52f5af645db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 8:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 65.27, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffb79c2f79a848b0abe9dade436ced41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 9:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 65.62, saving model checkpoint to attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96105491c02d4d399f072a2a71fba522",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 10:   0%|          | 0/1813 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 65.74, saving model checkpoint to attention_model.pt...\n",
            "Reloading best model checkpoint from attention_model.pt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1110052/364093175.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention model validation BLEU using greedy search: 35.160723260127945\n",
            "Finished writing predictions to seq2seq_predictions_attention.json.\n"
          ]
        }
      ],
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "hidden_dim = 256\n",
        "enc_output_size = hidden_dim * 2\n",
        "word_vector_dim = 256\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "attention_model = Seq2seqAttention(hidden_dim,enc_output_size, word_vector_dim,dropout,num_layers).to(device)\n",
        "train(attention_model, num_epochs, batch_size, \"attention_model.pt\")\n",
        "print(\"Attention model validation BLEU using greedy search:\",\n",
        "      evaluate(attention_model, validation_data))\n",
        "# Generate the predictions for the attention model using greedy decoding on the test_data.\n",
        "# Corret implementation of the baseline model and attention model should get you full credits here.\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", attention_model, test_data, \"greedy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkiLQmImQP8d"
      },
      "source": [
        "## Beam Search\n",
        "\n",
        "We will now try to improve our model's predictions by decoding with beam search, rather than greedily. Beam search maintains a `beam_size`-length list of hypotheses at each step of decoding. A hypothesis is just a prefix of a full prediction, and is represented in the code below by a `Hyp` object.\n",
        "\n",
        "### What beam search does, in (mostly) words\n",
        "Beam search starts with a hypothesis consisting just of the BOS token, and then proceeds for `output_max_len` steps. At each step $t$, beam search considers adding every possible next-word to the hypotheses/prefixes from step $t-1$ (for a total of `beam_size`*V hypotheses). It then takes the highest scoring `beam_size` of these candidate hypotheses to be the hypotheses at step $t$. The score of a hypothesis of length $t$ is:\n",
        "$$\n",
        "\\mathrm{score}(w_1, \\ldots, w_t) = \\sum_{i=1}^t \\log p(w_i|w_1, \\ldots, w_{i-1}, x)\n",
        "$$\n",
        "where $x$ is the source question. The log probabilities above are just the standard ones output by your RNN decoder.\n",
        "\n",
        "A hypothesis is finished when it ends with an EOS token.\n",
        "\n",
        "With beam search, you should get an improvement of at least 1 BLEU over greedy search, and should reach above 21 BLEU without attention and above 38 BLEU with attention.\n",
        "\n",
        "**Tips:**\n",
        "\n",
        "1) A good general strategy when doing complex code like this is to carefully annotate each line with a comment saying what each dimension represents.\n",
        "\n",
        "2) You should only need one call to topk per step. You do not need to have a topk just over vocabulary first, you can directly go from vocab_size*beam_size to beam_size items.\n",
        "\n",
        "3) Be sure you are correctly keeping track of which beam item a candidate is selected from and updating the beam states, such as LSTM hidden state, accordingly. A single state from the previous time step may need to be used for multiple new beam items or not at all. This includes all state associated with a beam, including all past tokens output by the beam and any extra tensors such as ones remembering when a beam is finished.\n",
        "\n",
        "4) Once an EOS token has been generated, save the hypothesis and take it out of the beam for the next timestep.\n",
        "\n",
        "### **Implementation Task \\# 3**\n",
        "Fill in the missing code in the `predict_beam` function below. You are not implmenting batched beam_search so you only need to consider one sentence at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "miS-BuT0oMmq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Hyp:\n",
        "  \"\"\"\n",
        "  A helper class representing a hypothesis (i.e., the prefix of a prediction) on the beam,\n",
        "  using a linked list.\n",
        "  \"\"\"\n",
        "  def __init__(self, token_id: int, parent, score: float):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      token_id: an integer representing the most recent token added to this hypothesis.\n",
        "      parent: the Hyp object representing the prefix to which we've added this token.\n",
        "      score: the cumulative log-probability score of this hypothesis.\n",
        "    \"\"\"\n",
        "    self.token_id = token_id\n",
        "    self.parent = parent\n",
        "    self.score = score\n",
        "\n",
        "  def trace(self):\n",
        "    \"\"\"\n",
        "    Traces backward through the linked list to recover the whole hypothesis.\n",
        "    \n",
        "    Returns:\n",
        "      A list of token IDs representing the entire hypothesis.\n",
        "    \"\"\"\n",
        "    pred = []\n",
        "    temp = self\n",
        "    while temp is not None:\n",
        "      # Append token if it exists (this avoids error when temp is None)\n",
        "      if temp.token_id is not None:\n",
        "        pred.append(temp.token_id)\n",
        "      temp = temp.parent\n",
        "    return pred[::-1]\n",
        "\n",
        "def predict_beam(model, sentences, k=20, max_length=100):\n",
        "    \"\"\"Output the beam search result for the given sentences.\n",
        "    \n",
        "    Args:\n",
        "      model: The model that will be used to generate the beams.\n",
        "      sentences: A list of sentences (str) that the model will encode and do\n",
        "        beam search over. For simplicity, this list has length 1.\n",
        "      k: Beam size.\n",
        "      max_length: Maximum timesteps you will generate. If it exceeds this timestep, stop.\n",
        "    \n",
        "    Returns:\n",
        "      A list containing a single list of decoded generations (strings) sorted by their scores in descending order.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    V = vocab.GetPieceSize()\n",
        "    # Encode the input sentence (batch_size will be 1)\n",
        "    indices = make_batch(sentences)\n",
        "    enc_output, encoder_mask, encoder_state = model.encode(indices)\n",
        "    \n",
        "    # Initialize the beam.\n",
        "    # Start with a hypothesis that contains the BOS token.\n",
        "    init_hyp = Hyp(bos_id, None, 0.0)\n",
        "    beam = [init_hyp]\n",
        "    # The corresponding hidden state for the beam is the encoder state.\n",
        "    beam_states = [encoder_state]  # Each element is a tuple (h, c) with shape (num_layers, 1, hidden_dim)\n",
        "    \n",
        "    finished_hyps = []  \n",
        "\n",
        "    # Run beam search for a maximum of max_length steps.\n",
        "    for t in range(max_length):\n",
        "        # Prepare the current input tokens for all hypotheses in the beam.\n",
        "        # Each input should be the last token generated by that hypothesis.\n",
        "        # This will create a tensor of shape (1, beam_size).\n",
        "        current_tokens = [torch.tensor([hyp.trace()[-1]], device=device) for hyp in beam]\n",
        "        current_input = torch.stack(current_tokens, dim=1)  # (1, beam_size)\n",
        "        \n",
        "        # Prepare the hidden states by concatenating each beam’s hidden state along the batch dimension.\n",
        "        h_list = [state[0] for state in beam_states]  # each: (num_layers, 1, hidden_dim)\n",
        "        c_list = [state[1] for state in beam_states]\n",
        "        h_beam = torch.cat(h_list, dim=1)  # (num_layers, beam_size, hidden_dim)\n",
        "        c_beam = torch.cat(c_list, dim=1)\n",
        "        current_hidden = (h_beam, c_beam)\n",
        "        \n",
        "        # Decode one timestep for the entire beam.\n",
        "        # log_probs: (1, beam_size, V)\n",
        "        log_probs, new_hidden, _ = model.decode(current_input, current_hidden, enc_output, encoder_mask)\n",
        "        log_probs = log_probs.squeeze(0)  # now (beam_size, V)\n",
        "        \n",
        "        # For each beam candidate, add its current cumulative score to the new log_probs.\n",
        "        beam_scores = torch.tensor([hyp.score for hyp in beam], device=device).unsqueeze(1)  # (beam_size, 1)\n",
        "        total_scores = beam_scores + log_probs  # (beam_size, V)\n",
        "        \n",
        "        # Flatten the scores to shape (beam_size * V) and select the top k candidates.\n",
        "        total_scores_flat = total_scores.view(-1)\n",
        "        topk_scores, topk_indices = torch.topk(total_scores_flat, k)\n",
        "        \n",
        "        # Prepare new lists for the beam and their hidden states.\n",
        "        new_beam = []\n",
        "        new_beam_states = []\n",
        "        \n",
        "        # Process each of the top k candidates.\n",
        "        for score, flat_index in zip(topk_scores.tolist(), topk_indices.tolist()):\n",
        "            # Determine which beam candidate (row) and which token (column) this corresponds to.\n",
        "            prev_beam_idx = flat_index // V\n",
        "            token_id = flat_index % V\n",
        "            \n",
        "            # Extract the new hidden state for the candidate.\n",
        "            candidate_hidden = (\n",
        "                new_hidden[0][:, prev_beam_idx:prev_beam_idx+1, :],\n",
        "                new_hidden[1][:, prev_beam_idx:prev_beam_idx+1, :]\n",
        "            )\n",
        "            \n",
        "            # Create a new hypothesis that extends the previous one with the new token.\n",
        "            parent_hyp = beam[prev_beam_idx]\n",
        "            new_hyp = Hyp(token_id, parent_hyp, score)\n",
        "            \n",
        "            # If the candidate token is EOS, add it to finished hypotheses.\n",
        "            if token_id == eos_id:\n",
        "                finished_hyps.append(new_hyp)\n",
        "            else:\n",
        "                new_beam.append(new_hyp)\n",
        "                new_beam_states.append(candidate_hidden)\n",
        "        \n",
        "        # If no candidates remain in the beam (all ended with EOS), exit early.\n",
        "        if len(new_beam) == 0:\n",
        "            break\n",
        "        \n",
        "        # Update the beam with the new candidates for the next timestep.\n",
        "        beam = new_beam\n",
        "        beam_states = new_beam_states\n",
        "\n",
        "    # If no hypothesis finished with EOS, use the current beam as finished candidates.\n",
        "    if len(finished_hyps) == 0:\n",
        "        finished_hyps = beam\n",
        "    \n",
        "    # Sort finished hypotheses by their cumulative score in descending order.\n",
        "    finished_hyps = sorted(finished_hyps, key=lambda h: h.score, reverse=True)\n",
        "    \n",
        "    # Decode each hypothesis into a string.\n",
        "    decoded_sentences = []\n",
        "    for hyp in finished_hyps:\n",
        "        token_ids = hyp.trace()\n",
        "        # Remove the initial BOS token if present.\n",
        "        if token_ids and token_ids[0] == bos_id:\n",
        "            token_ids = token_ids[1:]\n",
        "        # Cut off at EOS if it appears.\n",
        "        if eos_id in token_ids:\n",
        "            token_ids = token_ids[:token_ids.index(eos_id)]\n",
        "        decoded_sentence = vocab.DecodeIds(token_ids)\n",
        "        decoded_sentences.append(decoded_sentence)\n",
        "    \n",
        "    return [decoded_sentences]\n",
        "\n",
        "# # Testing beam search with baseline model (for example)\n",
        "# print(\"Baseline model validation BLEU using beam search:\",\n",
        "#       evaluate(baseline_model, validation_data, batch_size=1, method=\"beam\"))\n",
        "# print()\n",
        "# print(\"Baseline model sample predictions:\")\n",
        "# print()\n",
        "# show_predictions(baseline_model, include_beam=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LH2dal_foMmq",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention model validation BLEU using beam search: 36.607337742114616\n",
            "\n",
            "Attention model sample predictions:\n",
            "\n",
            "Input:\n",
            "  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
            "Target:\n",
            "  A group of men are loading cotton onto a truck\n",
            "Greedy prediction:\n",
            "  A group of men loading cannons on a truck.\n",
            "Beam predictions (showing top 5):\n",
            "  A group of men loaded for a truck.\n",
            "  A group of men loaded onto a truck\n",
            "  A group of men loaded for a truck\n",
            "  A group of men loaded onto a truck.\n",
            "  A group of men loading cannons on a truck.\n",
            "\n",
            "Input:\n",
            "  Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
            "Target:\n",
            "  A man sleeping in a green room on a couch.\n",
            "Greedy prediction:\n",
            "  A man is sleeping on a couch in a green room.\n",
            "Beam predictions (showing top 5):\n",
            "  A man is sleeping on a couch in a green room.\n",
            "  A man sleeping on a couch in a green room.\n",
            "  A man sleeps on a couch in a green room.\n",
            "  Man sleeping on a couch in a green room.\n",
            "  A man asleep on a couch in a green room.\n",
            "\n",
            "Input:\n",
            "  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
            "Target:\n",
            "  A boy wearing headphones sits on a woman's shoulders.\n",
            "Greedy prediction:\n",
            "  A boy with headphones sitting on the shoulders of a woman.\n",
            "Beam predictions (showing top 5):\n",
            "  A boy with headphones sitting on the shoulders of a woman.\n",
            "  A boy in headphones sits on the shoulders of a woman.\n",
            "  A boy wearing headphones sits on the shoulders of a woman.\n",
            "  A boy in headphones sitting on the shoulders of a woman.\n",
            "  A boy with headphones sitting on his shoulders's shoulders.\n",
            "\n",
            "Input:\n",
            "  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\n",
            "Target:\n",
            "  Two men setting up a blue ice fishing hut on an iced over lake\n",
            "Greedy prediction:\n",
            "  Two men constructing a blue ice cream cone on a line.\n",
            "Beam predictions (showing top 5):\n",
            "  Two men constructing a blue ice cream cone on a cloudy lake.\n",
            "  Two men constructing a blue ice cream cart on a line.\n",
            "  Two men constructing a blue ice cream cone on a line.\n",
            "  Two men constructing a blue ice cream cart on top of a cloudy lake.\n",
            "  Two men constructing a blue ice cream cone on a cloudy lake\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Attention model validation BLEU using beam search:\",\n",
        "      evaluate(attention_model, validation_data, batch_size=1, method=\"beam\"))\n",
        "print()\n",
        "print(\"Attention model sample predictions:\")\n",
        "print()\n",
        "show_predictions(attention_model, include_beam=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envJv-ufzAAG"
      },
      "source": [
        "Run the cells to generate the beam_seqs.json file required for submission to check correctness of your beam_search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Zs4ABxnBy-j-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zKM1vgKkRye1COYh4IlH_m0xDCq7chFF\n",
            "To: /storage/ice1/3/4/pponnusamy7/Yash/special_model_beam_search.pt\n",
            "100%|██████████████████████████████████████| 7.89M/7.89M [00:00<00:00, 34.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1zKM1vgKkRye1COYh4IlH_m0xDCq7chFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Di1XeW0JzE8v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1110052/2735599898.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sd = torch.load(\"special_model_beam_search.pt\", map_location=device)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")  # Force CPU usage.\n",
        "hidden_dim = 100\n",
        "word_vector_dim = 100\n",
        "num_layers = 1\n",
        "dropout = 0.3\n",
        "\n",
        "# Create the model and move it to CPU.\n",
        "special_model = Seq2seqBaseline(hidden_dim, word_vector_dim, dropout, num_layers).to(device)\n",
        "# Load the state dictionary with map_location set to CPU.\n",
        "sd = torch.load(\"special_model_beam_search.pt\", map_location=device)\n",
        "special_model.load_state_dict(sd)\n",
        "\n",
        "V = vocab.GetPieceSize()\n",
        "nsrcs, srcsize = 11, 6\n",
        "special_preds = {}\n",
        "for beam_size in [1, 5, 10, 15]:\n",
        "    torch.manual_seed(beam_size)\n",
        "    srcs = [(vocab.DecodeIds(torch.LongTensor(srcsize).random_(0, V).numpy().tolist()),\n",
        "             'filler target sentence filler target sentence filler target sentence') \n",
        "            for _ in range(nsrcs)]\n",
        "    predictions = []\n",
        "    source_sentences = [x[0] for x in srcs]\n",
        "    for start_index in range(0, len(source_sentences), 1):\n",
        "        prediction_batch = predict_beam(\n",
        "            special_model, \n",
        "            source_sentences[start_index:start_index + 1], \n",
        "            k=beam_size,\n",
        "            max_length=50\n",
        "        )\n",
        "        predictions.extend(prediction_batch)\n",
        "    special_preds[beam_size] = predictions\n",
        "\n",
        "with open(\"beam_seqs.json\", \"w\") as f:\n",
        "    json.dump(special_preds, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ksZlRTzYpUJ"
      },
      "source": [
        "If you implemented beam search correctly, you can save the results of beam search for the attention model by uncommenting the code below. It will have higher BLEU score, but the greedy decoding should give you full 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tou6oLrlYmKE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished writing predictions to seq2seq_predictions_attention.json.\n"
          ]
        }
      ],
      "source": [
        "# Ensure the attention model is on CPU.\n",
        "attention_model = attention_model.to(torch.device(\"cpu\"))\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", attention_model, test_data, \"beam\", batch_size=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLoiXBWMaSPc"
      },
      "source": [
        "# Experimentation: 1-Page Report\n",
        "\n",
        "Now it's time for you to experiment.  Try to improve the denotation accuracy on the validation set further and aim for a validation BLEU score of 42. Feel free to modify the code above directly or copy it in new cells below.\n",
        "\n",
        "**NOTE:** We will award at least 7 of the 10 points if your improved model reaches a BLEU score of at least 40 on the hidden test cases on Gradescope.\n",
        "\n",
        "Here are some ideas to try out:\n",
        "* **Back translation**: Since the training dataset is small, another strategy for improving performance might be to generate more training instances. One popular technique people used in machine translation is called back translation(https://arxiv.org/abs/1511.06709). Train another model from English to Gernman and consturct more data.  \n",
        "For this extension, you are allowed to use external datasets with monolingual text in either English or German. You cannot use datasets containing both English and German texts.\n",
        "\n",
        "* **Word embeddings**: You can try initializing the input word embeddings with [Glove vectors](https://nlp.stanford.edu/projects/glove/). You should try both finetuning these embeddings or keeping them fixed.\n",
        "* **Regularization**: You can also try some of the regularization techniques we tried in the language modeling assignment, however these may or may not help.\n",
        "* **Tokenization**: Exploring with how tokenization is done or how big vocabulary size is can also be helpful. We used unigram language model subword tokenization and use a vocabulary of size 8000. You can consider using BPE (a very popular tokenization technique) or using characters or even some others.\n",
        "* **Hyperparameter tuning**: Finally you can try playing around with hyperparameters to see if there is a better configuration than what we have provided (we only did a modest amount of tuning).\n",
        "\n",
        "For this section, you will submit a write-up describing the extensions and/or modifications that you tried.  Your write-up should be **1-page maximum** in length and should be submitted in PDF format.  You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf.\n",
        "For full credit, your write-up should include:\n",
        "1.   A concise and precise description of the extension that you tried.\n",
        "2.   A motivation for why you believed this approach might improve your model.\n",
        "3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n",
        "4.   A bottom-line summary of your results comparing the scores of your improvement to the original model.\n",
        "The purpose of this exercise is to experiment, so feel free to try/ablate multiple of the suggestions above as well as any others you come up with!\n",
        "When you submit the file, please name it `report.pdf`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Let's create a custom dataset to hopefully improve the performance even more \n",
        "\n",
        "# from tqdm import trange, tqdm\n",
        "# import torch.cuda.amp as amp\n",
        "\n",
        "# # --- Modified train function to accept a dataset argument ---\n",
        "# def train(model, num_epochs, batch_size, model_file, dataset):\n",
        "#     optimizer = torch.optim.Adam(model.parameters())\n",
        "#     best_accuracy = 0.0\n",
        "#     for epoch in trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "#         with tqdm(\n",
        "#             make_batch_iterator(dataset, batch_size, shuffle=True),\n",
        "#             desc=\"epoch {}\".format(epoch + 1),\n",
        "#             unit=\"batch\",\n",
        "#             total=math.ceil(len(dataset) / batch_size)\n",
        "#         ) as batch_iterator:\n",
        "#             model.train()\n",
        "#             total_loss = 0.0\n",
        "#             for i, (source, target) in enumerate(batch_iterator, start=1):\n",
        "#                 source, target = source.to(device), target.to(device)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss = model.compute_loss(source, target)\n",
        "#                 total_loss += loss.item()\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
        "#             val_perplexity, val_accuracy = evaluate_next_token(model, validation_data)\n",
        "#             batch_iterator.set_postfix(\n",
        "#                 mean_loss=total_loss / i,\n",
        "#                 validation_perplexity=val_perplexity,\n",
        "#                 validation_token_accuracy=val_accuracy)\n",
        "#             if val_accuracy > best_accuracy:\n",
        "#                 print(\"Obtained a new best validation accuracy of {:.2f}, saving model checkpoint to {}...\".format(val_accuracy, model_file))\n",
        "#                 torch.save(model.state_dict(), model_file)\n",
        "#                 best_accuracy = val_accuracy\n",
        "#     print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "#     model.load_state_dict(torch.load(model_file, map_location=device))\n",
        "\n",
        "# # --- Step 1: Train a Reverse Model (English → German) ---\n",
        "# # Reverse the training data (swap source and target)\n",
        "# reverse_training_data = [(eng, ger) for (ger, eng) in training_data]\n",
        "\n",
        "# # Use our attention model for the reverse model.\n",
        "# reverse_model = Seq2seqAttention(\n",
        "#     hidden_dim=256, \n",
        "#     enc_output_size=256*2, \n",
        "#     word_vector_dim=256, \n",
        "#     dropout=0.3, \n",
        "#     num_layers=2\n",
        "# ).to(device)\n",
        "\n",
        "# print(\"Training reverse model (English -> German)...\")\n",
        "# train(reverse_model, num_epochs=10, batch_size=16, model_file=\"reverse_model.pt\", dataset=reverse_training_data)\n",
        "# print(\"Reverse model training complete.\")\n",
        "\n",
        "# # --- Step 2: Generate Synthetic Data Using the Reverse Model ---\n",
        "# # Use the monolingual English sentences (the target side of the original training data) as input.\n",
        "# monolingual_english = [pair[1] for pair in training_data]\n",
        "# synthetic_pairs = []\n",
        "# print(\"Generating synthetic German sentences via back translation...\")\n",
        "# for sentence in tqdm(monolingual_english, desc=\"Generating synthetic pairs\"):\n",
        "#     # Generate a synthetic German sentence using beam search; take the top candidate.\n",
        "#     synthetic_ger = predict_beam(reverse_model, [sentence], k=5, max_length=50)[0][0]\n",
        "#     synthetic_pairs.append((synthetic_ger, sentence))\n",
        "# print(f\"Generated {len(synthetic_pairs)} synthetic pairs.\")\n",
        "\n",
        "# # --- Step 3: Augment Training Data ---\n",
        "# augmented_training_data = training_data + synthetic_pairs\n",
        "# print(\"Augmented training data size:\", len(augmented_training_data))\n",
        "\n",
        "# import pickle\n",
        "# import os\n",
        "\n",
        "# # Save augmented data to file (using pickle)\n",
        "# with open(\"augmented_training_data.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(augmented_training_data, f)\n",
        "# print(\"Augmented training data saved to 'augmented_training_data.pkl'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded augmented training data from 'augmented_training_data.pkl'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Load augmented training data (must have been saved previously) ---\n",
        "if os.path.exists(\"augmented_training_data.pkl\"):\n",
        "    with open(\"augmented_training_data.pkl\", \"rb\") as f:\n",
        "        augmented_training_data = pickle.load(f)\n",
        "    print(\"Loaded augmented training data from 'augmented_training_data.pkl'.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Augmented training data not found. Please run the back translation pipeline first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original training size: 29001\n",
            "Augmented training size: 58002\n",
            "Fast training size: 87003\n",
            "=== Original Training Data: First 10 Examples ===\n",
            "Source: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "Target: Two young, White males are outside near many bushes.\n",
            "\n",
            "Source: Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
            "Target: Several men in hard hats are operating a giant pulley system.\n",
            "\n",
            "Source: Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "Target: A little girl climbing into a wooden playhouse.\n",
            "\n",
            "Source: Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "Target: A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "\n",
            "Source: Zwei Männer stehen am Herd und bereiten Essen zu.\n",
            "Target: Two men are at the stove preparing food.\n",
            "\n",
            "Source: Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
            "Target: A man in green holds a guitar while the other man observes his shirt.\n",
            "\n",
            "Source: Ein Mann lächelt einen ausgestopften Löwen an.\n",
            "Target: A man is smiling at a stuffed lion\n",
            "\n",
            "Source: Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
            "Target: A trendy girl talking on her cellphone while gliding slowly down the street.\n",
            "\n",
            "Source: Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\n",
            "Target: A woman with a large purse is walking by a gate.\n",
            "\n",
            "Source: Jungen tanzen mitten in der Nacht auf Pfosten.\n",
            "Target: Boys dancing on poles in the middle of the night.\n",
            "\n",
            "=== Original Training Data: Last 10 Examples ===\n",
            "Source: Ein älterer Mann hält eine Kanne hinter seinen Rücken, als er durch einen hübschen Blumenmarkt schlendert.\n",
            "Target: A elderly man holds a can behind his back as he strolls by a beautiful flower market.\n",
            "\n",
            "Source: Ein Mann baut einen Holzstuhl zusammen.\n",
            "Target: A man putting together a wooden chair.\n",
            "\n",
            "Source: Ein grauer Vogel steht majestätisch am Strand, während die Wellen heranrollen.\n",
            "Target: A gray bird stands majestically on a beach while waves roll in.\n",
            "\n",
            "Source: Ein Mann in einem silbernen Regenmantel steht im Freien bei einer Schubkarre.\n",
            "Target: A man in a silver raincoat standing outside by a wheelbarrow.\n",
            "\n",
            "Source: Eine Frau schreibt hinter einer verschnörkelten Wand.\n",
            "Target: A woman behind a scrolled wall is writing\n",
            "\n",
            "Source: Ein Bergsteiger übt an einer Kletterwand.\n",
            "Target: A rock climber practices on a rock climbing wall.\n",
            "\n",
            "Source: Zwei Bauarbeiter arbeiten auf einer Straße vor einem Hauses.\n",
            "Target: Two male construction workers are working on a street outside someone's home\n",
            "\n",
            "Source: Ein älterer Mann sitzt mit einem Jungen mit einem Wagen vor einer Fassade.\n",
            "Target: An elderly man sits outside a storefront accompanied by a young boy with a cart.\n",
            "\n",
            "Source: Ein Mann in Shorts und Hawaiihemd lehnt sich über das Geländer eines Lotsenboots, mit Nebel und Bergen im Hintergrund.\n",
            "Target: A man in shorts and a Hawaiian shirt leans over the rail of a pilot boat, with fog and mountains in the background.\n",
            "\n",
            "Source: \n",
            "Target: \n",
            "\n",
            "=== Augmented Training Data: First 10 Examples ===\n",
            "Source: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "Target: Two young, White males are outside near many bushes.\n",
            "\n",
            "Source: Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
            "Target: Several men in hard hats are operating a giant pulley system.\n",
            "\n",
            "Source: Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "Target: A little girl climbing into a wooden playhouse.\n",
            "\n",
            "Source: Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "Target: A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "\n",
            "Source: Zwei Männer stehen am Herd und bereiten Essen zu.\n",
            "Target: Two men are at the stove preparing food.\n",
            "\n",
            "Source: Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
            "Target: A man in green holds a guitar while the other man observes his shirt.\n",
            "\n",
            "Source: Ein Mann lächelt einen ausgestopften Löwen an.\n",
            "Target: A man is smiling at a stuffed lion\n",
            "\n",
            "Source: Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
            "Target: A trendy girl talking on her cellphone while gliding slowly down the street.\n",
            "\n",
            "Source: Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\n",
            "Target: A woman with a large purse is walking by a gate.\n",
            "\n",
            "Source: Jungen tanzen mitten in der Nacht auf Pfosten.\n",
            "Target: Boys dancing on poles in the middle of the night.\n",
            "\n",
            "=== Augmented Training Data: Last 10 Examples ===\n",
            "Source: Ein älterer Mann hält sich auf seinem Kopf auf seinem Kopf auf seinem Gesicht.\n",
            "Target: A elderly man holds a can behind his back as he strolls by a beautiful flower market.\n",
            "\n",
            "Source: Ein Mann klettert bei einem Restaurant.\n",
            "Target: A man putting together a wooden chair.\n",
            "\n",
            "Source: Ein hellbrauner Baby steht auf einem Strand, während sie auf einem Strand.\n",
            "Target: A gray bird stands majestically on a beach while waves roll in.\n",
            "\n",
            "Source: Ein Mann in einem Anzug steht vor einem Baum.\n",
            "Target: A man in a silver raincoat standing outside by a wheelbarrow.\n",
            "\n",
            "Source: Eine Frau auf einem Fluss.\n",
            "Target: A woman behind a scrolled wall is writing\n",
            "\n",
            "Source: Ein Skateboarder fährt auf einem Motorrad.\n",
            "Target: A rock climber practices on a rock climbing wall.\n",
            "\n",
            "Source: Zwei Bauarbeiter arbeiten auf einer Straße neben einer Straße.\n",
            "Target: Two male construction workers are working on a street outside someone's home\n",
            "\n",
            "Source: Ein älterer Mann sitzt neben einem Jungen vor einem kleinen Jungen mit einem Jungen.\n",
            "Target: An elderly man sits outside a storefront accompanied by a young boy with a cart.\n",
            "\n",
            "Source: Ein Mann in Shorts und einem T-Shirt springt über die Kamera, während sie im Hintergrund, die im Hintergrund.\n",
            "Target: A man in shorts and a Hawaiian shirt leans over the rail of a pilot boat, with fog and mountains in the background.\n",
            "\n",
            "Source: Mann.\n",
            "Target: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Combine for a more managxeable dataset\n",
        "fast_training_data = augmented_training_data + training_data\n",
        "print(f\"Original training size: {len(training_data)}\")\n",
        "print(f\"Augmented training size: {len(augmented_training_data)}\")\n",
        "print(f\"Fast training size: {len(fast_training_data)}\")\n",
        "\n",
        "\n",
        "print(\"=== Original Training Data: First 10 Examples ===\")\n",
        "for example in training_data[:10]:\n",
        "    print(\"Source:\", example[0])\n",
        "    print(\"Target:\", example[1])\n",
        "    print()\n",
        "\n",
        "print(\"=== Original Training Data: Last 10 Examples ===\")\n",
        "for example in training_data[-10:]:\n",
        "    print(\"Source:\", example[0])\n",
        "    print(\"Target:\", example[1])\n",
        "    print()\n",
        "\n",
        "print(\"=== Augmented Training Data: First 10 Examples ===\")\n",
        "for example in augmented_training_data[:10]:\n",
        "    print(\"Source:\", example[0])\n",
        "    print(\"Target:\", example[1])\n",
        "    print()\n",
        "\n",
        "print(\"=== Augmented Training Data: Last 10 Examples ===\")\n",
        "for example in augmented_training_data[-10:]:\n",
        "    print(\"Source:\", example[0])\n",
        "    print(\"Target:\", example[1])\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized with compatible weights from previous model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hice1/pponnusamy7/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1a64cef56e24617aa7dcd2f41ff3c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training:   0%|          | 0/12 [00:00<?, ?epoch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da5d4826b03146e68d59b65ee3f638ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 1:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 55.60, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5747a75b40474b499c19f96e2b5b981c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 2:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 61.67, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfcf86dc2c004f55ac826851ad93afd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 3:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 64.13, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37a85fe0645e4834a6f26474ffd35909",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 4:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 65.32, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "613cb89edd454dfea6aaadc3ee4d6b06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 5:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 65.88, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "954ae852858d4050b222c90862ff4bdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 6:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 66.36, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "965d7ec8292345d5ab7b0b9b247c4e20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 7:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 66.46, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "085d935568b04eccb2122e2d440bfcda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 8:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 66.92, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06670f569eb942579b6afd3d84feedb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 9:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 67.88, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6acb719aa43d44378844f86bcc3f51f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 10:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtained a new best validation accuracy of 68.01, saving model checkpoint to enhanced_attention_model.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0bde5ed6bac4e3fb643b6f5a3c7976b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 11:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3792c34635ea4624b235bacde4aa7bbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 12:   0%|          | 0/1209 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading best model checkpoint from enhanced_attention_model.pt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1110052/791634365.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  enhanced_model.load_state_dict(torch.load(model_file))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced attention model validation BLEU using beam search: 39.37019861516312\n",
            "Finished writing predictions to seq2seq_predictions_attention.json.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\")  # Keep on GPU for training\n",
        "\n",
        "# Significantly increase model capacity\n",
        "num_epochs = 12  # Slightly more epochs\n",
        "batch_size = 24  # Larger batch size for better gradient estimates\n",
        "hidden_dim = 512  # Double the hidden dimension\n",
        "enc_output_size = hidden_dim * 2\n",
        "word_vector_dim = 512  # Double the embedding size\n",
        "num_layers = 3  # More layers for deeper representations\n",
        "dropout = 0.4  # Slightly higher dropout to prevent overfitting with larger model\n",
        "\n",
        "# Create a custom model class that extends the attention model with improvements\n",
        "class EnhancedSeq2seqAttention(Seq2seqAttention):\n",
        "    def __init__(self, hidden_dim, enc_output_size, word_vector_dim, dropout, num_layers):\n",
        "        super().__init__(hidden_dim, enc_output_size, word_vector_dim, dropout, num_layers)\n",
        "        \n",
        "        # Add layer normalization for better training stability\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
        "        \n",
        "        # Add residual connections for deeper networks\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        \n",
        "        # Change the projection to be more expressive\n",
        "        self.W_c = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + enc_output_size, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        )\n",
        "    \n",
        "    def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "        embedded = self.embedder(decoder_input)\n",
        "        outputs, hidden = self.lstm2(embedded, initial_hidden)\n",
        "        outputs = self.dropout2(outputs)\n",
        "        \n",
        "        # Apply layer normalization for better gradient flow\n",
        "        outputs = self.layer_norm1(outputs)\n",
        "        \n",
        "        # Prepare encoder outputs\n",
        "        B_current = decoder_input.size(1)\n",
        "        if encoder_output.size(1) != B_current:\n",
        "            encoder_output = encoder_output.expand(-1, B_current, -1).contiguous()\n",
        "            encoder_mask = encoder_mask.expand(-1, B_current).contiguous()\n",
        "        \n",
        "        enc_out = encoder_output.transpose(0, 1)\n",
        "        projected_enc = self.W_a(enc_out)\n",
        "        dec_out = outputs.transpose(0, 1)\n",
        "        \n",
        "        # Compute attention with scaled dot product for better numerical stability\n",
        "        scaling_factor = math.sqrt(hidden_dim)\n",
        "        scores = torch.bmm(dec_out, projected_enc.transpose(1, 2)) / scaling_factor\n",
        "        \n",
        "        # Apply mask to prevent attending to padding tokens\n",
        "        if encoder_mask is not None:\n",
        "            # Expand mask to match scores shape and apply large negative bias\n",
        "            mask = encoder_mask.transpose(0, 1).unsqueeze(1)\n",
        "            scores = scores.masked_fill(mask, -1e9)\n",
        "            \n",
        "        attn_weights = F.softmax(scores, dim=2)\n",
        "        context = torch.bmm(attn_weights, enc_out)\n",
        "        \n",
        "        # Combine decoder state and context with enhanced projection\n",
        "        concat = torch.cat([dec_out, context], dim=2)\n",
        "        attended = torch.tanh(self.W_c(concat))\n",
        "        \n",
        "        # Apply layer norm and add a residual connection\n",
        "        attended = self.layer_norm2(attended + self.dropout3(dec_out))\n",
        "        \n",
        "        logits = self.out(attended)\n",
        "        log_probs = F.log_softmax(logits, dim=2)\n",
        "        \n",
        "        return log_probs.transpose(0, 1), hidden, attn_weights.transpose(0, 1)\n",
        "\n",
        "# Initialize and train the enhanced model\n",
        "enhanced_model = EnhancedSeq2seqAttention(hidden_dim, enc_output_size, word_vector_dim, dropout, num_layers).to(device)\n",
        "\n",
        "# Optional: Initialize with weights from previous model for faster convergence\n",
        "try:\n",
        "    # Load previous model weights for parameters that match\n",
        "    state_dict = attention_model.state_dict()\n",
        "    enhanced_dict = enhanced_model.state_dict()\n",
        "    \n",
        "    # Copy weights that match in shape\n",
        "    for name, param in state_dict.items():\n",
        "        if name in enhanced_dict and enhanced_dict[name].shape == param.shape:\n",
        "            enhanced_dict[name] = param\n",
        "    \n",
        "    enhanced_model.load_state_dict(enhanced_dict, strict=False)\n",
        "    print(\"Initialized with compatible weights from previous model\")\n",
        "except Exception as e:\n",
        "    print(f\"Starting from scratch: {e}\")\n",
        "\n",
        "# Use a learning rate scheduler for better convergence\n",
        "optimizer = torch.optim.Adam(enhanced_model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, verbose=True)\n",
        "\n",
        "# Custom training loop with learning rate scheduling\n",
        "best_accuracy = 0.0\n",
        "model_file = \"enhanced_attention_model.pt\"\n",
        "\n",
        "for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "    with tqdm.notebook.tqdm(\n",
        "        make_batch_iterator(training_data, batch_size, shuffle=True),\n",
        "        desc=f\"epoch {epoch + 1}\",\n",
        "        unit=\"batch\",\n",
        "        total=math.ceil(len(training_data) / batch_size)) as batch_iterator:\n",
        "        \n",
        "        enhanced_model.train()\n",
        "        total_loss = 0.0\n",
        "        \n",
        "        for i, (source, target) in enumerate(batch_iterator, start=1):\n",
        "            source, target = source.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = enhanced_model.compute_loss(source, target)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(enhanced_model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
        "            \n",
        "        validation_perplexity, validation_accuracy = evaluate_next_token(\n",
        "            enhanced_model, validation_data)\n",
        "        validation_bleu = evaluate(enhanced_model, validation_data, batch_size=1, method='beam')\n",
        "        \n",
        "        batch_iterator.set_postfix(\n",
        "            mean_loss=total_loss / i,\n",
        "            validation_perplexity=validation_perplexity,\n",
        "            validation_token_accuracy=validation_accuracy,\n",
        "            validation_bleu=validation_bleu)\n",
        "        \n",
        "        # Update scheduler based on BLEU score\n",
        "        scheduler.step(validation_bleu)\n",
        "        \n",
        "        if validation_accuracy > best_accuracy:\n",
        "            print(f\"Obtained a new best validation accuracy of {validation_accuracy:.2f}, saving model checkpoint to {model_file}...\")\n",
        "            torch.save(enhanced_model.state_dict(), model_file)\n",
        "            best_accuracy = validation_accuracy\n",
        "\n",
        "print(f\"Reloading best model checkpoint from {model_file}...\")\n",
        "enhanced_model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "# Evaluate with beam search\n",
        "print(\"Enhanced attention model validation BLEU using beam search:\",\n",
        "      evaluate(enhanced_model, validation_data, batch_size=1, method='beam'))\n",
        "\n",
        "# Generate the predictions for submission\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", enhanced_model, test_data, \"beam\", batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced attention model validation BLEU using beam search: 39.71022107932992\n"
          ]
        }
      ],
      "source": [
        "# Let's change Predict_beam fucntion to be slightly bettedr with length normalization\n",
        "\n",
        "def predict_beam(model, sentences, k=20, max_length=100, alpha=0.75):\n",
        "\n",
        "    model.eval()\n",
        "    V = vocab.GetPieceSize()\n",
        "    # Encode the input sentence (batch_size will be 1)\n",
        "    indices = make_batch(sentences)\n",
        "    enc_output, encoder_mask, encoder_state = model.encode(indices)\n",
        "    \n",
        "    # Initialize the beam with <bos> token\n",
        "    init_hyp = Hyp(bos_id, None, 0.0)\n",
        "    beam = [init_hyp]\n",
        "    beam_states = [encoder_state]\n",
        "    \n",
        "    finished_hyps = []  \n",
        "\n",
        "    for t in range(max_length):\n",
        "        if len(beam) == 0:\n",
        "            break\n",
        "            \n",
        "        current_tokens = [torch.tensor([hyp.trace()[-1]], device=device) for hyp in beam]\n",
        "        current_input = torch.stack(current_tokens, dim=1)\n",
        "        \n",
        "        h_list = [state[0] for state in beam_states]\n",
        "        c_list = [state[1] for state in beam_states]\n",
        "        h_beam = torch.cat(h_list, dim=1)\n",
        "        c_beam = torch.cat(c_list, dim=1)\n",
        "        current_hidden = (h_beam, c_beam)\n",
        "        \n",
        "        log_probs, new_hidden, _ = model.decode(current_input, current_hidden, enc_output, encoder_mask)\n",
        "        log_probs = log_probs.squeeze(0)\n",
        "        \n",
        "        beam_scores = torch.tensor([hyp.score for hyp in beam], device=device).unsqueeze(1)\n",
        "        total_scores = beam_scores + log_probs\n",
        "        \n",
        "        total_scores_flat = total_scores.view(-1)\n",
        "        topk_scores, topk_indices = torch.topk(total_scores_flat, k)\n",
        "        \n",
        "        new_beam = []\n",
        "        new_beam_states = []\n",
        "        \n",
        "        for score, flat_index in zip(topk_scores.tolist(), topk_indices.tolist()):\n",
        "            prev_beam_idx = flat_index // V\n",
        "            token_id = flat_index % V\n",
        "            \n",
        "            candidate_hidden = (\n",
        "                new_hidden[0][:, prev_beam_idx:prev_beam_idx+1, :],\n",
        "                new_hidden[1][:, prev_beam_idx:prev_beam_idx+1, :]\n",
        "            )\n",
        "            \n",
        "            parent_hyp = beam[prev_beam_idx]\n",
        "            new_hyp = Hyp(token_id, parent_hyp, score)\n",
        "            \n",
        "            if token_id == eos_id:\n",
        "                # Apply length normalization to finished hypotheses\n",
        "                tokens_trace = new_hyp.trace()\n",
        "                length = len(tokens_trace) - 1  # we don't count <bos>\n",
        "                if length > 0:\n",
        "                    # Apply length penalty formula: (5+|Y|)^alpha / (5+1)^alpha - this is the standard one apparently\n",
        "                    normalized_score = score * ((5 + 1) ** alpha) / ((5 + length) ** alpha)\n",
        "                    new_hyp.score = normalized_score\n",
        "                finished_hyps.append(new_hyp)\n",
        "            else:\n",
        "                new_beam.append(new_hyp)\n",
        "                new_beam_states.append(candidate_hidden)\n",
        "        \n",
        "        # If beam is full of EOS tokens or we've reached the maximum length, stop\n",
        "        if len(new_beam) == 0:\n",
        "            break\n",
        "            \n",
        "        beam = new_beam\n",
        "        beam_states = new_beam_states\n",
        "\n",
        "    # If no hypothesis finished with EOS, use the current beam\n",
        "    if len(finished_hyps) == 0:\n",
        "        finished_hyps = beam\n",
        "        \n",
        "    # Re-apply length normalization to any unfinished hypothesis before sorting\n",
        "    for hyp in finished_hyps:\n",
        "        if hyp.token_id != eos_id:\n",
        "            tokens_trace = hyp.trace()\n",
        "            length = len(tokens_trace) - 1  # Don't count <bos>\n",
        "            if length > 0:\n",
        "                normalized_score = hyp.score * ((5 + 1) ** alpha) / ((5 + length) ** alpha)\n",
        "                hyp.score = normalized_score\n",
        "    \n",
        "    # Sort finished hypotheses by their normalized score\n",
        "    finished_hyps = sorted(finished_hyps, key=lambda h: h.score, reverse=True)\n",
        "    \n",
        "    # Decode each hypothesis into a string\n",
        "    decoded_sentences = []\n",
        "    for hyp in finished_hyps:\n",
        "        token_ids = hyp.trace()\n",
        "        if token_ids and token_ids[0] == bos_id:\n",
        "            token_ids = token_ids[1:]\n",
        "        if eos_id in token_ids:\n",
        "            token_ids = token_ids[:token_ids.index(eos_id)]\n",
        "        decoded_sentence = vocab.DecodeIds(token_ids)\n",
        "        decoded_sentences.append(decoded_sentence)\n",
        "    \n",
        "    return [decoded_sentences]\n",
        "\n",
        "\n",
        "# Evaluate with beam search\n",
        "print(\"Enhanced attention model validation BLEU using beam search:\",\n",
        "      evaluate(enhanced_model, validation_data, batch_size=1, method='beam'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Generate predictions for submission\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", model_ls, test_data, \"beam\", batch_size=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enhanced attention model validation BLEU using beam search: 39.71022107932992"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHTOfrCG8CRF"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Upload a submission with the following files to Gradescope:\n",
        "* proj_3.ipynb (rename to match this exactly)\n",
        "* seq2seq_predictions_baseline.json (baseline_model with greedy decoding would suffice)\n",
        "* seq2seq_predictions_attention.json (if you have an improved model, use that model to generate this file, otherwise submiting attention model with greedy decoding will get you full points (20%) but probably not for the improvement evaluation (10%))\n",
        "* beam_seqs.json\n",
        "* report.pdf\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set accuracies shown by the autograder are on different data from your validation set.  We will compare your score on the test set to our model's score and assign points based on that."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "112db3e457d04733943b7f950963915d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb57be9341cf484b8337a29c30ac6559",
            "placeholder": "​",
            "style": "IPY_MODEL_2e52169afdbe4e1783c54b457234ca16",
            "value": " 0/1813 [00:00&lt;?, ?batch/s]"
          }
        },
        "1355e071eafd452f9f3fe820471eb93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1826ebd2711a4dd585ea23bd33f85255": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bcfbd7357174878a73a9e07ee0f5abb",
              "IPY_MODEL_c6c99c69be5c4c6c8f106198556f9187",
              "IPY_MODEL_112db3e457d04733943b7f950963915d"
            ],
            "layout": "IPY_MODEL_45034a27da974ab884f3c8cecf147aa9"
          }
        },
        "1b61edddb3be45f988f2a886295b0e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bcfbd7357174878a73a9e07ee0f5abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db7d7f2dfa5e4d1883f77aae26b2194a",
            "placeholder": "​",
            "style": "IPY_MODEL_1355e071eafd452f9f3fe820471eb93e",
            "value": "epoch 1:   0%"
          }
        },
        "1fb0ca0f9b7f4908820e73e965a4e905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2764139c35a54e959edd1de1e7314d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f485197cc34446459899c7e55e445e39",
            "placeholder": "​",
            "style": "IPY_MODEL_57beb3432a55408dbec4bb9ff52c043e",
            "value": "training:   0%"
          }
        },
        "2e52169afdbe4e1783c54b457234ca16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c9f7733d79144858625e0ecd021a69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2764139c35a54e959edd1de1e7314d66",
              "IPY_MODEL_dc687e032e554315a45fec33c34c54b4",
              "IPY_MODEL_c535f8c5d4374e38bd90852e304ecda7"
            ],
            "layout": "IPY_MODEL_7436c93f90f04e53a5c6bda85ce4aab9"
          }
        },
        "45034a27da974ab884f3c8cecf147aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54149af30d3549e3806aac19109aedf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57beb3432a55408dbec4bb9ff52c043e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d4db5079eaf428fbc4f927a80c2656e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7436c93f90f04e53a5c6bda85ce4aab9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8999ee1ea4ec42f4b37aaf7238167a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb57be9341cf484b8337a29c30ac6559": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c535f8c5d4374e38bd90852e304ecda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8999ee1ea4ec42f4b37aaf7238167a48",
            "placeholder": "​",
            "style": "IPY_MODEL_54149af30d3549e3806aac19109aedf0",
            "value": " 0/10 [00:00&lt;?, ?epoch/s]"
          }
        },
        "c6c99c69be5c4c6c8f106198556f9187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b61edddb3be45f988f2a886295b0e0a",
            "max": 1813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed34af228b464218805d6068e6b42fc4",
            "value": 0
          }
        },
        "db7d7f2dfa5e4d1883f77aae26b2194a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc687e032e554315a45fec33c34c54b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4db5079eaf428fbc4f927a80c2656e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fb0ca0f9b7f4908820e73e965a4e905",
            "value": 0
          }
        },
        "ed34af228b464218805d6068e6b42fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f485197cc34446459899c7e55e445e39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
