{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvcnj-gNfWxy"
      },
      "source": [
        "# Assignment 3: Machine Translation\n",
        "\n",
        "TA contact for this assignment: Raymond Xiong (raymond.xiong@duke.edu), Xinchang Xiong (xinchang.xiong@duke.edu)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this assignment you will implement a LSTM based sequence-to-sequence model for machine translation.\n",
        "* We have provided an implementation of the encoder. You will need to implement an LSTM based decoder and then use it to train a basic sequence-to-sequence model.\n",
        "* Next, you will extend the decoder with attention and implement beam search decoding (we have provided a greedy decoder as reference).\n",
        "* Lastly you can try to improve the model using extensions such as a back translation or data augmentation.\n",
        "\n",
        "**Warning**: Attention and beam search can be tricky to implement. We expect this assignment to take longer than the CRF one. Please don't start the day before it is due!\n",
        "\n",
        "We will use the Multi30k for this assignment which consists of 30k German and English sentences.\n",
        "\n",
        "**Note**: When implementing beam search, to keep things simple we will not use batching (beam search on one sentence at a time). However, for implementing the decoders, please use batching.\n",
        "\n",
        "**Grading Rubric**\n",
        "- 70% results\n",
        "  - 20% seq2seq_predictions_baseline.json (meets target)\n",
        "  - 20% seq2seq_predictions_attention.json (meets target)\n",
        "  - 20% beam_seqs.json (correctness)\n",
        "  - 10% seq2seq_predictions_attention.json (improvement)\n",
        "  \n",
        "- 30% writeup\n",
        " - 12.5% clarity\n",
        " - 12.5% correctness\n",
        " - 5% interestingness of ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNTSYtkLhCw"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Feel free to add other libraries here (that don't already implement what you are supposed to!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pMgTktaSLgVy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade sacrebleu sentencepiece gdown\n",
        "# Standard library imports\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "\n",
        "# Third party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "import sentencepiece\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm.notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ETWUf7d978"
      },
      "source": [
        "Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU. Manage this by go to the Runtime tab in your colab.\n",
        "We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.\n",
        "\n",
        "Note that if you use \"CPU\" training time would be much slower depending on the CPU (likely 20 times slower). So use of GPU is recommended, be sure to manage your GPU so that it doesn't run out of quota."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FvPeUJmd2nM",
        "outputId": "2123d9ee-7d2c-4f08-ab26-aaea10ab3f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    assert torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\")\n",
        "except:\n",
        "    device = torch.device(\"mps\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxHeAMdOMZbq"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data for this assignment comes from the [Multi30K dataset](https://arxiv.org/abs/1605.00459), which contains English and German captions for images from Flickr. We can download it using `gdown`. We use the Multi30K dataset because it is simpler than standard translation benchmark datasets and allows for models to be trained and evaluated in a matter of minutes rather than days using a GPU.\n",
        "\n",
        "We will be translating from German to English in this assignment, but the same techniques apply equally well to any language pair.\n",
        "\n",
        "\n",
        "\n",
        "**You do not need to modify anything in this section.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd70s6-3n2vJ"
      },
      "source": [
        "First let's download the data and visualize some of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwiYczSz6B7n",
        "outputId": "b2f96e6a-70ed-4d90-90b5-464684300aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ll4fDiPLQ0u9osdtSlsUcehK_p_2dykV\n",
            "To: /storage/ice1/3/4/pponnusamy7/Yash/training_data.json\n",
            "100%|███████████████████████████████████████| 4.28M/4.28M [00:00<00:00, 185MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OEBVpX9F2FX0Mqj17jOWJKI2efUN_HBR\n",
            "To: /storage/ice1/3/4/pponnusamy7/Yash/validation_data.json\n",
            "100%|████████████████████████████████████████| 152k/152k [00:00<00:00, 24.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1ll4fDiPLQ0u9osdtSlsUcehK_p_2dykV\n",
        "!gdown 1OEBVpX9F2FX0Mqj17jOWJKI2efUN_HBR\n",
        "!gdown 1zZF8EXtzcd3oXSGEfyKywkSMosX_T6Jo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFWARAfahBaT",
        "outputId": "19fc1dc3-51b4-4105-e6ae-ebd0b9d45660"
      },
      "outputs": [],
      "source": [
        "with open(\"training_data.json\",\"r\") as f:\n",
        "    training_data = json.load(f)\n",
        "with open(\"validation_data.json\",\"r\") as f:\n",
        "    validation_data = json.load(f)\n",
        "with open(\"test_data.json\",\"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "print(\"Number of training examples:\", len(list(training_data)))\n",
        "print(\"Number of validation examples:\", len(list(validation_data)))\n",
        "print(\"Number of test examples:\", len(list(test_data)))\n",
        "print()\n",
        "\n",
        "for example in training_data[:10]:\n",
        "  print(example[0])\n",
        "  print(example[1])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZILrv5m9oyDQ"
      },
      "source": [
        "Vocabulary:\n",
        "Now We can use `sentencepiece` to create a joint German-English subword vocabulary from the training corpus. Subwords are words being divided into smaller pieces. They usually provide better performance since it takes advantages over common section among different words (see more at https://huggingface.co/docs/transformers/en/tokenizer_summary) and it handles Out of Vocabulary (OOV) words a lot better (https://blog.octanove.org/guide-to-subword-tokenization/). Because the number of training examples is small, we choose a smaller vocabulary size than would be used for large-scale NMT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS5TwOStbUlL"
      },
      "source": [
        "Let's download the English and German training corpous to construct our vocabulary. The two files downloaded here contains English and German sentences are from the training data we downloaded above but are decoupled to be used for the `sentencepiece` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF_S8ObkHHQA",
        "outputId": "42b9cdbe-d9f7-437a-f95c-889c0454eea5"
      },
      "outputs": [],
      "source": [
        "!gdown 1bO7SVCjvVzp__ibwED8wbMRSiJQNNP52\n",
        "!gdown 1A2w-F6kmUXNtuFtG2qdpfw0dx2Mk7phR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eseil4-uj4e7"
      },
      "source": [
        "We will use a unigram language model for subword segmentation (https://aclanthology.org/P18-1007.pdf). There are other techniques such as using BPE and or using characters, but we won't explore into them here (you can consider trying different subword strategies to improve your model later in the improvement section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAohTmoLMbMh"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"pad_id\": 0,\n",
        "    \"bos_id\": 1,\n",
        "    \"eos_id\": 2,\n",
        "    \"unk_id\": 3,\n",
        "    \"input\": \"train.de,train.en\",\n",
        "    \"vocab_size\": 8000,\n",
        "    \"model_prefix\": \"Multi30k\",\n",
        "    # \"model_type\": \"word\",\n",
        "}\n",
        "combined_args = \" \".join(\n",
        "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
        "sentencepiece.SentencePieceTrainer.Train(combined_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px9gmkJfvPkd"
      },
      "source": [
        "This creates two files: `Multi30k.model` and `Multi30k.vocab`. The first is a binary file containing the relevant data for the vocabulary. The second is a human-readable listing of each subword and its associated score. The score is the logged probability of the subword in the corpus. A higher score means that subword appears more frequently in the corpus.\n",
        "\n",
        "`sentencepiece` trainer basically finds a set of those subwords such that their joint probabily maximaizes over the corpus. How do you find the correct segmentation of words to subword such that you can maximize this joint probability becomes the question. This can be done by using a Viterbi algorithm (you implemented last assignment!). You don't need to know exactly how this is done but if you are intrested you can look into this [paper](https://aclanthology.org/P18-1007.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5ZC5rQpvV-7"
      },
      "source": [
        "You can preveiw some of the word scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z56YdM52vbQq",
        "outputId": "fcff34e0-023b-42fa-f029-faf117f5683a"
      },
      "outputs": [],
      "source": [
        "!head -n 30 Multi30k.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK9W1jt0vmBk"
      },
      "source": [
        "As we can see, the vocabulary consists of four special tokens (`<pad>` for padding, `<s>` for beginning of sentence (BOS), `</s>` for end of sentence (EOS), `<unk>` for unknown) and a mixture of German and English words and subwords. In order to ensure reversability, word boundaries are encoded with a special unicode character \"▁\" (U+2581)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suOYWS_AwBzz"
      },
      "source": [
        "To use the vocabulary, we first need to load it from the binary file produced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuqd9EhmwEDQ",
        "outputId": "ba40589d-96e2-45aa-d14a-276cbde854ef"
      },
      "outputs": [],
      "source": [
        "vocab = sentencepiece.SentencePieceProcessor()\n",
        "vocab.Load(\"Multi30k.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRZkyH-xwIx4"
      },
      "source": [
        "The vocabulary object includes a number of methods for working with full sequences or individual pieces. We explore the most relevant ones below. A complete interface can be found on [GitHub](https://github.com/google/sentencepiece/tree/master/python#usage) for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL507EdMtSdD",
        "outputId": "fe50a986-1dc8-406a-ec6c-1f67cfc31b18"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary size:\", vocab.GetPieceSize())\n",
        "print()\n",
        "\n",
        "for example in training_data[:3]:\n",
        "  sentence = example[1]\n",
        "  pieces = vocab.EncodeAsPieces(sentence)\n",
        "  indices = vocab.EncodeAsIds(sentence)\n",
        "  print(sentence)\n",
        "  print(pieces)\n",
        "  print(vocab.DecodePieces(pieces))\n",
        "  print(indices)\n",
        "  print(vocab.DecodeIds(indices))\n",
        "  print()\n",
        "\n",
        "piece = vocab.EncodeAsPieces(\"the\")[0]\n",
        "index = vocab.PieceToId(piece)\n",
        "print(piece)\n",
        "print(index)\n",
        "print(vocab.IdToPiece(index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRzFneNwNOi"
      },
      "source": [
        "We define some constants here for the first three special tokens that you may find useful in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzmImUrYwN0J",
        "outputId": "31fd911f-4f68-4476-b68c-46a76b25d611"
      },
      "outputs": [],
      "source": [
        "pad_id = vocab.PieceToId(\"<pad>\")\n",
        "bos_id = vocab.PieceToId(\"<s>\")\n",
        "eos_id = vocab.PieceToId(\"</s>\")\n",
        "print(f\"<pad>: {pad_id}, <s>: {bos_id}, </s>: {eos_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIRLxTA3wUjw"
      },
      "source": [
        "Note that these tokens will be stripped from the output when converting from word pieces to text. This may be helpful when implementing greedy search and beam search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DOK5hzbwRqC",
        "outputId": "766ca63c-cf90-4923-d483-86cbaff495a0"
      },
      "outputs": [],
      "source": [
        "sentence = training_data[0][1]\n",
        "indices = vocab.EncodeAsIds(sentence)\n",
        "indices_augmented = [bos_id] + indices + [eos_id, pad_id, pad_id, pad_id]\n",
        "print(vocab.DecodeIds(indices))\n",
        "print(vocab.DecodeIds(indices_augmented))\n",
        "print(vocab.DecodeIds(indices) == vocab.DecodeIds(indices_augmented))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zdx72ZEY6El"
      },
      "source": [
        "Code for saving your results for submission. You don't have to read this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0HQOLF3Y4-B"
      },
      "outputs": [],
      "source": [
        "# Please do not change the code below\n",
        "def generate_predictions_file_for_submission(filepath, model, dataset, method, batch_size=64):\n",
        "    assert method in {\"greedy\", \"beam\"}\n",
        "    source_sentences = [example[0] for example in dataset]\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "      for start_index in range(0, len(source_sentences), batch_size):\n",
        "        if method == \"greedy\":\n",
        "          prediction_batch = predict_greedy(\n",
        "              model, source_sentences[start_index:start_index + batch_size])\n",
        "          prediction_batch = [[x] for x in prediction_batch]\n",
        "        else:\n",
        "          prediction_batch = predict_beam(\n",
        "              model, source_sentences[start_index:start_index + batch_size])\n",
        "        predictions.extend(prediction_batch)\n",
        "    with open(filepath, \"w\") as outfile:\n",
        "        json.dump(predictions, outfile, indent=2)\n",
        "    print(\"Finished writing predictions to {}.\".format(filepath))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgmN23YvdX1M"
      },
      "source": [
        "## Seq2Seq Machine Translation Model\n",
        "\n",
        "Now let's implement a sequence-to-sequence machine translation model. We will first implement an `Encode` and an `Decode` method and then put these together in a `Seq2seqBaseline` class.\n",
        "\n",
        "We have implemented the `Encode` method below which encodes input sequences using a bi-directional LSTM. A bi-LSTM consists of a stack of two LSTM networks, one which processes the sequence in forward direction and another which processes the sequence in reverse direction. The output hidden states from both are concatenated to get the representations at each position. Further, we average the final states in either direction before returning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOL73L9wDpo6"
      },
      "source": [
        "### **Implementation Task \\# 1**\n",
        "\n",
        "Let's begin by defining a batch iterator for the training data. Given a dataset and a batch size, it will iterate over the dataset and yield pairs of tensors containing the subword indices for the source and target sentences in the batch, respectively.  We filled in `make_batch` below. It is advised to read the code below to get a sense of how sentences are tokenized and batched.\n",
        "\n",
        "**Note**: Maybe a little different from previous assignments, we are keeping batch_size to be the **2nd** dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfLjDrg6ynL8",
        "outputId": "06e2ce6b-08a1-421b-b321-4248479e36e4"
      },
      "outputs": [],
      "source": [
        "def make_batch(sentences):\n",
        "  \"\"\"Convert a list of sentences into a batch of subword indices.\n",
        "\n",
        "  Args:\n",
        "    sentences: A list of sentences, each of which is a string.\n",
        "\n",
        "  Returns:\n",
        "    A LongTensor of size (max_sequence_length, batch_size) containing the\n",
        "    subword indices for the sentences, where max_sequence_length is the length\n",
        "    of the longest sentence as encoded by the subword vocabulary and batch_size\n",
        "    is the number of sentences in the batch. A beginning-of-sentence token\n",
        "    should be included before each sequence, and an end-of-sentence token should\n",
        "    be included after each sequence. Empty slots at the end of shorter sequences\n",
        "    should be filled with padding tokens. The tensor should be located on the\n",
        "    device defined at the beginning of the notebook.\n",
        "  \"\"\"\n",
        "\n",
        "  batch_indices = []\n",
        "  for sentence in sentences:\n",
        "    indices = vocab.EncodeAsIds(sentence)\n",
        "    indices_augmented = [bos_id] + indices + [eos_id]\n",
        "    indices_augmented = torch.LongTensor(indices_augmented)\n",
        "    batch_indices.append(indices_augmented)\n",
        "\n",
        "  batched_seq = torch.nn.utils.rnn.pad_sequence(batch_indices, padding_value=pad_id).to(device)\n",
        "  return batched_seq\n",
        "\n",
        "def make_batch_iterator(dataset, batch_size, shuffle=False):\n",
        "  \"\"\"Make a batch iterator that yields source-target pairs.\n",
        "\n",
        "  Args:\n",
        "    dataset: A torchtext dataset object.\n",
        "    batch_size: An integer batch size.\n",
        "    shuffle: A boolean indicating whether to shuffle the examples.\n",
        "\n",
        "  Yields:\n",
        "    Pairs of tensors constructed by calling the make_batch function on the\n",
        "    source and target sentences in the current group of examples. The max\n",
        "    sequence length can differ between the source and target tensor, but the\n",
        "    batch size will be the same. The final batch may be smaller than the given\n",
        "    batch size.\n",
        "  \"\"\"\n",
        "\n",
        "  examples = list(dataset)\n",
        "  if shuffle:\n",
        "    random.shuffle(examples)\n",
        "\n",
        "  for start_index in range(0, len(examples), batch_size):\n",
        "    example_batch = examples[start_index:start_index + batch_size]\n",
        "    source_sentences = [example[0] for example in example_batch]\n",
        "    target_sentences = [example[1] for example in example_batch]\n",
        "    yield make_batch(source_sentences), make_batch(target_sentences)\n",
        "\n",
        "test_batch = make_batch([\"a test input\", \"a longer input than the first\"])\n",
        "print(\"Example batch tensor:\")\n",
        "print(test_batch)\n",
        "assert test_batch.shape[1] == 2\n",
        "assert test_batch[0, 0] == bos_id\n",
        "assert test_batch[0, 1] == bos_id\n",
        "assert test_batch[-1, 0] == pad_id\n",
        "assert test_batch[-1, 1] == eos_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6VhQOnXyprP"
      },
      "source": [
        "Implement an LSTM based decoder below. The decoder should be similar to the encoder which is already implemented, except it will accept a `state` tuple with the initial values of the `h_n` and `c_n` states (which will be the final states from the encoder above). Also the inputs to the decoder will be embed using the same embedder from the encoder. We will also return the final state from the decoder since we will need it for inference later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbe4yj_l4QQ0"
      },
      "outputs": [],
      "source": [
        "class Seq2seqBaseline(nn.Module):\n",
        "  def __init__(self, hidden_dim, word_vector_dim, dropout,num_layers):\n",
        "    super().__init__()\n",
        "    \"\"\"\n",
        "    args:\n",
        "      hidden_dim: hidden state size of LSTM\n",
        "      word_vector_dim: size of the word embedding table\n",
        "      dropout: this is applied to the output of the LSTM\n",
        "    \"\"\"\n",
        "    ### Encoder Params. Please do not change these functions at all.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # Embedding table over input vocabulary\n",
        "    self.embedder = nn.Embedding(vocab.GetPieceSize(), word_vector_dim)\n",
        "    self.lstm = nn.LSTM(word_vector_dim, hidden_dim, bidirectional=True, num_layers=num_layers)\n",
        "    self.layer = nn.Linear(hidden_dim*num_layers*2, hidden_dim*num_layers)\n",
        "    self.layer2 = nn.Linear(hidden_dim*num_layers*2, hidden_dim*num_layers)\n",
        "    self.num_layers = num_layers\n",
        "    ### Decoder Params.\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.lstm2 = nn.LSTM(word_vector_dim, hidden_dim, num_layers=2)\n",
        "    self.output_layer2 = nn.Linear(hidden_dim, vocab.GetPieceSize())\n",
        "    self.log_softmax_layer = nn.LogSoftmax(dim=2)\n",
        "\n",
        "  def encode(self, source):\n",
        "    \"\"\"Encode the source batch using a bidirectional LSTM encoder.\n",
        "\n",
        "    Args:\n",
        "      source: An integer tensor with shape (max_source_sequence_length,\n",
        "        batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        encoder_output: The output of the bidirectional LSTM with shape\n",
        "          (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "        encoder_mask: A boolean tensor with shape (max_source_sequence_length,\n",
        "          batch_size) indicating which encoder outputs correspond to padding\n",
        "          tokens. Its elements should be True at positions corresponding to\n",
        "          padding tokens and False elsewhere.\n",
        "        encoder_hidden: The final hidden states of the bidirectional LSTM (after\n",
        "          a suitable projection) that will be used to initialize the decoder.\n",
        "          This should be a pair of tensors (h_n, c_n), each with shape\n",
        "          (num_layers, batch_size, hidden_size). Note that the hidden state\n",
        "          returned by the LSTM cannot be used directly. Its initial dimension is\n",
        "          twice the required size because it contains state from two directions.\n",
        "\n",
        "    The first two return values are not required for the baseline model and will\n",
        "    only be used later in the attention model. If desired, they can be replaced\n",
        "    with None for the initial implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Using packed sequences to more easily work\n",
        "    # with the variable-length sequences represented by the source tensor.\n",
        "    # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "\n",
        "    # Compute a tensor containing the length of each source sequence.\n",
        "    lengths = torch.sum(source != pad_id, axis=0).cpu()\n",
        "\n",
        "    seq_len, batch_size = source.size()\n",
        "\n",
        "    # embedded_sentence: seq_len x batch_size x word_vector_dim\n",
        "    embedded_sentence = self.embedder(source)\n",
        "\n",
        "    # pack it for rnn input\n",
        "    embedded_sentence = torch.nn.utils.rnn.pack_padded_sequence(embedded_sentence,lengths,enforce_sorted=False)\n",
        "\n",
        "    # lstm_out: seq_len x batch_size x 2 * hidden_dim\n",
        "    # h_n, c_n: num_lay*2 x batch_size x hidden_dim\n",
        "    lstm_out, (h_n, c_n) = self.lstm(embedded_sentence)\n",
        "\n",
        "    # Take sum of states across forward and reverse directions.\n",
        "    h_n = h_n.view(2, -1, batch_size, h_n.shape[-1]).sum(1)\n",
        "    c_n = c_n.view(2, -1, batch_size, c_n.shape[-1]).sum(1)\n",
        "\n",
        "    encoder_mask = source == pad_id\n",
        "\n",
        "    lstm_out, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "    return lstm_out, encoder_mask,(h_n, c_n)\n",
        "\n",
        "\n",
        "\n",
        "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "    \"\"\"Run the decoder LSTM starting from an initial hidden state.\n",
        "\n",
        "    The third and fourth arguments are not used in the baseline model, but are\n",
        "    included for compatibility with the attention model in the next section.\n",
        "\n",
        "    Args:\n",
        "      decoder_input: An integer tensor with shape (max_decoder_sequence_length,\n",
        "        batch_size) containing the subword indices for the decoder input. During\n",
        "        evaluation, where decoding proceeds one step at a time, the initial\n",
        "        dimension should be 1.\n",
        "      initial_hidden: A pair of tensors (h_0, c_0) representing the initial\n",
        "        state of the decoder, each with shape (2, batch_size,\n",
        "        hidden_size).\n",
        "      encoder_output: The output of the encoder with shape\n",
        "        (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "      encoder_mask: The output mask from the encoder with shape\n",
        "        (max_source_sequence_length, batch_size). Encoder outputs at positions\n",
        "        with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        log_probs: A tensor with shape (max_decoder_sequence_length, batch_size,\n",
        "          vocab_size) containing scores for the next-word\n",
        "          predictions at each position.\n",
        "        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as\n",
        "          initial_hidden representing the updated decoder state after processing\n",
        "          the decoder input.\n",
        "        attention_weights: This will be implemented later in the attention\n",
        "          model, but in order to maintain compatible type signatures, we also\n",
        "          include it here. This can be None or any other placeholder value.\n",
        "    \"\"\"\n",
        "\n",
        "    # These arguments are not used in the baseline model.\n",
        "    del encoder_output\n",
        "    del encoder_mask\n",
        "\n",
        "    # For the baseline model, we ignore encoder_output and encoder_mask.\n",
        "    # Embed the decoder input\n",
        "    embedded = self.embedder(decoder_input)  # * Added: embed decoder input; shape (seq_len, batch, word_vector_dim).\n",
        "    # Run through the decoder LSTM using the provided initial hidden state.\n",
        "    outputs, hidden = self.lstm2(embedded, initial_hidden)  # * Added: call to lstm2; outputs: (seq_len, batch, hidden_dim).\n",
        "    outputs = self.dropout2(outputs)  # * Added: apply dropout.\n",
        "    logits = self.output_layer2(outputs)  # * Added: project to vocabulary size.\n",
        "    log_probs = self.log_softmax_layer(logits)  # * Added: compute log probabilities.\n",
        "    return log_probs, hidden, None  # * Returns log_probs, updated hidden state, and None for attention.\n",
        "\n",
        "\n",
        "  def compute_loss(self, source, target):\n",
        "    \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "    Args:\n",
        "      source: An integer tensor with shape (max_source_sequence_length,\n",
        "        batch_size) containing subword indices for the source sentences.\n",
        "      target: An integer tensor with shape (max_target_sequence_length,\n",
        "        batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "    Returns:\n",
        "      A scalar float tensor representing cross-entropy loss on the current batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Note that for a target sequence like <s> A B C </s>, you would\n",
        "    # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "    # suffix A B C </s>.\n",
        "\n",
        "    _, batch_size = source.size()\n",
        "    enc_output, encoder_mask, curr_state = self.encode(source)\n",
        "\n",
        "    lengths = torch.sum(target != pad_id, axis=0).cpu()-1\n",
        "    target_prefix = torch.clone(target).cpu()\n",
        "    target_prefix[lengths,torch.arange(target_prefix.size(1))] = pad_id\n",
        "    decoder_input = target_prefix[:-1,:].to(device)\n",
        "\n",
        "    log_probs, _, _ = self.decode(decoder_input, curr_state, enc_output, encoder_mask)\n",
        "\n",
        "    criterion = nn.NLLLoss(ignore_index=pad_id)\n",
        "    target = target[1:,:]\n",
        "    loss = criterion(log_probs.reshape(-1, vocab.GetPieceSize()), target.reshape(-1))\n",
        "\n",
        "\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_UVQzoeHaGn"
      },
      "source": [
        "We define the following functions for training.  This code will run as provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqhYwvEPErel"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, batch_size, model_file):\n",
        "  \"\"\"Train the model and save its best checkpoint.\n",
        "\n",
        "  Model performance across epochs is evaluated using token-level accuracy on the\n",
        "  validation set. The best checkpoint obtained during training will be stored on\n",
        "  disk and loaded back into the model at the end of training.\n",
        "  \"\"\"\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  best_accuracy = 0.0\n",
        "  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "    with tqdm.notebook.tqdm(\n",
        "        make_batch_iterator(training_data, batch_size, shuffle=True),\n",
        "        desc=\"epoch {}\".format(epoch + 1),\n",
        "        unit=\"batch\",\n",
        "        total=math.ceil(len(training_data) / batch_size)) as batch_iterator:\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      for i, (source, target) in enumerate(batch_iterator, start=1):\n",
        "        source, target = source.to(device),target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.compute_loss(source, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
        "      validation_perplexity, validation_accuracy = evaluate_next_token(\n",
        "          model, validation_data)\n",
        "      batch_iterator.set_postfix(\n",
        "          mean_loss=total_loss / i,\n",
        "          validation_perplexity=validation_perplexity,\n",
        "          validation_token_accuracy=validation_accuracy)\n",
        "      if validation_accuracy > best_accuracy:\n",
        "        print(\n",
        "            \"Obtained a new best validation accuracy of {:.2f}, saving model \"\n",
        "            \"checkpoint to {}...\".format(validation_accuracy, model_file))\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        best_accuracy = validation_accuracy\n",
        "  print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "  model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "def evaluate_next_token(model, dataset, batch_size=64):\n",
        "  \"\"\"Compute token-level perplexity and accuracy metrics.\n",
        "\n",
        "  Note that the perplexity here is over subwords, not words.\n",
        "\n",
        "  This function is used for validation set evaluation at the end of each epoch\n",
        "  and should not be modified.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  total_cross_entropy = 0.0\n",
        "  total_predictions = 0\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for source, target in make_batch_iterator(dataset, batch_size):\n",
        "      encoder_output, encoder_mask, encoder_hidden = model.encode(source)\n",
        "      decoder_input, decoder_target = target[:-1], target[1:]\n",
        "      logits, decoder_hidden, attention_weights = model.decode(\n",
        "          decoder_input, encoder_hidden, encoder_output, encoder_mask)\n",
        "      total_cross_entropy += F.cross_entropy(\n",
        "          logits.permute(1, 2, 0), decoder_target.permute(1, 0),\n",
        "          ignore_index=pad_id, reduction=\"sum\").item()\n",
        "      total_predictions += (decoder_target != pad_id).sum().item()\n",
        "      correct_predictions += (\n",
        "          (decoder_target != pad_id) &\n",
        "          (decoder_target == logits.argmax(2))).sum().item()\n",
        "  perplexity = math.exp(total_cross_entropy / total_predictions)\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  return perplexity, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwREgE23qsUz"
      },
      "source": [
        "We can now train the baseline model.\n",
        "\n",
        "Since we haven't yet defined a decoding method to output an entire string, we will measure performance for now by computing perplexity and the accuracy of predicting the next token given a gold prefix of the output. A correct implementation should get a validation token accuracy above 55%. The training code will automatically save the model with the highest validation accuracy and reload that checkpoint's parameters at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "3c9f7733d79144858625e0ecd021a69f",
            "2764139c35a54e959edd1de1e7314d66",
            "dc687e032e554315a45fec33c34c54b4",
            "c535f8c5d4374e38bd90852e304ecda7",
            "7436c93f90f04e53a5c6bda85ce4aab9",
            "f485197cc34446459899c7e55e445e39",
            "57beb3432a55408dbec4bb9ff52c043e",
            "6d4db5079eaf428fbc4f927a80c2656e",
            "1fb0ca0f9b7f4908820e73e965a4e905",
            "8999ee1ea4ec42f4b37aaf7238167a48",
            "54149af30d3549e3806aac19109aedf0",
            "1826ebd2711a4dd585ea23bd33f85255",
            "1bcfbd7357174878a73a9e07ee0f5abb",
            "c6c99c69be5c4c6c8f106198556f9187",
            "112db3e457d04733943b7f950963915d",
            "45034a27da974ab884f3c8cecf147aa9",
            "db7d7f2dfa5e4d1883f77aae26b2194a",
            "1355e071eafd452f9f3fe820471eb93e",
            "1b61edddb3be45f988f2a886295b0e0a",
            "ed34af228b464218805d6068e6b42fc4",
            "bb57be9341cf484b8337a29c30ac6559",
            "2e52169afdbe4e1783c54b457234ca16"
          ]
        },
        "id": "9HamQt-oHgC2",
        "outputId": "ec92f64a-a9f2-4d82-b824-be57e5db1e0f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# We tune it to be fairly optimal, so idealy you don't have to change the parameters below.\n",
        "# But you are welcome to adjust these parameters.\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "hidden_dim = 256\n",
        "word_vector_dim = 256\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "baseline_model = Seq2seqBaseline(hidden_dim,word_vector_dim,dropout,num_layers).to(device)\n",
        "train(baseline_model, num_epochs, batch_size, \"baseline_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lvQiTeih4T3"
      },
      "source": [
        "For evaluation, we also need to be able to generate entire strings from the model. We gave a greedy decoding algorithm here. Later on, we'll implement beam search.\n",
        "\n",
        "A correct implementation of baseline model with greedy decoding should get above 19 BLEU on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBbbZodXKq9J",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def predict_greedy(model, sentences, max_length=100):\n",
        "  \"\"\"Make predictions for the given inputs using greedy inference.\n",
        "\n",
        "  Args:\n",
        "    model: A sequence-to-sequence model.\n",
        "    sentences: A list of input sentences, represented as strings.\n",
        "    max_length: The maximum length at which to truncate outputs in order to\n",
        "      avoid non-terminating inference.\n",
        "\n",
        "  Returns:\n",
        "    A list of predicted translations, represented as strings.\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  batch_size = len(sentences)\n",
        "  indices = make_batch(sentences)\n",
        "  pred_translations = torch.zeros(max_length,batch_size, dtype=torch.long) # max_seq_length x batch_size\n",
        "\n",
        "  enc_output, encoder_mask, curr_state = model.encode(indices)\n",
        "  input = torch.LongTensor([bos_id] * batch_size).view(1, -1).to(device)\n",
        "  finished_mask = torch.zeros(batch_size, dtype=torch.bool).to(device) # mask ones that have finished because some may finish ealier than the other in the same batch\n",
        "  for i in range(0, max_length):\n",
        "      log_probs, hidden, _ = model.decode(input, curr_state, enc_output, encoder_mask)\n",
        "\n",
        "      # Prevent finished sequences from producing non-padding tokens.\n",
        "      log_probs[:, finished_mask, pad_id] = 1e9\n",
        "\n",
        "      # Get the most likely next token and its index.\n",
        "      _, next_tokens = log_probs.squeeze(0).max(dim=1)\n",
        "      pred_translations[i] = next_tokens\n",
        "\n",
        "      # Update the input for the next decoding step.\n",
        "      input = next_tokens.unsqueeze(0)\n",
        "\n",
        "      # Update the state and finished masks.\n",
        "      curr_state = hidden\n",
        "\n",
        "      finished_mask = finished_mask | next_tokens.eq(eos_id)\n",
        "      if finished_mask.all():\n",
        "          break\n",
        "\n",
        "  pred_translation_str = []\n",
        "  for i in range(batch_size):\n",
        "      string = vocab.DecodeIds(pred_translations[:,i].detach().cpu().numpy().astype(int).tolist())\n",
        "      pred_translation_str.append(string)\n",
        "  return pred_translation_str\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, batch_size=64, method=\"greedy\"):\n",
        "  assert method in {\"greedy\", \"beam\"}\n",
        "  source_sentences = [example[0] for example in dataset]\n",
        "  target_sentences = [example[1] for example in dataset]\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    for start_index in range(0, len(source_sentences), batch_size):\n",
        "      if method == \"greedy\":\n",
        "        prediction_batch = predict_greedy(\n",
        "            model, source_sentences[start_index:start_index + batch_size])\n",
        "      else:\n",
        "        prediction_batch = predict_beam(\n",
        "            model, source_sentences[start_index:start_index + batch_size])\n",
        "        prediction_batch = [candidates[0] for candidates in prediction_batch]\n",
        "      predictions.extend(prediction_batch)\n",
        "  return sacrebleu.corpus_bleu(predictions, [target_sentences]).score\n",
        "\n",
        "print(\"Baseline model validation BLEU using greedy search:\",\n",
        "      evaluate(baseline_model, validation_data))\n",
        "\n",
        "### Generate the predictions for the baseline model using greedy decoding on the test_data.\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_baseline.json\", baseline_model, test_data, \"greedy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVQO5ouAKuFQ"
      },
      "outputs": [],
      "source": [
        "def show_predictions(model, num_examples=4, num_beam=5,include_beam=False):\n",
        "  for example in validation_data[:num_examples]:\n",
        "    print(\"Input:\")\n",
        "    print(\" \", example[0])\n",
        "    print(\"Target:\")\n",
        "    print(\" \", example[1])\n",
        "    print(\"Greedy prediction:\")\n",
        "    print(\" \", predict_greedy(model, [example[0]])[0])\n",
        "    if include_beam:\n",
        "      print(f\"Beam predictions (showing top {num_beam}):\")\n",
        "      for candidate in predict_beam(model, [example[0]])[0][:num_beam]:\n",
        "        print(\" \", candidate)\n",
        "    print()\n",
        "\n",
        "print(\"Baseline model sample predictions:\")\n",
        "print()\n",
        "show_predictions(baseline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qre2zMYwi9se"
      },
      "source": [
        "## Attention\n",
        "We'll now improve our seq2seq parsing model by adding [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) style attention. In particular, (largely following the notation in the paper) let $\\bar{\\mathbf{h}}_1, \\ldots, \\bar{\\mathbf{h}}_S$ be the sequence of *encoder* RNN states which are obtained from running the encoder RNN over $S$ source tokens (i.e., the English words in the question in our case). Also let $\\mathbf{h}_t$ be the *decoder* RNN state after it consumes the $t$th target token (i.e., the $t$th token in the logical form). Then $\\boldsymbol{\\alpha}_t$, the attention vector at time $t$, has $S$ elements defined as follows:\n",
        "$$\n",
        "\\alpha_{t,s} = \\mathrm{softmax} ([\\bar{\\mathbf{h}}_{1}^{\\top}; \\ldots; \\bar{\\mathbf{h}}_{S}^{\\top}] \\mathbf{W}_a^{\\top} \\mathbf{h}_t)_s = \\frac{\\exp(\\mathbf{h}_t^{\\top} \\mathbf{W}_a \\bar{\\mathbf{h}}_s)}{\\sum_{s'=1}^S\\exp(\\mathbf{h}_t^{\\top} \\mathbf{W}_a \\bar{\\mathbf{h}}_{s'})},\n",
        "$$\n",
        "where vectors are assumed to be column-vectors and $;$ represents vertical stacking. Note $\\mathbf{W}_a \\in \\mathbb{R}^{d_{dec} \\times d_{enc}}$ is a learnable parameter matrix.\n",
        "\n",
        "Given $\\boldsymbol{\\alpha}_t$, we can then compute a \"context vector\" $\\mathbf{c}_t$ that is a weighted average of the encoder states:\n",
        "$$\n",
        "\\mathbf{c}_t = [\\bar{\\mathbf{h}}_{1}, \\ldots, \\bar{\\mathbf{h}}_{S}] \\boldsymbol{\\alpha}_t\n",
        "$$\n",
        "where $,$ represents horizontal stacking.\n",
        "\n",
        "Finally, we concatenate $\\mathbf{c}_t$ and $\\mathbf{h}_t$ to arrive at a modified decoder state at time $t$ defined as follows:\n",
        "$$\n",
        "\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_c [\\mathbf{c}_t; \\mathbf{h}_t]),\n",
        "$$\n",
        "where $\\mathbf{W}_c \\in \\mathbb{R}^{d_{dec} + d_{enc} \\times d_{dec} + d_{enc}} $ is some learned projection. We can then obtain our logits for each word type as usual with $\\mathbf{V} \\tilde{\\mathbf{h}}_t + \\mathbf{b}$.\n",
        "\n",
        "\n",
        "### **Implementation Task \\# 2**\n",
        "Complete the `decode()` function of the `Seq2seqAttention` module below so that it implements the attention scheme described above. The function should return log probabilities and the final decoder state and cell just as the `decode()` function of the `Seq2seqBaseline` module above does.\n",
        "\n",
        "**Hint:** The most efficient implementations will make use of [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) in computing attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O552dQ8coMmp"
      },
      "outputs": [],
      "source": [
        "class Seq2seqAttention(Seq2seqBaseline):\n",
        "  # Note that this class inherents from Seq2seqBaseline, so all the parameters in Seq2seqBaseline are initialized when this class is\n",
        "  # initialized.\n",
        "  def __init__(self, hidden_dim, enc_output_size, word_vector_dim, dropout,num_layers):\n",
        "    super().__init__(hidden_dim, word_vector_dim, dropout,num_layers)\n",
        "\n",
        "\n",
        "    # Initialize any additional parameters needed for this model that are not\n",
        "    # already included in the baseline model.\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    # Initialize additional parameters for Luong-style attention.\n",
        "    self.W_a = nn.Linear(enc_output_size, hidden_dim, bias=False)  # * Added: projects encoder outputs to hidden_dim.\n",
        "    self.W_c = nn.Linear(hidden_dim + enc_output_size, hidden_dim)   # * Added: projects concatenated [h_t; c_t] to hidden_dim.\n",
        "    self.out = nn.Linear(hidden_dim, vocab.GetPieceSize())           # * Added: final output layer for vocabulary logits.\n",
        "    \n",
        "    ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "    \"\"\"Run the decoder LSTM starting from an initial hidden state.\n",
        "\n",
        "    Args:\n",
        "      decoder_input: An integer tensor with shape (max_decoder_sequence_length,\n",
        "        batch_size) containing the subword indices for the decoder input. During\n",
        "        evaluation, where decoding proceeds one step at a time, the initial\n",
        "        dimension should be 1.\n",
        "      initial_hidden: A pair of tensors (h_0, c_0) representing the initial\n",
        "        state of the decoder, each with shape (num_layers, batch_size,\n",
        "        hidden_size).\n",
        "      encoder_output: The output of the encoder with shape\n",
        "        (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "      encoder_mask: The output mask from the encoder with shape\n",
        "        (max_source_sequence_length, batch_size). Encoder outputs at positions\n",
        "        with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        logits: A tensor with shape (max_decoder_sequence_length, batch_size,\n",
        "          vocab_size) containing scores for the next-word\n",
        "          predictions at each position.\n",
        "        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as\n",
        "          initial_hidden representing the updated decoder state after processing\n",
        "          the decoder input.\n",
        "        attention_weights: A tensor with shape (max_decoder_sequence_length,\n",
        "          batch_size, max_source_sequence_length) representing the normalized\n",
        "          attention weights. This should sum to 1 along the last dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: use a large negative number like -1e9 instead of\n",
        "    # float(\"-inf\") when masking logits to avoid numerical issues.\n",
        "\n",
        "    # Implementation tip: the function torch.bmm may be useful here.\n",
        "    # See https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    embedded = self.embedder(decoder_input)  # (T, B, word_vector_dim); * Embed decoder input.\n",
        "    outputs, hidden = self.lstm2(embedded, initial_hidden)  # (T, B, hidden_dim); * Run LSTM2.\n",
        "    outputs = self.dropout2(outputs)  # * Apply dropout.\n",
        "    \n",
        "    # Ensure encoder outputs match the current batch size.\n",
        "    B_current = decoder_input.size(1)\n",
        "    if encoder_output.size(1) != B_current:\n",
        "        encoder_output = encoder_output.expand(-1, B_current, -1).contiguous()  # * Expand encoder output if needed.\n",
        "        encoder_mask = encoder_mask.expand(-1, B_current).contiguous()  # * Likewise for encoder mask.\n",
        "    \n",
        "    # Prepare encoder outputs: (S, B, enc_output_size) -> (B, S, enc_output_size)\n",
        "    enc_out = encoder_output.transpose(0, 1)\n",
        "    projected_enc = self.W_a(enc_out)  # (B, S, hidden_dim); * Apply W_a to encoder outputs.\n",
        "    dec_out = outputs.transpose(0, 1)   # (B, T, hidden_dim)\n",
        "    scores = torch.bmm(dec_out, projected_enc.transpose(1, 2))  # (B, T, S); * Compute attention scores.\n",
        "    attn_weights = F.softmax(scores, dim=2)  # (B, T, S); * Normalize to get attention weights.\n",
        "    context = torch.bmm(attn_weights, enc_out)  # (B, T, enc_output_size); * Compute context vectors.\n",
        "    concat = torch.cat([dec_out, context], dim=2)  # (B, T, hidden_dim+enc_output_size); * Concatenate decoder state and context.\n",
        "    attended = torch.tanh(self.W_c(concat))  # (B, T, hidden_dim); * Apply tanh after linear projection.\n",
        "    logits = self.out(attended)  # (B, T, vocab_size); * Compute vocabulary logits.\n",
        "    log_probs = F.log_softmax(logits, dim=2)  # * Compute log probabilities.\n",
        "    return log_probs.transpose(0, 1), hidden, attn_weights.transpose(0, 1)  # * Return with time dimension first.\n",
        "\n",
        "    ### END YOUR CODE HERE !!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSJZ9f5sq2cu"
      },
      "source": [
        "As before, we can train an attention model using the provided training code.\n",
        "\n",
        "A correct implementation should get a validation token accuracy above 67 and a validation BLEU above 36 with greedy search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJfL51MKoMmp"
      },
      "outputs": [],
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "hidden_dim = 256\n",
        "enc_output_size = hidden_dim * 2\n",
        "word_vector_dim = 256\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "attention_model = Seq2seqAttention(hidden_dim,enc_output_size, word_vector_dim,dropout,num_layers).to(device)\n",
        "train(attention_model, num_epochs, batch_size, \"attention_model.pt\")\n",
        "print(\"Attention model validation BLEU using greedy search:\",\n",
        "      evaluate(attention_model, validation_data))\n",
        "# Generate the predictions for the attention model using greedy decoding on the test_data.\n",
        "# Corret implementation of the baseline model and attention model should get you full credits here.\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", attention_model, test_data, \"greedy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkiLQmImQP8d"
      },
      "source": [
        "## Beam Search\n",
        "\n",
        "We will now try to improve our model's predictions by decoding with beam search, rather than greedily. Beam search maintains a `beam_size`-length list of hypotheses at each step of decoding. A hypothesis is just a prefix of a full prediction, and is represented in the code below by a `Hyp` object.\n",
        "\n",
        "### What beam search does, in (mostly) words\n",
        "Beam search starts with a hypothesis consisting just of the BOS token, and then proceeds for `output_max_len` steps. At each step $t$, beam search considers adding every possible next-word to the hypotheses/prefixes from step $t-1$ (for a total of `beam_size`*V hypotheses). It then takes the highest scoring `beam_size` of these candidate hypotheses to be the hypotheses at step $t$. The score of a hypothesis of length $t$ is:\n",
        "$$\n",
        "\\mathrm{score}(w_1, \\ldots, w_t) = \\sum_{i=1}^t \\log p(w_i|w_1, \\ldots, w_{i-1}, x)\n",
        "$$\n",
        "where $x$ is the source question. The log probabilities above are just the standard ones output by your RNN decoder.\n",
        "\n",
        "A hypothesis is finished when it ends with an EOS token.\n",
        "\n",
        "With beam search, you should get an improvement of at least 1 BLEU over greedy search, and should reach above 21 BLEU without attention and above 38 BLEU with attention.\n",
        "\n",
        "**Tips:**\n",
        "\n",
        "1) A good general strategy when doing complex code like this is to carefully annotate each line with a comment saying what each dimension represents.\n",
        "\n",
        "2) You should only need one call to topk per step. You do not need to have a topk just over vocabulary first, you can directly go from vocab_size*beam_size to beam_size items.\n",
        "\n",
        "3) Be sure you are correctly keeping track of which beam item a candidate is selected from and updating the beam states, such as LSTM hidden state, accordingly. A single state from the previous time step may need to be used for multiple new beam items or not at all. This includes all state associated with a beam, including all past tokens output by the beam and any extra tensors such as ones remembering when a beam is finished.\n",
        "\n",
        "4) Once an EOS token has been generated, save the hypothesis and take it out of the beam for the next timestep.\n",
        "\n",
        "### **Implementation Task \\# 3**\n",
        "Fill in the missing code in the `predict_beam` function below. You are not implmenting batched beam_search so you only need to consider one sentence at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miS-BuT0oMmq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class Hyp:\n",
        "  \"\"\"\n",
        "  A helper class representing a hypothesis (i.e., the prefix of a prediction) on the beam,\n",
        "  using a linked list.\n",
        "  \"\"\"\n",
        "  def __init__(self, token_id: int, parent, score: float):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      token_id: an integer representing the most recent token added to this hypothesis.\n",
        "      parent: the Hyp object representing the prefix to which we've added this token.\n",
        "      score: the cumulative log-probability score of this hypothesis.\n",
        "    \"\"\"\n",
        "    self.token_id = token_id\n",
        "    self.parent = parent\n",
        "    self.score = score\n",
        "\n",
        "  def trace(self):\n",
        "    \"\"\"\n",
        "    Traces backward through the linked list to recover the whole hypothesis.\n",
        "    \n",
        "    Returns:\n",
        "      A list of token IDs representing the entire hypothesis.\n",
        "    \"\"\"\n",
        "    pred = []\n",
        "    temp = self\n",
        "    while temp is not None:\n",
        "      # Append token if it exists (this avoids error when temp is None)\n",
        "      if temp.token_id is not None:\n",
        "        pred.append(temp.token_id)\n",
        "      temp = temp.parent\n",
        "    return pred[::-1]\n",
        "\n",
        "def predict_beam(model, sentences, k=5, max_length=100):\n",
        "    \"\"\"Output the beam search result for the given sentences.\n",
        "    \n",
        "    Args:\n",
        "      model: The model that will be used to generate the beams.\n",
        "      sentences: A list of sentences (str) that the model will encode and do\n",
        "        beam search over. For simplicity, this list has length 1.\n",
        "      k: Beam size.\n",
        "      max_length: Maximum timesteps you will generate. If it exceeds this timestep, stop.\n",
        "    \n",
        "    Returns:\n",
        "      A list containing a single list of decoded generations (strings) sorted by their scores in descending order.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    V = vocab.GetPieceSize()\n",
        "    # Encode the input sentence (batch_size will be 1)\n",
        "    indices = make_batch(sentences)\n",
        "    enc_output, encoder_mask, encoder_state = model.encode(indices)\n",
        "    \n",
        "    # Initialize the beam.\n",
        "    # Start with a hypothesis that contains the BOS token.\n",
        "    init_hyp = Hyp(bos_id, None, 0.0)\n",
        "    beam = [init_hyp]\n",
        "    # The corresponding hidden state for the beam is the encoder state.\n",
        "    beam_states = [encoder_state]  # Each element is a tuple (h, c) with shape (num_layers, 1, hidden_dim)\n",
        "    \n",
        "    finished_hyps = []  # List to hold finished hypotheses\n",
        "\n",
        "    # Run beam search for a maximum of max_length steps.\n",
        "    for t in range(max_length):\n",
        "        # Prepare the current input tokens for all hypotheses in the beam.\n",
        "        # Each input should be the last token generated by that hypothesis.\n",
        "        # This will create a tensor of shape (1, beam_size).\n",
        "        current_tokens = [torch.tensor([hyp.trace()[-1]], device=device) for hyp in beam]\n",
        "        current_input = torch.stack(current_tokens, dim=1)  # (1, beam_size)\n",
        "        \n",
        "        # Prepare the hidden states by concatenating each beam’s hidden state along the batch dimension.\n",
        "        h_list = [state[0] for state in beam_states]  # each: (num_layers, 1, hidden_dim)\n",
        "        c_list = [state[1] for state in beam_states]\n",
        "        h_beam = torch.cat(h_list, dim=1)  # (num_layers, beam_size, hidden_dim)\n",
        "        c_beam = torch.cat(c_list, dim=1)\n",
        "        current_hidden = (h_beam, c_beam)\n",
        "        \n",
        "        # Decode one timestep for the entire beam.\n",
        "        # log_probs: (1, beam_size, V)\n",
        "        log_probs, new_hidden, _ = model.decode(current_input, current_hidden, enc_output, encoder_mask)\n",
        "        log_probs = log_probs.squeeze(0)  # now (beam_size, V)\n",
        "        \n",
        "        # For each beam candidate, add its current cumulative score to the new log_probs.\n",
        "        beam_scores = torch.tensor([hyp.score for hyp in beam], device=device).unsqueeze(1)  # (beam_size, 1)\n",
        "        total_scores = beam_scores + log_probs  # (beam_size, V)\n",
        "        \n",
        "        # Flatten the scores to shape (beam_size * V) and select the top k candidates.\n",
        "        total_scores_flat = total_scores.view(-1)\n",
        "        topk_scores, topk_indices = torch.topk(total_scores_flat, k)\n",
        "        \n",
        "        # Prepare new lists for the beam and their hidden states.\n",
        "        new_beam = []\n",
        "        new_beam_states = []\n",
        "        \n",
        "        # Process each of the top k candidates.\n",
        "        for score, flat_index in zip(topk_scores.tolist(), topk_indices.tolist()):\n",
        "            # Determine which beam candidate (row) and which token (column) this corresponds to.\n",
        "            prev_beam_idx = flat_index // V\n",
        "            token_id = flat_index % V\n",
        "            \n",
        "            # Extract the new hidden state for the candidate.\n",
        "            candidate_hidden = (\n",
        "                new_hidden[0][:, prev_beam_idx:prev_beam_idx+1, :],\n",
        "                new_hidden[1][:, prev_beam_idx:prev_beam_idx+1, :]\n",
        "            )\n",
        "            \n",
        "            # Create a new hypothesis that extends the previous one with the new token.\n",
        "            parent_hyp = beam[prev_beam_idx]\n",
        "            new_hyp = Hyp(token_id, parent_hyp, score)\n",
        "            \n",
        "            # If the candidate token is EOS, add it to finished hypotheses.\n",
        "            if token_id == eos_id:\n",
        "                finished_hyps.append(new_hyp)\n",
        "            else:\n",
        "                new_beam.append(new_hyp)\n",
        "                new_beam_states.append(candidate_hidden)\n",
        "        \n",
        "        # If no candidates remain in the beam (all ended with EOS), exit early.\n",
        "        if len(new_beam) == 0:\n",
        "            break\n",
        "        \n",
        "        # Update the beam with the new candidates for the next timestep.\n",
        "        beam = new_beam\n",
        "        beam_states = new_beam_states\n",
        "\n",
        "    # If no hypothesis finished with EOS, use the current beam as finished candidates.\n",
        "    if len(finished_hyps) == 0:\n",
        "        finished_hyps = beam\n",
        "    \n",
        "    # Sort finished hypotheses by their cumulative score in descending order.\n",
        "    finished_hyps = sorted(finished_hyps, key=lambda h: h.score, reverse=True)\n",
        "    \n",
        "    # Decode each hypothesis into a string.\n",
        "    decoded_sentences = []\n",
        "    for hyp in finished_hyps:\n",
        "        token_ids = hyp.trace()\n",
        "        # Remove the initial BOS token if present.\n",
        "        if token_ids and token_ids[0] == bos_id:\n",
        "            token_ids = token_ids[1:]\n",
        "        # Cut off at EOS if it appears.\n",
        "        if eos_id in token_ids:\n",
        "            token_ids = token_ids[:token_ids.index(eos_id)]\n",
        "        decoded_sentence = vocab.DecodeIds(token_ids)\n",
        "        decoded_sentences.append(decoded_sentence)\n",
        "    \n",
        "    return [decoded_sentences]\n",
        "\n",
        "# Testing beam search with baseline model (for example)\n",
        "print(\"Baseline model validation BLEU using beam search:\",\n",
        "      evaluate(baseline_model, validation_data, batch_size=1, method=\"beam\"))\n",
        "print()\n",
        "print(\"Baseline model sample predictions:\")\n",
        "print()\n",
        "show_predictions(baseline_model, include_beam=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH2dal_foMmq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Attention model validation BLEU using beam search:\",\n",
        "      evaluate(attention_model, validation_data, batch_size=1, method=\"beam\"))\n",
        "print()\n",
        "print(\"Attention model sample predictions:\")\n",
        "print()\n",
        "show_predictions(attention_model, include_beam=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envJv-ufzAAG"
      },
      "source": [
        "Run the cells to generate the beam_seqs.json file required for submission to check correctness of your beam_search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs4ABxnBy-j-"
      },
      "outputs": [],
      "source": [
        "!gdown 1zKM1vgKkRye1COYh4IlH_m0xDCq7chFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di1XeW0JzE8v"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")  # Force CPU usage.\n",
        "hidden_dim = 100\n",
        "word_vector_dim = 100\n",
        "num_layers = 1\n",
        "dropout = 0.3\n",
        "\n",
        "# Create the model and move it to CPU.\n",
        "special_model = Seq2seqBaseline(hidden_dim, word_vector_dim, dropout, num_layers).to(device)\n",
        "# Load the state dictionary with map_location set to CPU.\n",
        "sd = torch.load(\"special_model_beam_search.pt\", map_location=device)\n",
        "special_model.load_state_dict(sd)\n",
        "\n",
        "V = vocab.GetPieceSize()\n",
        "nsrcs, srcsize = 11, 6\n",
        "special_preds = {}\n",
        "for beam_size in [1, 5, 10, 15]:\n",
        "    torch.manual_seed(beam_size)\n",
        "    srcs = [(vocab.DecodeIds(torch.LongTensor(srcsize).random_(0, V).numpy().tolist()),\n",
        "             'filler target sentence filler target sentence filler target sentence') \n",
        "            for _ in range(nsrcs)]\n",
        "    predictions = []\n",
        "    source_sentences = [x[0] for x in srcs]\n",
        "    for start_index in range(0, len(source_sentences), 1):\n",
        "        prediction_batch = predict_beam(\n",
        "            special_model, \n",
        "            source_sentences[start_index:start_index + 1], \n",
        "            k=beam_size,\n",
        "            max_length=50\n",
        "        )\n",
        "        predictions.extend(prediction_batch)\n",
        "    special_preds[beam_size] = predictions\n",
        "\n",
        "with open(\"beam_seqs.json\", \"w\") as f:\n",
        "    json.dump(special_preds, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ksZlRTzYpUJ"
      },
      "source": [
        "If you implemented beam search correctly, you can save the results of beam search for the attention model by uncommenting the code below. It will have higher BLEU score, but the greedy decoding should give you full 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tou6oLrlYmKE"
      },
      "outputs": [],
      "source": [
        "# Ensure the attention model is on CPU.\n",
        "attention_model = attention_model.to(torch.device(\"cpu\"))\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", attention_model, test_data, \"beam\", batch_size=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLoiXBWMaSPc"
      },
      "source": [
        "# Experimentation: 1-Page Report\n",
        "\n",
        "Now it's time for you to experiment.  Try to improve the denotation accuracy on the validation set further and aim for a validation BLEU score of 42. Feel free to modify the code above directly or copy it in new cells below.\n",
        "\n",
        "**NOTE:** We will award at least 7 of the 10 points if your improved model reaches a BLEU score of at least 40 on the hidden test cases on Gradescope.\n",
        "\n",
        "Here are some ideas to try out:\n",
        "* **Back translation**: Since the training dataset is small, another strategy for improving performance might be to generate more training instances. One popular technique people used in machine translation is called back translation(https://arxiv.org/abs/1511.06709). Train another model from English to Gernman and consturct more data.  \n",
        "For this extension, you are allowed to use external datasets with monolingual text in either English or German. You cannot use datasets containing both English and German texts.\n",
        "\n",
        "* **Word embeddings**: You can try initializing the input word embeddings with [Glove vectors](https://nlp.stanford.edu/projects/glove/). You should try both finetuning these embeddings or keeping them fixed.\n",
        "* **Regularization**: You can also try some of the regularization techniques we tried in the language modeling assignment, however these may or may not help.\n",
        "* **Tokenization**: Exploring with how tokenization is done or how big vocabulary size is can also be helpful. We used unigram language model subword tokenization and use a vocabulary of size 8000. You can consider using BPE (a very popular tokenization technique) or using characters or even some others.\n",
        "* **Hyperparameter tuning**: Finally you can try playing around with hyperparameters to see if there is a better configuration than what we have provided (we only did a modest amount of tuning).\n",
        "\n",
        "For this section, you will submit a write-up describing the extensions and/or modifications that you tried.  Your write-up should be **1-page maximum** in length and should be submitted in PDF format.  You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf.\n",
        "For full credit, your write-up should include:\n",
        "1.   A concise and precise description of the extension that you tried.\n",
        "2.   A motivation for why you believed this approach might improve your model.\n",
        "3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n",
        "4.   A bottom-line summary of your results comparing the scores of your improvement to the original model.\n",
        "The purpose of this exercise is to experiment, so feel free to try/ablate multiple of the suggestions above as well as any others you come up with!\n",
        "When you submit the file, please name it `report.pdf`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###########################\n",
        "# Improved Model Training #\n",
        "###########################\n",
        "\n",
        "# Define a custom label smoothing loss function.\n",
        "def label_smoothing_loss(log_probs, target, smoothing=0.1, vocab_size=vocab.GetPieceSize(), ignore_index=pad_id):\n",
        "    # log_probs: (N, vocab_size), target: (N)\n",
        "    with torch.no_grad():\n",
        "        true_dist = torch.zeros_like(log_probs)\n",
        "        true_dist.fill_(smoothing / (vocab_size - 1))\n",
        "        mask = (target == ignore_index)\n",
        "        target = target.clone()\n",
        "        target[mask] = 0  # Arbitrary index for ignore positions.\n",
        "        true_dist.scatter_(1, target.unsqueeze(1), 1.0 - smoothing)\n",
        "        true_dist[mask] = 0\n",
        "    loss = torch.sum(-true_dist * log_probs, dim=1)\n",
        "    return loss.mean()\n",
        "\n",
        "# Create an improved model class that overrides compute_loss with label smoothing.\n",
        "class ImprovedSeq2seqAttention(Seq2seqAttention):\n",
        "    def compute_loss(self, source, target):\n",
        "        \"\"\"Compute loss using label smoothing instead of plain NLLLoss.\"\"\"\n",
        "        _, batch_size = source.size()\n",
        "        enc_output, encoder_mask, curr_state = self.encode(source)\n",
        "        \n",
        "        # Create the decoder input: shift target right (remove the last token).\n",
        "        lengths = torch.sum(target != pad_id, axis=0).cpu() - 1\n",
        "        target_prefix = torch.clone(target).cpu()\n",
        "        target_prefix[lengths, torch.arange(target_prefix.size(1))] = pad_id\n",
        "        decoder_input = target_prefix[:-1, :].to(device)\n",
        "        \n",
        "        log_probs, _, _ = self.decode(decoder_input, curr_state, enc_output, encoder_mask)\n",
        "        # Flatten the predictions and targets.\n",
        "        log_probs_flat = log_probs.reshape(-1, vocab.GetPieceSize())\n",
        "        target_flat = target[1:, :].reshape(-1)\n",
        "        loss = label_smoothing_loss(log_probs_flat, target_flat)\n",
        "        return loss\n",
        "\n",
        "# Hyperparameters for the improved model.\n",
        "improved_hidden_dim = 512\n",
        "improved_word_vector_dim = 512\n",
        "improved_num_layers = 2\n",
        "improved_dropout = 0.5\n",
        "num_epochs = 15\n",
        "batch_size = 32\n",
        "\n",
        "# Create the improved model.\n",
        "improved_model = ImprovedSeq2seqAttention(improved_hidden_dim, \n",
        "                                          improved_hidden_dim * 2,  # encoder output size = 2 * hidden_dim\n",
        "                                          improved_word_vector_dim, \n",
        "                                          improved_dropout, \n",
        "                                          improved_num_layers).to(device)\n",
        "\n",
        "# Set up optimizer and a learning rate scheduler.\n",
        "optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
        "\n",
        "# Training loop.\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    improved_model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for source, target in make_batch_iterator(training_data, batch_size, shuffle=True):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = improved_model.compute_loss(source, target)\n",
        "        loss.backward()\n",
        "        # Gradient clipping to prevent exploding gradients.\n",
        "        torch.nn.utils.clip_grad_norm_(improved_model.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    avg_loss = total_loss / num_batches\n",
        "    val_perplexity, val_accuracy = evaluate_next_token(improved_model, validation_data)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Val Perplexity: {val_perplexity:.4f}, Val Accuracy: {val_accuracy:.2f}\")\n",
        "    scheduler.step(avg_loss)\n",
        "    # Save the model if validation accuracy improves.\n",
        "    if val_accuracy > best_val_acc:\n",
        "         best_val_acc = val_accuracy\n",
        "         torch.save(improved_model.state_dict(), \"improved_model.pt\")\n",
        "         print(\"New best model saved.\")\n",
        "\n",
        "# Reload best model.\n",
        "improved_model.load_state_dict(torch.load(\"improved_model.pt\"))\n",
        "print(\"Improved model BLEU with greedy search:\", evaluate(improved_model, validation_data))\n",
        "\n",
        "# Generate predictions for submission using beam search.\n",
        "generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", improved_model, test_data, \"beam\", batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUB9GnsQYwX3"
      },
      "outputs": [],
      "source": [
        "### For the improvement:\n",
        "# If you implemented your own improvements, submit the predictions using that model on the test data instead:\n",
        "\n",
        "#generate_predictions_file_for_submission(\"seq2seq_predictions_attention.json\", improved_model, test_data, \"beam\", batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHTOfrCG8CRF"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Upload a submission with the following files to Gradescope:\n",
        "* proj_3.ipynb (rename to match this exactly)\n",
        "* seq2seq_predictions_baseline.json (baseline_model with greedy decoding would suffice)\n",
        "* seq2seq_predictions_attention.json (if you have an improved model, use that model to generate this file, otherwise submiting attention model with greedy decoding will get you full points (20%) but probably not for the improvement evaluation (10%))\n",
        "* beam_seqs.json\n",
        "* report.pdf\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set accuracies shown by the autograder are on different data from your validation set.  We will compare your score on the test set to our model's score and assign points based on that."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "112db3e457d04733943b7f950963915d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb57be9341cf484b8337a29c30ac6559",
            "placeholder": "​",
            "style": "IPY_MODEL_2e52169afdbe4e1783c54b457234ca16",
            "value": " 0/1813 [00:00&lt;?, ?batch/s]"
          }
        },
        "1355e071eafd452f9f3fe820471eb93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1826ebd2711a4dd585ea23bd33f85255": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bcfbd7357174878a73a9e07ee0f5abb",
              "IPY_MODEL_c6c99c69be5c4c6c8f106198556f9187",
              "IPY_MODEL_112db3e457d04733943b7f950963915d"
            ],
            "layout": "IPY_MODEL_45034a27da974ab884f3c8cecf147aa9"
          }
        },
        "1b61edddb3be45f988f2a886295b0e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bcfbd7357174878a73a9e07ee0f5abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db7d7f2dfa5e4d1883f77aae26b2194a",
            "placeholder": "​",
            "style": "IPY_MODEL_1355e071eafd452f9f3fe820471eb93e",
            "value": "epoch 1:   0%"
          }
        },
        "1fb0ca0f9b7f4908820e73e965a4e905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2764139c35a54e959edd1de1e7314d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f485197cc34446459899c7e55e445e39",
            "placeholder": "​",
            "style": "IPY_MODEL_57beb3432a55408dbec4bb9ff52c043e",
            "value": "training:   0%"
          }
        },
        "2e52169afdbe4e1783c54b457234ca16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c9f7733d79144858625e0ecd021a69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2764139c35a54e959edd1de1e7314d66",
              "IPY_MODEL_dc687e032e554315a45fec33c34c54b4",
              "IPY_MODEL_c535f8c5d4374e38bd90852e304ecda7"
            ],
            "layout": "IPY_MODEL_7436c93f90f04e53a5c6bda85ce4aab9"
          }
        },
        "45034a27da974ab884f3c8cecf147aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54149af30d3549e3806aac19109aedf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57beb3432a55408dbec4bb9ff52c043e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d4db5079eaf428fbc4f927a80c2656e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7436c93f90f04e53a5c6bda85ce4aab9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8999ee1ea4ec42f4b37aaf7238167a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb57be9341cf484b8337a29c30ac6559": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c535f8c5d4374e38bd90852e304ecda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8999ee1ea4ec42f4b37aaf7238167a48",
            "placeholder": "​",
            "style": "IPY_MODEL_54149af30d3549e3806aac19109aedf0",
            "value": " 0/10 [00:00&lt;?, ?epoch/s]"
          }
        },
        "c6c99c69be5c4c6c8f106198556f9187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b61edddb3be45f988f2a886295b0e0a",
            "max": 1813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed34af228b464218805d6068e6b42fc4",
            "value": 0
          }
        },
        "db7d7f2dfa5e4d1883f77aae26b2194a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc687e032e554315a45fec33c34c54b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4db5079eaf428fbc4f927a80c2656e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fb0ca0f9b7f4908820e73e965a4e905",
            "value": 0
          }
        },
        "ed34af228b464218805d6068e6b42fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f485197cc34446459899c7e55e445e39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
