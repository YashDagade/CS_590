{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1VQ7aXeMqZa"
      },
      "source": [
        "# Assignment 4: Information Retrieval\n",
        "\n",
        "In this assignment you will implement a sparse retriever (TF-IDF) and a dense retriever (DPR) that find answers for a QA task. For the latter, we will finetune a pretrained (compact) [BERT](https://arxiv.org/abs/1810.04805) model from the HuggingFace library.\n",
        "\n",
        "Real retrievers operate on very large corpora of text, e.g., the entire Wikipedia, but in this assignment to save on computational resources we will instead build retrievers that only operate on a small set of text passages (~100) provided for each input query. Such models are also known as *rerankers* and are often the second step of commercial retrieval systems.\n",
        "\n",
        "**Warning**: Do not start this project the day before it is due!  Some parts require 20 minutes or more to run, so debugging and tuning can take a significant amount of time.\n",
        "\n",
        "**Grading Rubric**\n",
        "- 70% results\n",
        " - 20% preds_tfidf.txt (5% correctness + 15% meets target)\n",
        " - 20% preds_inbatch.txt (meets target)\n",
        " - 20% preds_hardneg.txt (meets target)\n",
        " - 10% preds_hardneg.txt (improvement over target)\n",
        "  \n",
        "- 30% writeup\n",
        " - 12.5% clarity\n",
        " - 12.5% correctness\n",
        " - 5% interestingness of ideas\n",
        "\n",
        "\n",
        "TA contact for this assignment:\n",
        "Gaurav Rajesh Parikh(gr90@duke.edu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiRScr9gme7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Te1VClNdfmt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import transformers\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import notebook as tqdm\n",
        "\n",
        "## my imports\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRaydJxJD7yx"
      },
      "source": [
        "We will use [BERT-Mini](https://huggingface.co/google/bert_uncased_L-4_H-256_A-4) for this assignment which is a relatively small pretrained model (~11M parameters), since it easily fits in the memory of Colab GPUs and training and inference are both usually fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xEAPwnv-D5SP"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/bert_uncased_L-4_H-256_A-4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-FvPeUJmd2nM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    assert torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\")\n",
        "except:\n",
        "    device = torch.device(\"mps\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwofi3PoxLlv"
      },
      "source": [
        "The following code helps wrap the output text for better readability. You can ignore this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G5A3dTKNxKX8"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import HTML, display\n",
        "\n",
        "# def set_css():\n",
        "#   display(HTML('''\n",
        "#   <style>\n",
        "#     pre {\n",
        "#         white-space: pre-wrap;\n",
        "#     }\n",
        "#   </style>\n",
        "#   '''))\n",
        "# get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPUTSaTLM2VG"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use the [CuratedTREC](https://trec.nist.gov/data/qa.html) QA data which consists of questions paired with regex patterns specifying the expected answers to those questions. In addition to the questions, for each of them, we also have 100 passages from Wikipedia, which are all somewhat relevant to the question, but only some of them actually contain the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeLlbCo-bvFW"
      },
      "source": [
        "First lets download the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "enmkGvVeMn-v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-FSEPWVOX2l7BCvjuIa3SwW37osJEBhH\n",
            "To: /Users/yd211/Documents/GitHub/CS_590/HW4/train.jsonl\n",
            "100%|██████████████████████████████████████| 73.1M/73.1M [00:02<00:00, 27.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AAu2LBaSjK754zHydc4BBybYgb_GV2e\n",
            "To: /Users/yd211/Documents/GitHub/CS_590/HW4/dev.jsonl\n",
            "100%|██████████████████████████████████████| 7.54M/7.54M [00:00<00:00, 20.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L3KNqeGQqmHob7v7oDPiyb-foqQSMHlA\n",
            "To: /Users/yd211/Documents/GitHub/CS_590/HW4/test.jsonl\n",
            "100%|██████████████████████████████████████| 44.9M/44.9M [00:01<00:00, 33.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1-FSEPWVOX2l7BCvjuIa3SwW37osJEBhH\n",
        "!gdown 1-AAu2LBaSjK754zHydc4BBybYgb_GV2e\n",
        "!gdown 1L3KNqeGQqmHob7v7oDPiyb-foqQSMHlA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4aeKDllcWjZ"
      },
      "source": [
        "The first thing we need to do is tokenize the questions and passages in all the above files. We will use the Huggingface Tokenizer class for this. Note that tokenizers are specific to the pretrained model that we use (and are hence loaded from the model checkpoint). While the pretrained model will only be used later for the dense retriever, we will use the same tokenizer throughout.\n",
        "\n",
        "Note that we do not need to tokenize the answers as these will only be used for evaluation.\n",
        "\n",
        "Running the cell below will take a few minutes as the passages need to be tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zOyOeqPwc23c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e74469aa85054813b49dc2c8e25b457a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read and tokenized 1125 items from train.jsonl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe401c6e14044f95bb1abbfa0d8fef6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read and tokenized 116 items from dev.jsonl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d8f7ed3a78420e8925762fefbe4f1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read and tokenized 694 items from test.jsonl\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "def tokenize_qa(filename):\n",
        "  with open(filename) as f:\n",
        "    data = []\n",
        "    for line in tqdm.tqdm(f):\n",
        "      item = json.loads(line.strip())\n",
        "      item[\"question_indices\"] = tokenizer(item[\"question\"])[\"input_ids\"]\n",
        "      item[\"passage_indices\"] = tokenizer(item[\"passages\"])[\"input_ids\"]\n",
        "      data.append(item)\n",
        "  print(\"Read and tokenized %d items from %s\" % (len(data), filename))\n",
        "  return data\n",
        "\n",
        "train_data = tokenize_qa(\"train.jsonl\")\n",
        "dev_data = tokenize_qa(\"dev.jsonl\")\n",
        "test_data = tokenize_qa(\"test.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR7NxPxamxEL"
      },
      "source": [
        "We can inspect what a typical example in the train and dev sets looks like. Note that the answers are regular expressions which need to be matched to a string / passage returned by the model.\n",
        "\n",
        "When retrieving small passages of text, one common trick is to prepend the title of the page on which the passage appears to the passage itself. This sometimes helps resolve referential ambiguities in the passage. We have already done this for the passages provided: you will notice that each passage has the following structure \"*page_title* || *passage_text*\". These passages were capped at 100 words, so they might contain incomplete sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gLYlDWIJmD97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How many states have a lottery?\n",
            "Answer regex: ['26\\\\s+states?|27\\\\s+states?|28\\\\s+states?|30\\\\s+states?|32\\\\s+states?|33\\\\s+lottery\\\\s+states?|33\\\\s+states?|Twenty\\\\s*-?\\\\s*eight|Twenty\\\\s*-?\\\\s*six|^\\\\s*26\\\\s*$|^\\\\s*27\\\\s*$|^\\\\s*28\\\\s*$|^\\\\s*30\\\\s*$|^\\\\s*31\\\\s*$|^\\\\s*33\\\\s*$|44 States|43 states']\n",
            "-----------------------------------------\n",
            "Random passage: Civic lottery || Civic lottery A civic lottery, a popular term for the contemporary use of sortition or allotment, is a lottery-based method for selecting citizens for public service or office. It is based on the premise that citizens in a democracy have both a duty and the desire to serve their society by participating in its governance. Today, the most common use of the civic lottery process is found in many Anglo-Saxon judicial systems where citizen juries are summoned to hear and render verdicts in court cases. The term for this is popularly known as jury duty. Civic lotteries are increasingly popular\n"
          ]
        }
      ],
      "source": [
        "index = random.choice(range(len(dev_data)))\n",
        "print(\"Question:\", dev_data[index][\"question\"])\n",
        "print(\"Answer regex:\", dev_data[index][\"answers\"])\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Random passage:\", random.choice(dev_data[index][\"passages\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_ROHWZDGaDy"
      },
      "source": [
        "As always we will iterate over the data in batches. For retrieval reranking our batches will consist of queries and the associated passage lists. For the latter we will use a 3-dimensional tensor with the shape `(batch_size, num_passages, max_passage_length)`.\n",
        "\n",
        "Also, the pretrained model we will use has a max length it can process (`tokenizer.model_max_length`), so we will truncate the passages to be shorter than this maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VfLjDrg6ynL8"
      },
      "outputs": [],
      "source": [
        "def make_batch(batch_indices):\n",
        "  \"\"\"Convert a list of variable length texts into a batch.\n",
        "\n",
        "  Args:\n",
        "    texts: A list of list of token indices.\n",
        "\n",
        "  Returns:\n",
        "    A LongTensor of size (batch_size, max_sequence_length) containing the\n",
        "    subword indices for the texts, where max_sequence_length is the length\n",
        "    of the longest text and batch_size is the number of sentences in the batch.\n",
        "    Empty slots at the end of shorter sequences should be filled with padding\n",
        "    tokens. The tensor should be located on the device defined at the beginning\n",
        "    of the notebook.\n",
        "  \"\"\"\n",
        "  return torch.nn.utils.rnn.pad_sequence(\n",
        "      [torch.LongTensor(item) for item in batch_indices],\n",
        "      padding_value=tokenizer.pad_token_id,\n",
        "      batch_first=True)\n",
        "\n",
        "def make_batch_iterator(dataset, batch_size, shuffle=False):\n",
        "  \"\"\"Make a batch iterator that yields source-target pairs.\n",
        "\n",
        "  Args:\n",
        "    dataset: A list of dicts with the keys `question_indices` and `passages_indices`.\n",
        "    batch_size: An integer batch size.\n",
        "    shuffle: A boolean indicating whether to shuffle the examples.\n",
        "\n",
        "  Yields:\n",
        "    question_batch: batch_size x max_question_length\n",
        "    passage_batch: batch_size x num_passages x max_passage_length\n",
        "    example_batch: batch_size list of complete examples\n",
        "  \"\"\"\n",
        "\n",
        "  if shuffle:\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "  for start_index in range(0, len(dataset), batch_size):\n",
        "    example_batch = dataset[start_index:start_index + batch_size]\n",
        "    num_exs = len(example_batch)\n",
        "    # tokenize questions.\n",
        "    question_indices = [example[\"question_indices\"] for example in example_batch]\n",
        "    question_batch = make_batch(question_indices)\n",
        "    # Each example has exactly 100 passages so we can flatten and reshape later.\n",
        "    passage_indices = [psg_idx for example in example_batch for psg_idx in example[\"passage_indices\"]]\n",
        "    passage_batch = make_batch(passage_indices)\n",
        "    passage_batch = passage_batch[:, :tokenizer.model_max_length]\n",
        "    passage_batch = passage_batch.view(num_exs, len(dataset[0][\"passages\"]), -1)\n",
        "    yield question_batch, passage_batch, example_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVRx-fDU6ekc"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We will evaluate our rerankers by measuring recall@K. This checks whether at least one of the top-K passages after reranking contain the answer or not (as judged by the regex expression). The overall recall@K is computed as the fraction of questions for which one of the top-K passages contain the answer. We will focus on recall@5 and recall@20 throughout this assignment.\n",
        "\n",
        "You don't need to modify the evaluation code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hhnv5u7N6d9V"
      },
      "outputs": [],
      "source": [
        "class Reranker:\n",
        "  \"\"\"Interface for different rerankers to subclass.\"\"\"\n",
        "  def fit(self, data_iterator):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def rerank(self, query_batch, passage_batch):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def _normalize(text):\n",
        "  return unicodedata.normalize(\"NFD\", text)\n",
        "\n",
        "def regex_match(text, pattern):\n",
        "  \"\"\"Test if a regex pattern is contained within a text.\"\"\"\n",
        "  try:\n",
        "    pattern = re.compile(pattern, flags=re.IGNORECASE + re.UNICODE + re.MULTILINE)\n",
        "  except BaseException:\n",
        "    return False\n",
        "  return pattern.search(text) is not None\n",
        "\n",
        "def has_answer(text, answers):\n",
        "  \"\"\"Checks if any of the answers are in the text.\"\"\"\n",
        "  for single_answer in answers:\n",
        "    single_answer = _normalize(single_answer)\n",
        "    if regex_match(text, single_answer):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def in_top_K(answers, ranked_passages, K):\n",
        "  \"\"\"Checks whether any of the top-K passages matches the answer regex.\n",
        "\n",
        "  Args:\n",
        "    answers: A list of regex patterns for the target answers.\n",
        "    ranked_passages: A list of passage strings ranked in decreasing order of\n",
        "      relevance.\n",
        "    K: A list of top-K values to check. These must be in ascending order.\n",
        "\n",
        "  Returns:\n",
        "    A list, same size as K, with booleans indicating whether the answer was in\n",
        "    the top-K or not. Note that once we have a True for one particular value of\n",
        "    K, all subsequent values will also be True.\n",
        "  \"\"\"\n",
        "  top_hit = None\n",
        "  for i, psg in enumerate(ranked_passages):\n",
        "    if has_answer(psg, answers):\n",
        "      top_hit = i+1\n",
        "      break\n",
        "  if top_hit is None:\n",
        "    return [False] * len(K)\n",
        "  out = []\n",
        "  for k in K:\n",
        "    out.append(k >= top_hit)\n",
        "  return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(data_iterator, model, K=[5, 20], verbose=False):\n",
        "  \"\"\"Computes recall@K for the rerankings produced by the model.\n",
        "\n",
        "  Args:\n",
        "    data_iterator: An iterator over the data as produced by the make_batch_iterator\n",
        "      above.\n",
        "    model: A Reranker model which implements the rerank method (see below).\n",
        "\n",
        "  Returns:\n",
        "    The recall@K scores of the reranker.\n",
        "  \"\"\"\n",
        "  # if model is a torch module, put it in eval mode.\n",
        "  if isinstance(model, nn.Module):\n",
        "    model.eval()\n",
        "\n",
        "  recalls = [0.] * len(K)\n",
        "  total = 0\n",
        "  for question_batch, passage_batch, example_batch in tqdm.tqdm(data_iterator):\n",
        "    ranking_indices, ranking_scores = model.rerank(question_batch, passage_batch)\n",
        "    for i in range(len(example_batch)):\n",
        "      ranked_passages = [example_batch[i][\"passages\"][j] for j in ranking_indices[i]]\n",
        "      topK = in_top_K(example_batch[i][\"answers\"], ranked_passages, K)\n",
        "      recalls = [r+int(k) for r, k in zip(recalls, topK)]\n",
        "      total += 1\n",
        "  recalls = [r / total for r in recalls]\n",
        "  if verbose:\n",
        "    print(\"\\t\".join([\"K\"] + [str(k) for k in K]))\n",
        "    print(\"-\" * (12 * len(K)))\n",
        "    print(\"\\t\".join([\"Rec@K\"] + [\"%.3f\" % r for r in recalls]))\n",
        "  return recalls\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_predictions(data_iterator, model, output_file):\n",
        "  \"\"\"Save the rank scores of passages for each question in the data.\n",
        "\n",
        "  Args:\n",
        "    data_iterator: An iterator over the data as produced by the make_batch_iterator\n",
        "      above.\n",
        "    model: A Reranker model which implements the rerank method (see below).\n",
        "    output_file: A file to save the predictions to.\n",
        "  \"\"\"\n",
        "\n",
        "  # if model is a torch module, put it in eval mode.\n",
        "  if isinstance(model, nn.Module):\n",
        "    model.eval()\n",
        "\n",
        "  print('Saving predictions to', output_file)\n",
        "\n",
        "  with open(output_file, \"w\") as f:\n",
        "    for question_batch, passage_batch, example_batch in tqdm.tqdm(data_iterator):\n",
        "      ranking_indices, ranking_scores = model.rerank(question_batch, passage_batch)\n",
        "      # convert numpy to python lists\n",
        "      ranking_indices = ranking_indices.tolist()\n",
        "      ranking_scores = ranking_scores.tolist()\n",
        "      for i in range(len(example_batch)):\n",
        "        f.write(' '.join([str(x) for x in ranking_indices[i]]) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnpP1WU9zZ1l"
      },
      "source": [
        "## Random Reranker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF8_XpmZX_KX"
      },
      "source": [
        "Let's define a trivial reranker which simply returns a random ordering of the documents and check its recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cqxHFKbjYHo4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "990aa9ecb5ef4358ade0f32b31226b09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K\t5\t20\n",
            "------------------------\n",
            "Rec@K\t0.422\t0.741\n"
          ]
        }
      ],
      "source": [
        "class TrivialReranker(Reranker):\n",
        "  def rerank(self, query_batch, passage_batch):\n",
        "    num_passages = passage_batch.shape[1]\n",
        "    indices = np.zeros(passage_batch.shape[:2], dtype=np.int32)\n",
        "    scores = np.zeros(passage_batch.shape[:2], dtype=np.int32)\n",
        "    for i in range(passage_batch.shape[0]):\n",
        "      indices[i, :] = np.random.permutation(passage_batch.shape[1])\n",
        "    return indices, scores\n",
        "\n",
        "dev_iterator = make_batch_iterator(dev_data, 16)\n",
        "trivial_reranker = TrivialReranker()\n",
        "_ = evaluate(dev_iterator, trivial_reranker, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uXQ296g0x3Q"
      },
      "source": [
        "The trivial reranker already gets ~35-40\\% recall@5 and ~75\\% recall@20. This is because there are several passages for each question which contain the correct answer and even a random ordering can sometimes place (at least) one of them in the top K.\n",
        "\n",
        "These numbers will serve as a baseline to compare our other rerankers to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GWPHgYscuYa"
      },
      "source": [
        "## TFIDF\n",
        "\n",
        "Next, lets implement a TF-IDF reranker. Recall that TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical score that reflects how important a word is to a document. The TF component measures how frequently a term appears in a document, while the IDF component measures how unique or rare a term is across multiple documents.\n",
        "\n",
        "Mathematically, the TF-IDF score for a term in a document can be calculated as:\n",
        "\n",
        "\\begin{gathered}\n",
        "\\operatorname{tf-idf}(t, d)=\\operatorname{tf}_{t, d} \\cdot \\operatorname{idf}_t,\n",
        "\\end{gathered}\n",
        "\n",
        "\n",
        "where TF is computed from the frequency of the term in the document:\n",
        "\n",
        "\\begin{gathered}\n",
        "\\mathrm{tf}_{t, d}=\\log _{10}(\\#(t, d)+1),\n",
        "\\end{gathered}\n",
        "\n",
        "and IDF is calculated as:\n",
        "\n",
        "\\begin{gathered}\n",
        "\\operatorname{idf}_t=\\log _{10} \\frac{1 + N}{1 + \\sum_{d^{\\prime}} 1\\left(t \\in d^{\\prime}\\right)}.\n",
        "\\end{gathered}\n",
        "\n",
        "Here, $N$ is the total number of documents in the corpus, and $\\sum_{d^{\\prime}} 1\\left(t \\in d^{\\prime}\\right)$ is the number of documents containing the term (document frequency). To avoid a division by zero if the term is not in the corpus, we adopt the common practice to increase both the numerator and the denominator by 1.\n",
        "\n",
        "Given L2-normalized TF-IDF vectors for a query and a document, we compute the similarity score between them by taking the dot-product between them.\n",
        "\n",
        "TF-IDF vectors are the same size as the vocabulary but they are very sparse, i.e., most entries are zero. Hence, we will use [Sparse Matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html) from Scipy for storing the document and query vectors across a batch and taking the dot product between them. These data structures only store the non-zero elements of the sparse matrix and are hence very efficient both in terms of memory and computation. In particular, we will use the compressed sparse row (CSR) matrix representation which you can read more about [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).\n",
        "\n",
        "Please complete the `fit` and `vectorize` functions below. The `fit` method takes in an iterator over the training data and computes the IDF scores for the entire vocabulary. The `vectorize` method takes in a batch of token indices and computes the TF-IDF vector for each element in the batch. For both of these, you might want to (but don't have to) first implement a common utility for computing a sparse vector of term counts in the document, which can then be used to compute the IDF scores as well as the TF-IDF vectors.\n",
        "\n",
        "**Hint**: see this [post](https://stackoverflow.com/questions/26958233/numpy-row-wise-unique-elements) for efficient batched `unique` in numpy\n",
        "\n",
        "A correctly implemented TF-IDF reranker should give you a recall@5 above 0.55 and a recall@20 above 0.81."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_BM_ECKfu6Kd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dde755b355d64d7298eb36ffaf435d9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K\t5\t20\n",
            "------------------------\n",
            "Rec@K\t0.517\t0.828\n",
            "Saving predictions to preds_tfidf.txt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efbcc944865a494686aac50055b8e8c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class TFIDFReranker(Reranker):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def fit(self, data_iterator):\n",
        "    \"\"\"Compute inverse document frequences (IDF) from the provided data.\n",
        "\n",
        "    Args:\n",
        "      data_iterator: An iterator over the data as produced by the make_batch_iterator\n",
        "        function above.\n",
        "    \"\"\"\n",
        "\n",
        "    # The code below should fill out the `idf_vals` vector with the non-negative\n",
        "    # IDF value of each term in the vocabulary\n",
        "    idf_vals = np.zeros((self.vocab_size,), dtype=np.float32)\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    # 1) count the number of documents each term in the vocabulary appears in\n",
        "    # 2) use the counts to compute inverse document frequencies (DF)\n",
        "    \n",
        "    # Count documents for each term\n",
        "    doc_counts = np.zeros(self.vocab_size, dtype=np.int32)\n",
        "    total_docs = 0\n",
        "    \n",
        "    for batch_tuple in data_iterator:\n",
        "        # The iterator returns a tuple, so we need to extract the indices\n",
        "        batch = batch_tuple[0]  # Extract the indices from the tuple\n",
        "        total_docs += batch.shape[0]\n",
        "        # For each document in the batch, find unique tokens\n",
        "        for i in range(batch.shape[0]):\n",
        "            # Get unique tokens in this document (excluding padding)\n",
        "            unique_tokens = np.unique(batch[i])\n",
        "            # Increment document count for each unique token\n",
        "            for token in unique_tokens:\n",
        "                if token != 0:  # Skip padding token\n",
        "                    doc_counts[token] += 1\n",
        "    \n",
        "    # Compute IDF values using the formula: log_10((1 + N) / (1 + doc_count))\n",
        "    idf_vals = np.log10((1 + total_docs) / (1 + doc_counts))\n",
        "    ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "    # We will create a diagonal sparse matrix of size V x V to store the IDF values\n",
        "    # so that they can be easily multiplied with sparse term frequency vectors to\n",
        "    # get the TFIDF later.\n",
        "    self.idf = csr_matrix(\n",
        "        (idf_vals, (np.arange(self.vocab_size), np.arange(self.vocab_size))),\n",
        "        shape=(self.vocab_size, self.vocab_size))\n",
        "\n",
        "  def vectorize(self, indices):\n",
        "    \"\"\"Convert a batch of token indices to TFIDF vectors.\n",
        "\n",
        "    Args:\n",
        "      indices: (batch_size x max_len) Matrix of token indices, optionally ending\n",
        "        with padding tokens.\n",
        "\n",
        "    Returns:\n",
        "      A sparse matrix of size (batch_size x vocab_size) containing the TFIDF vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    # This code should compute the term frequencies of every vocab item in\n",
        "    # a document. The result should be a sparse CSR matrix `term_frequencies`\n",
        "    # of size batch_size x vocab_size, where each row contains the term frequencies\n",
        "    # tf(t, d) of the corresponding batch element d.\n",
        "    \n",
        "    batch_size = indices.shape[0]\n",
        "    \n",
        "    # Initialize lists to store the sparse matrix data\n",
        "    data = []  # Values\n",
        "    row_ind = []  # Row indices\n",
        "    col_ind = []  # Column indices\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        # Get counts of each token in the document (excluding padding)\n",
        "        token_counts = {}\n",
        "        for token in indices[i]:\n",
        "            if token != 0:  # Skip padding token\n",
        "                if token in token_counts:\n",
        "                    token_counts[token] += 1\n",
        "                else:\n",
        "                    token_counts[token] = 1\n",
        "        \n",
        "        # Compute log term frequency: tf = log10(count + 1)\n",
        "        for token, count in token_counts.items():\n",
        "            tf = np.log10(count + 1)\n",
        "            data.append(tf)\n",
        "            row_ind.append(i)\n",
        "            col_ind.append(token)\n",
        "    \n",
        "    # Create sparse matrix of term frequencies\n",
        "    term_frequencies = csr_matrix((data, (row_ind, col_ind)), \n",
        "                                 shape=(batch_size, self.vocab_size))\n",
        "    ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "    # multiply term_frequencies (batch_size x vocab_size) with the IDFs\n",
        "    # (vocab_size x vocab_size) to get TF-IDFs.\n",
        "    tfidfs = term_frequencies.dot(self.idf)\n",
        "\n",
        "    # lastly we apply L2 normalization within each document\n",
        "    return normalize(tfidfs, norm='l2', axis=1)\n",
        "\n",
        "  def rerank(self, query_batch, passages_batch):\n",
        "    \"\"\"Compute TFIDF scores between queries and their associated passages.\n",
        "\n",
        "    Args:\n",
        "      query_batch: (batch_size x max len) A batch of query indices.\n",
        "      passages_batch: (batch_size x num_psgs x max_len) A batch of passages for\n",
        "        each query.\n",
        "\n",
        "    Returns:\n",
        "      indices: (batch_size x num_psgs) Indices which sort passages in descending\n",
        "        order of TFIDF scores.\n",
        "      scores: (batch_size x num_psgs) Sorted TFIDF scores.\n",
        "    \"\"\"\n",
        "    indices = np.zeros(passages_batch.shape[:2], dtype=np.int32)\n",
        "    scores = np.zeros(passages_batch.shape[:2], dtype=np.float32)\n",
        "    for i in range(query_batch.shape[0]):\n",
        "      query_tfidf = self.vectorize(query_batch[[i], :]) # 1 x vocab_size\n",
        "      passages_tfidf = self.vectorize(passages_batch[i, :, :]) # num_psgs x vocab_size\n",
        "      sc = (passages_tfidf.dot(query_tfidf.transpose())).toarray().squeeze()\n",
        "      idx = np.argsort(sc)[::-1]\n",
        "      indices[i, :] = idx\n",
        "      scores[i, :] = sc[idx]\n",
        "    return indices, scores\n",
        "\n",
        "# Train the TF-IDF reranker.\n",
        "tfidf_reranker = TFIDFReranker(tokenizer.vocab_size)\n",
        "train_iterator = make_batch_iterator(train_data, 16)\n",
        "tfidf_reranker.fit(train_iterator)\n",
        "\n",
        "# Evaluate on the dev set.\n",
        "dev_iterator = make_batch_iterator(dev_data, 16)\n",
        "_ = evaluate(dev_iterator, tfidf_reranker, verbose=True)\n",
        "\n",
        "save_predictions(make_batch_iterator(test_data, 4), tfidf_reranker, \"preds_tfidf.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pT9DFoko6vh"
      },
      "source": [
        "## Dense Retriever\n",
        "\n",
        "Next let's explore a dense retriever, similar to the one introduced by [Karpukhin et al., 2020](https://arxiv.org/abs/2004.04906), which can capture more semantics of the query and documents. The dense retriever adopts a bi-encoder architecture, essentially using a pretrained BERT model to separately encode the queries and the passages, and then compare their embeddings to rank by similarity.\n",
        "\n",
        "In this assignment, we will use a shared encoder for both the query and passage, which means that a single model will be used for producing the embeddings of both. You can however try using different encoders in the improvement section. We will use **dot-product** as the similarity measurement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkUmcvhMTgfz"
      },
      "source": [
        "### Off-the-shelf DPR\n",
        "\n",
        "First, let's use the pretrained model directly without any finetuning. We will compute the embedding of both the query and the list of passages by taking the output from BERT corresponding to the `[CLS]` token. Then the score of each passage will be defined as the dot product between these embeddings.\n",
        "\n",
        "Please complete the `rerank` function, the implementation should be very similar to the TF-IDF `rerank`.\n",
        "\n",
        "The pretrained model should give you a recall@5 above 0.59 and recall@20 above 0.83."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TE58dEY6G0Lz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdc7f40d16d841d79a36fdf303548998",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K\t5\t20\n",
            "------------------------\n",
            "Rec@K\t0.595\t0.836\n"
          ]
        }
      ],
      "source": [
        "class PretrainedDualEncoderModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Load the pretrained model checkpoint.\n",
        "    self.encoder = AutoModel.from_pretrained(model_checkpoint).to(device)\n",
        "\n",
        "  def vectorize(self, indices):\n",
        "    \"\"\"Returns the [CLS] token embeddings after passing through the encoder.\"\"\"\n",
        "    # The tokenizer already adds a [CLS] token to the beginning of the tokenized\n",
        "    # indices, so we just need to take the hidden state output at position 0.\n",
        "    mask = indices != tokenizer.pad_token_id\n",
        "    outputs = self.encoder(input_ids=indices.to(device), attention_mask=mask.to(device))\n",
        "    return outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "  def rerank(self, query_batch, passages_batch):\n",
        "      \"\"\"Rerank passages based on pretrained model embeddings.\n",
        "\n",
        "      Args:\n",
        "        query_batch: (batch_size x max_len) A batch of query indices.\n",
        "        passages_batch: (batch_size x num_psgs x max_len) A batch of passages for\n",
        "          each query.\n",
        "\n",
        "      Returns:\n",
        "        indices: (batch_size x num_psgs) Indices which sort passages in descending\n",
        "          order of relevance scores.\n",
        "        scores: (batch_size x num_psgs) Sorted relevance scores.\n",
        "      \"\"\"\n",
        "      indices = np.zeros(passages_batch.shape[:2], dtype=np.int32)\n",
        "      scores = np.zeros(passages_batch.shape[:2], dtype=np.float32)\n",
        "\n",
        "      ### YOUR CODE HERE !!!!!\n",
        "\n",
        "      # Get query embeddings for the entire batch\n",
        "      query_embeddings = self.vectorize(query_batch)  # batch_size x hidden_dim\n",
        "      \n",
        "      # Process each query and its passages\n",
        "      for i in range(query_batch.shape[0]):\n",
        "          # Reshape passages for this query to process them all at once\n",
        "          num_passages = passages_batch.shape[1]\n",
        "          passages_flat = passages_batch[i].reshape(-1, passages_batch.shape[-1])  # (num_passages x max_len)\n",
        "          \n",
        "          # Get passage embeddings\n",
        "          passage_embeddings = self.vectorize(passages_flat)  # num_passages x hidden_dim\n",
        "          \n",
        "          # Compute similarity scores using dot product\n",
        "          # Take the current query embedding and compute dot product with all passage embeddings\n",
        "          similarity_scores = torch.matmul(passage_embeddings, query_embeddings[i].unsqueeze(1)).squeeze(1)\n",
        "          \n",
        "          # Convert to numpy for sorting\n",
        "          similarity_scores_np = similarity_scores.cpu().detach().numpy()\n",
        "          \n",
        "          # Sort passages by similarity scores in descending order\n",
        "          idx = np.argsort(similarity_scores_np)[::-1]\n",
        "          \n",
        "          # Store the sorted indices and scores\n",
        "          indices[i, :] = idx\n",
        "          scores[i, :] = similarity_scores_np[idx]\n",
        "\n",
        "      ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "      return indices, scores\n",
        "\n",
        "# Evaluate on the dev set.\n",
        "pretrained_reranker = PretrainedDualEncoderModel()\n",
        "dev_iterator = make_batch_iterator(dev_data, 16)\n",
        "_ = evaluate(dev_iterator, pretrained_reranker, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPpNvoBOwjIc"
      },
      "source": [
        "### In-batch Finetuned DPR\n",
        "\n",
        "The pretrained reranker already shows an improvement in recall@K over the TFIDF approach. Next, let's try to finetune the pretrained BERT model on our training data to see if that helps.\n",
        "\n",
        "Recall that the loss we use to optimize the dense retriever is:\n",
        "\n",
        "\\begin{gathered}\n",
        "\\mathcal{L} = \\sum_{q, p^+, \\{p_i^-\\}} -\\log \\frac{\\exp\\left(\\operatorname{sim}(q, p^+)\\right)}{\\exp\\left(\\operatorname{sim}(q, p^+)\\right) + \\sum_{p_i^-}\\exp\\left(\\operatorname{sim}(q, p_i^+)\\right)}.\n",
        "\\end{gathered}\n",
        "\n",
        "Note that this loss can be interpreted as a softmax over a vector of scores between the query and the positive passage as well as all the negative passages. For this part, we will only use **in-batch negatives** to train the reranker, i.e., the $p_i^-$ will be the positive passages $p^+$ for the other queries in the same batch. As a result the number of negatives per query will be `batch_size - 1`.\n",
        "\n",
        "In the class below, first implement the `forward` method which takes a batch of queries and a single positive passage per query and returns the log probabilities across all passages in the batch. Note that this probability needs to be normalized w.r.t the in-batch negatives, i.e., the positives of the all the other questions in the batch.\n",
        "\n",
        "Then, implement the `fit` method which trains the network by iterating over the data, **randomly** selecting a single positive for each query in the batch and calling the forward method for computing the logits. To train the network you can use the `NLLLoss()` criterion by passing in the index of the positive passage for the query as the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zOx40iOLJE4v"
      },
      "outputs": [],
      "source": [
        "class InBatchDualEncoderModel(PretrainedDualEncoderModel):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, query_batch, passage_batch):\n",
        "    \"\"\"Encode queries and passages and compute log probabilities.\n",
        "\n",
        "    Args:\n",
        "      query_batch: (batch_size x max_len) token indices for the queries.\n",
        "      passage_batch: (batch_size x max_len) token indices for the passages.\n",
        "\n",
        "    Returns:\n",
        "      log_probs: (batch_size x batch_size) normalized log probabilities obtained by\n",
        "        taking a softmax of the inner products between each query and all of\n",
        "        the passages.\n",
        "    \"\"\"\n",
        "    # Run the forward pass on queries and passages.\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "    # Get query embeddings\n",
        "    query_embeddings = self.vectorize(query_batch)  # batch_size x hidden_dim\n",
        "    \n",
        "    # Get passage embeddings\n",
        "    passage_embeddings = self.vectorize(passage_batch)  # batch_size x hidden_dim\n",
        "    \n",
        "    # Compute similarity scores using dot product between all queries and all passages\n",
        "    # This creates a batch_size x batch_size matrix of similarity scores\n",
        "    similarity_scores = torch.matmul(query_embeddings, passage_embeddings.transpose(0, 1))\n",
        "    \n",
        "    # Compute log probabilities using log_softmax along dimension 1 (passages)\n",
        "    log_probs = F.log_softmax(similarity_scores, dim=1)\n",
        "    ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "    return log_probs\n",
        "\n",
        "\n",
        "  def fit(self, train_data, dev_data, model_file, n_epochs=5, batch_size = 16):\n",
        "    dev_data_iterator = list(make_batch_iterator(dev_data, 4))\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=5e-5)\n",
        "    best_recall = 0.\n",
        "    for epoch in range(n_epochs):\n",
        "      recall_at_5 = evaluate(dev_data_iterator, self, K=[5], verbose=True)[0]\n",
        "      if recall_at_5 > best_recall:\n",
        "        print(\n",
        "            \"Obtained a new best recall@5 of {:.2f}, saving model \"\n",
        "            \"checkpoint to {}...\".format(recall_at_5, model_file))\n",
        "        torch.save(self.state_dict(), model_file)\n",
        "        best_recall = recall_at_5\n",
        "      self.train()\n",
        "      running_loss = 0.\n",
        "      train_data_iterator = list(make_batch_iterator(train_data, batch_size, shuffle=True))\n",
        "      pbar = tqdm.tqdm(enumerate(train_data_iterator))\n",
        "      for i, (query_batch, passage_batch, example_batch) in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        n = passage_batch.shape[0]\n",
        "\n",
        "        # `query_batch`: (batch_size x max len) A batch of query indices.\n",
        "        # `passages_batch`: (batch_size x num_psgs x max_len) A batch of passages for each query.\n",
        "        # `example_batch`: (batch_size) A list of dict,\n",
        "         # {..., 'positives': [] }  containing indices of positive passages.\n",
        "\n",
        "        # Sample a random positive for each query to get batch_size passages,\n",
        "        # a positive to one query is a negative to the other query\n",
        "        # calculate nll loss based on where the positive is among the batch.\n",
        "        # Think carefully about what the `target` argument should look like! We\n",
        "        # have already initialized the NLLLoss() `criterion` for you above.\n",
        "        ### YOUR CODE HERE !!!!!\n",
        "        # For each query, randomly select one positive passage\n",
        "        selected_passages = []\n",
        "        for j in range(n):\n",
        "            # Get the list of positive passage indices for this query\n",
        "            positives = example_batch[j]['positives']\n",
        "            # Randomly select one positive passage\n",
        "            pos_idx = random.choice(positives)\n",
        "            # Get the selected passage\n",
        "            selected_passage = passage_batch[j, pos_idx]\n",
        "            selected_passages.append(selected_passage)\n",
        "        \n",
        "        # Stack the selected passages into a batch\n",
        "        selected_passages = torch.stack(selected_passages)\n",
        "        \n",
        "        # Forward pass to get log probabilities\n",
        "        log_probs = self.forward(query_batch, selected_passages)\n",
        "        \n",
        "        # The target for each query is its own positive passage (diagonal elements)\n",
        "        # So target[i] = i for all i\n",
        "        target = torch.arange(n).to(query_batch.device)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(log_probs, target)\n",
        "        ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_description(\"Epoch: {}, Loss: {:.2f}, Best R@5: {:.3f}, lr: {:.2g}\".format(epoch, running_loss / (i + 1), best_recall, lr))\n",
        "    print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "    self.load_state_dict(torch.load(model_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij4H1hC9Mdpm"
      },
      "source": [
        "Lets train the model above.\n",
        "You should able to get a recall@5 of 0.70 and recall@20 of 0.85 with in-batch negatives, but you might need to run multiple times in case you get an unlucky seed the first time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GqC319bAMfeO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bff26f1efec4c028518a0dad247b46f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/29 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K\t5\n",
            "------------\n",
            "Rec@K\t0.595\n",
            "Obtained a new best recall@5 of 0.59, saving model checkpoint to inbatch_dualencoder.pt...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01385253c591481d8a5c1af022e48228",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Placeholder storage has not been allocated on MPS device!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m inbatch_reranker \u001b[38;5;241m=\u001b[39m InBatchDualEncoderModel()\n\u001b[0;32m----> 3\u001b[0m \u001b[43minbatch_reranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minbatch_dualencoder.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate on the dev set.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m _ \u001b[38;5;241m=\u001b[39m evaluate(make_batch_iterator(dev_data, \u001b[38;5;241m4\u001b[39m), inbatch_reranker, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[13], line 91\u001b[0m, in \u001b[0;36mInBatchDualEncoderModel.fit\u001b[0;34m(self, train_data, dev_data, model_file, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(n)\u001b[38;5;241m.\u001b[39mto(query_batch\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m### END YOUR CODE HERE !!!!!\u001b[39;00m\n\u001b[1;32m     94\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/loss.py:251\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/functional.py:3148\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3147\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m   3150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
          ]
        }
      ],
      "source": [
        "# Train.\n",
        "inbatch_reranker = InBatchDualEncoderModel()\n",
        "inbatch_reranker.fit(train_data, dev_data, \"inbatch_dualencoder.pt\")\n",
        "\n",
        "# Evaluate on the dev set.\n",
        "_ = evaluate(make_batch_iterator(dev_data, 4), inbatch_reranker, verbose=True)\n",
        "\n",
        "save_predictions(make_batch_iterator(test_data, 4), inbatch_reranker, \"preds_inbatch.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb7lg4F94v2n"
      },
      "source": [
        "### Hard Negative DPR\n",
        "\n",
        "Finetuning the pretrained model improves recall, but the benefit of training with only in-batch negatives quickly diminishes. The task of distinguishing between positives for one query and positives for a completely different query is much easier than our actual task of finding positives among negatives for the same query. The model might just learn a task of differentiating between very distant topics.\n",
        "\n",
        "To improve the dense reranker, we can reduce this training objective mismatch by introducing \"hard negatives\", i.e., passages that are related to the query but do not actually contain the answer. Essentially for each query, **in addition to the in-batch negatives**, we further sample one negative passage for each query from the provided passages (**excluding those labeled as positive, if all passages are positive, you can choose a random one**). Then the softmax in the loss will be over all positives as well as all hard negatives for all queries in the batch. This results in total 2 x batch\\_size terms in the denominator above, where exactly one passage is a positive.\n",
        "\n",
        "Please also complete the `forward` and `fit` functions below. The `forward` function will now take in two sets of passages (`pos_batch` and `neg_batch`) and compute log probabilities across both of these. The `fit` function will train the model by computing the standard DPR loss using these 2 x batch\\_size passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pai41UZJNKj8"
      },
      "outputs": [],
      "source": [
        "class HardNegativeDualEncoderModel(PretrainedDualEncoderModel):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, query_batch, pos_batch, neg_batch):\n",
        "    \"\"\"Encode queries and passages and compute log probabilities.\n",
        "\n",
        "    Args:\n",
        "      query_batch: (batch_size x max_len) token indices for the queries.\n",
        "      pos_batch: (batch_size x max_len) token indices for a single positive\n",
        "        passage per query.\n",
        "      neg_batch: (batch_size x max_len) token indices for a single negative\n",
        "        passage per query.\n",
        "\n",
        "    Returns:\n",
        "      log_probs: (batch_size x 2 * batch_size) normalized log probabilities obtained by\n",
        "        taking a softmax of the inner products between the queries and each of\n",
        "        the positive and negative passages across the batch.\n",
        "    \"\"\"\n",
        "    # Run the forward pass on queries and passages.\n",
        "\n",
        "    ### YOUR CODE HERE !!!!!\n",
        "\n",
        "    # You will probably want to use the `log_softmax` function to compute the log\n",
        "    # probabilities once you have the scores.\n",
        "\n",
        "    ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "    return log_probs\n",
        "\n",
        "  def fit(self, train_data, dev_data, model_file, n_epochs=10, batch_size = 16):\n",
        "    dev_data_iterator = list(make_batch_iterator(dev_data, 4))\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(self.parameters(), lr=5e-5)\n",
        "    best_recall = 0.\n",
        "    for epoch in range(n_epochs):\n",
        "      recall_at_5 = evaluate(dev_data_iterator, self, K=[5], verbose=True)[0]\n",
        "      if recall_at_5 > best_recall:\n",
        "        print(\n",
        "            \"Obtained a new best recall@5 of {:.2f}, saving model \"\n",
        "            \"checkpoint to {}...\".format(recall_at_5, model_file))\n",
        "        torch.save(self.state_dict(), model_file)\n",
        "        best_recall = recall_at_5\n",
        "      self.train()\n",
        "      running_loss = 0.\n",
        "      train_data_iterator = list(make_batch_iterator(train_data, batch_size, shuffle=True))\n",
        "      pbar = tqdm.tqdm(enumerate(train_data_iterator))\n",
        "      for i, (query_batch, passage_batch, example_batch) in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        n = passage_batch.shape[0]\n",
        "\n",
        "        # `query_batch`: (batch_size x max len) A batch of query indices.\n",
        "        # `passages_batch`: (batch_size x num_psgs x max_len) A batch of passages for each query.\n",
        "        # `example_batch`: (batch_size) A list of dict,\n",
        "         # {..., 'positives': [] }  containing indices of positive passages.\n",
        "\n",
        "        # Sample a random positive and a random negative for each question,\n",
        "        # calculate nll loss based on where the positive is.\n",
        "        # Think carefully about what the `target` argument should look like! We\n",
        "        # have already initialized the NLLLoss() `criterion` for you above.\n",
        "        ### YOUR CODE HERE !!!!!\n",
        "\n",
        "        ### END YOUR CODE HERE !!!!!\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_description(\"Epoch: {}, Loss: {:.2f}, Best R@5: {:.3f}, lr: {:.2g}\".format(epoch, running_loss / (i + 1), best_recall, lr))\n",
        "    print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "    self.load_state_dict(torch.load(model_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI5Qmeoy7M3p"
      },
      "source": [
        "You should able to get a recall@5 of 0.76 and recall@20 of 0.91 with hard negatives, but you might need several runs to avoid random worse results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCZSqU9NOGi4"
      },
      "outputs": [],
      "source": [
        "# Train.\n",
        "hardneg_reranker = HardNegativeDualEncoderModel()\n",
        "hardneg_reranker.fit(train_data, dev_data, \"hardneg_dualencoder.pt\")\n",
        "\n",
        "# Evaluate on the dev set.\n",
        "_ = evaluate(make_batch_iterator(dev_data, 4), hardneg_reranker, verbose=True)\n",
        "\n",
        "save_predictions(make_batch_iterator(test_data, 4), hardneg_reranker, \"preds_hardneg.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLoiXBWMaSPc"
      },
      "source": [
        "# Experimentation: 1-Page Report\n",
        "\n",
        "Now it's time for you to experiment.  Try to improve the recall on the validation set further. Note that we will mainly check the recall@5 on gradescope. Feel free to modify the code above directly or copy it in new cells below.\n",
        "\n",
        "Hyper-parameter tuning, learning rate scheduling, or adding regularization might not be as helpful (as in previous assignments) for the DPR models, because they were pre-trained and the fine-tuning is only meant to adapt the model slightly.\n",
        "\n",
        "Here are some substantial and open-ended ideas to try out. You might want to refer to the [DPR paper](https://arxiv.org/abs/2004.04906) as well as [this paper](https://arxiv.org/abs/2005.00181) from Luan et al which provides some intuitions about sparse and dense retrieval.\n",
        "- **Harder negative examples**, in the hard negative selection we have tried, simply broadly related passages were sampled. We can further experiment with other heuristics to sample **harder** nagatives.\n",
        "  - The simplest way is perhaps increasing the number of hard negatives. Previously we only include one hard negative per example. Note that this hyper-parameter change will be considered a less interesting approach.\n",
        "  - We can utilize the model's own predictions, especially those where the model assigns high relevance scores to incorrect passages. These instances, where the model is mistaken but confident, represent valuable hard negatives because they highlight the model's vulnerabilities.\n",
        "  - We can utilize sparse retrieval model to fetch top-ranking passages for a query.\n",
        "  - Does a training curriculum (i.e., from simple to hard) help with the optimization?\n",
        "  - **Note** that you can manually write or collect additional documents from the web for this purpose. You cannot use additional labeled positives.\n",
        "- **Hybrid retrieval systems** with both sparse and dense retrieval. These systems can capture different aspects of the query-document relations (you might want to explain why). For example, you can explore ensemble approach that takes the scores produced by both retrieval methods to form an aggregated results. The aggregation can be defined by manually selected hyper-parameter or trained parameters.\n",
        "- **Cross-encoder** to allow the query and context to attend each other. The DPR model above encodes the query and context separately -- this is essential for efficiency when implementing retrievers which need to work on millions of documents. But for just reranking, it is ok to encode the documents and queries together. See [Humeau et al., 2019](https://arxiv.org/abs/1905.01969) (Section 4.2-4.3) for a detailed comparison on the cross-encoder and the bi-encoder architecture implemented above.\n",
        "\n",
        "For fair comparison, you cannot use any pre-trained language model other than BERT-Mini.\n",
        "\n",
        "For this section, you will submit a write-up describing the extensions and/or modifications that you tried.  Your write-up should be **1-page maximum** in length and should be submitted in PDF format.  You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf.\n",
        "For full credit, your write-up should include:\n",
        "1.   A concise and precise description of the extension that you tried.\n",
        "2.   A motivation for why you believed this approach might improve your model.\n",
        "3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n",
        "4.   A bottom-line summary of your results comparing the scores of your improvement to the original model.\n",
        "The purpose of this exercise is to experiment, so feel free to try/ablate multiple of the suggestions above as well as any others you come up with!\n",
        "When you submit the file, please name it `report.pdf`.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mGiRScr9gme7",
        "uPUTSaTLM2VG",
        "VVRx-fDU6ekc",
        "xnpP1WU9zZ1l"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
